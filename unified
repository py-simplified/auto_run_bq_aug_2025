#!/usr/bin/env python3
"""
Unified BigQuery Data Validation Tool
Combines all modules into a single file with enhanced reference data mapping support.
"""

import os
import sys
import subprocess
import time
import webbrowser
import threading
from pathlib import Path
import socket
import streamlit as st
import pandas as pd
from google.cloud import bigquery
import logging
from datetime import datetime
import re
from io import BytesIO


# =============================================================================
# ENHANCED LOGGING SETUP
# =============================================================================

def setup_enhanced_logging():
    """Setup logging for terminal output only."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        force=True  # Override any existing logging config
    )
    
    logger = logging.getLogger(__name__)
    return logger

# Global logger
logger = setup_enhanced_logging()

def log_to_streamlit(message, level="info"):
    """Display log messages in Streamlit interface (minimal)."""
    timestamp = datetime.now().strftime('%H:%M:%S')
    log_entry = f"[{timestamp}] {message}"
    
    # Add to session state logs
    if 'recent_logs' not in st.session_state:
        st.session_state.recent_logs = []
    st.session_state.recent_logs.append(log_entry)
    
    # Keep only last 50 logs to prevent memory issues
    if len(st.session_state.recent_logs) > 50:
        st.session_state.recent_logs = st.session_state.recent_logs[-50:]
    
    # Only show important messages in Streamlit interface
    if level == "error":
        st.error(f"âŒ {message}")
    elif level == "success":
        st.success(f"âœ… {message}")
    elif level == "warning":
        st.warning(f"âš ï¸ {message}")
    # Remove info messages from Streamlit interface
    
    # Always log to console/terminal for debugging
    getattr(logger, level.lower())(message)

def show_logs_section():
    """Create a minimal section to show recent activity (no log files)."""
    if 'recent_logs' not in st.session_state:
        st.session_state.recent_logs = []
    
    # Only show logs section if there are actual logs
    if st.session_state.recent_logs:
        with st.expander("ðŸ“‹ Recent Activity", expanded=False):
            # Show logs in reverse order (newest first)
            for log_entry in reversed(st.session_state.recent_logs[-10:]):  # Show only last 10
                st.text(log_entry)
            
            if st.button("Clear Activity"):
                st.session_state.recent_logs = []
                st.rerun()


# =============================================================================
# BIGQUERY CLIENT MODULE
# =============================================================================

def connect_to_bigquery(project_id, dataset_id):
    """Initialize BigQuery connection."""
    try:
        # Configure logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
        logger = logging.getLogger(__name__)
        
        logger.info(f"Initializing BigQuery client for project: {project_id}")
        
        # Initialize BigQuery client
        client = bigquery.Client(project=project_id)
        
        # Store client and dataset in session state
        st.session_state.bigquery_client = client
        st.session_state.project_id = project_id
        st.session_state.dataset_id = dataset_id
        st.session_state.connection_status = "connected"
        
        logger.info("âœ… BigQuery client initialized successfully")
        return True, "âœ… Successfully connected to BigQuery!"
        
    except Exception as e:
        st.session_state.connection_status = "failed"
        logging.error(f"âŒ BigQuery connection failed: {str(e)}")
        return False, f"âŒ Connection error: {str(e)}"


def execute_custom_query(query, query_name):
    """Execute a custom BigQuery query."""
    if st.session_state.connection_status != "connected":
        return None, "âŒ Not connected to BigQuery"
    
    try:
        client = st.session_state.bigquery_client
        job = client.query(query)
        results = job.result()
        
        # Convert to pandas DataFrame
        df = results.to_dataframe()
        return {
            'status': 'success',
            'data': df,
            'row_count': len(df),
            'timestamp': datetime.now()
        }, f"âœ… Query executed successfully - {len(df)} rows returned"
        
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now()
        }, f"âŒ Query execution failed: {str(e)}"


def initialize_session_state():
    """Initialize session state variables with default BigQuery connection."""
    if 'test_scenarios' not in st.session_state:
        st.session_state.test_scenarios = None
    if 'results_cache' not in st.session_state:
        st.session_state.results_cache = {}
    if 'connection_status' not in st.session_state:
        # Auto-initialize with default values
        try:
            project_id = 'cohesive-apogee-411113'
            dataset_id = 'banking_sample_data'
            
            client = bigquery.Client(project=project_id)
            st.session_state.bigquery_client = client
            st.session_state.project_id = project_id
            st.session_state.dataset_id = dataset_id
            st.session_state.connection_status = "connected"
        except Exception:
            st.session_state.connection_status = None


# =============================================================================
# ENHANCED SQL GENERATOR MODULE WITH REFERENCE DATA SUPPORT
# =============================================================================

def convert_business_logic_to_safe_sql(derivation_logic, source_table, project_id, dataset_id, reference_table=None):
    """Convert business logic to safe SQL using column names exactly as provided.
    Enhanced to support reference data mapping."""
    
    # Clean and normalize the derivation logic
    logic = derivation_logic.strip()
    
    try:
        # Handle different business logic patterns
        
        # Basic aggregations
        if logic.upper().startswith('SUM(') and 'GROUP_BY' in logic.upper():
            # Extract and use the exact column name from SUM()
            match = re.search(r'SUM\((.*?)\)', logic, re.IGNORECASE)
            if match:
                column_name = match.group(1).strip()
                return f"SUM({column_name})"
            else:
                return f"COUNT(*)"
        
        elif logic.upper().startswith('COUNT(') and 'GROUP_BY' in logic.upper():
            return "COUNT(*)"
        
        elif logic.upper().startswith('AVG(') and 'GROUP_BY' in logic.upper():
            # Extract and use the exact column name from AVG()
            match = re.search(r'AVG\((.*?)\)', logic, re.IGNORECASE)
            if match:
                column_name = match.group(1).strip()
                return f"AVG({column_name})"
            else:
                return "COUNT(*)"
        
        # Conditional logic
        elif logic.upper().startswith('IF('):
            # Convert simple IF to CASE WHEN, preserving exact column references
            if_match = re.search(r'IF\((.*?)\)', logic, re.IGNORECASE)
            if if_match:
                condition = if_match.group(1)
                # Simple conversion - can be enhanced based on specific patterns
                return f'CASE WHEN {condition} THEN "True" ELSE "False" END'
            else:
                return '"Standard"'  # Safe fallback
        
        # String concatenation - Use exactly as provided
        elif logic.upper().startswith('CONCAT('):
            # Return the CONCAT exactly as provided by user
            return logic
        
        # Date operations - Use exactly as provided
        elif 'FORMAT_DATE' in logic.upper():
            return logic
        
        # CASE WHEN conditional logic - Use exactly as provided
        elif logic.upper().startswith('CASE WHEN'):
            return logic  # Use original logic exactly as provided
        
        # Enhanced: Handle reference table column references
        elif reference_table and reference_table in logic:
            # Replace reference table references with proper alias
            # The logic already contains table.column format, we'll handle aliasing in the main SQL
            logging.info(f"ðŸ” Logic contains reference table '{reference_table}': '{logic}'")
            return logic
        
        # Default: Use exactly what the user specified, but replace table names with aliases
        else:
            # Clean the input and replace table references with proper aliases
            cleaned_logic = logic.strip()
            
            # Replace source table references with 's.' alias
            if source_table and source_table in cleaned_logic:
                # Replace all instances of 'source_table.' with 's.'
                cleaned_logic = cleaned_logic.replace(f"{source_table}.", "s.")
                logging.info(f"ï¿½ Replaced '{source_table}.' with 's.' in logic: '{cleaned_logic}'")
            
            # Replace reference table references with 'r.' alias if reference_table is provided
            if reference_table and reference_table in cleaned_logic:
                cleaned_logic = cleaned_logic.replace(f"{reference_table}.", "r.")
                logging.info(f"ðŸ”§ Replaced '{reference_table}.' with 'r.' in logic: '{cleaned_logic}'")
            
            logging.info(f"ðŸ” Final safe SQL logic: '{cleaned_logic}'")
            return cleaned_logic
    
    except Exception as e:
        # Safe fallback for any parsing errors
        logging.error(f"ðŸš¨ Exception in convert_business_logic_to_safe_sql: {e}")
        logging.error(f"ðŸš¨ Input was: '{derivation_logic}' for table: '{source_table}'")
        return logic.strip()  # Return original logic as fallback


def create_data_quality_validation_sql(source_table, source_column, target_column, project_id, source_dataset_id,
                                      reference_table=None, reference_join_key=None, reference_lookup_column=None, 
                                      source_join_key='account_id'):
    """Create SQL for data quality validation when no target table is provided."""
    
    source_ref = f"`{project_id}.{source_dataset_id}.{source_table}`"
    
    # Enhanced: Add reference data CTE if reference table is provided
    reference_cte = ""
    reference_join = ""
    if reference_table and reference_join_key and reference_lookup_column:
        reference_ref = f"`{project_id}.{source_dataset_id}.{reference_table}`"
        # Only select columns that exist in the reference table
        # reference_lookup_column is from source table, not reference table
        reference_select = reference_join_key
            
        reference_cte = f"""
ref_data AS (
    SELECT 
        {reference_select}
    FROM {reference_ref}
),"""
        # Add reference join to source data CTE
        reference_join = f"""
    LEFT JOIN ref_data r ON s.{reference_lookup_column} = r.{reference_join_key}"""
    
    # Clean the source column reference
    clean_source_column = source_column
    if '.' in source_column:
        parts = source_column.split('.')
        if len(parts) == 2:
            table_part, column_part = parts
            if table_part == source_table:
                clean_source_column = f"s.{column_part}"
            elif table_part == 'r' and reference_table:
                clean_source_column = source_column
            else:
                clean_source_column = source_column
        else:
            clean_source_column = source_column
    else:
        if reference_table and source_column in ['category_name', 'description', 'type']:
            clean_source_column = f"r.{source_column}"
        else:
            clean_source_column = f"s.{source_column}"
    
    sql = f"""
-- DATA QUALITY Validation: {target_column}
-- Source Table: {source_table}
-- Column: {source_column}
{f"-- Reference: {reference_table} on {reference_join_key} -> {reference_lookup_column}" if reference_table else ""}
-- Validating data completeness and quality

WITH {reference_cte.lstrip() if reference_cte else ""}source_data AS (
    SELECT 
        s.{source_join_key} as join_key,
        {clean_source_column} as source_value
    FROM {source_ref} s{reference_join if reference_cte else ""}
),
data_quality_summary AS (
    SELECT 
        COUNT(*) as total_records,
        COUNT(source_value) as non_null_values,
        COUNT(*) - COUNT(source_value) as null_values,
        COUNT(DISTINCT source_value) as distinct_values,
        COUNTIF(LENGTH(CAST(source_value AS STRING)) = 0) as empty_values
    FROM source_data
),
final_results AS (
    -- Summary row
    SELECT 
        'SUMMARY' as record_type,
        NULL as join_key,
        NULL as source_value,
        NULL as target_value,
        CASE 
            WHEN non_null_values >= total_records * 0.95 THEN 'OVERALL_PASS'
            WHEN non_null_values >= total_records * 0.8 THEN 'PASS_WITH_NULLS'
            ELSE 'OVERALL_FAIL'
        END as comparison_result,
        CONCAT(
            'Total: ', CAST(total_records AS STRING),
            ', Non-Null: ', CAST(non_null_values AS STRING),
            ', Null: ', CAST(null_values AS STRING),
            ', Distinct: ', CAST(distinct_values AS STRING),
            ', Empty: ', CAST(empty_values AS STRING),
            ', Quality Rate: ', CAST(ROUND(non_null_values * 100.0 / NULLIF(total_records, 0), 2) AS STRING), '%'
        ) as comparison_details,
        1 as sort_order
    FROM data_quality_summary

    UNION ALL

    -- Sample data (limited to 5 for brevity)
    SELECT 
        'SAMPLE_DATA' as record_type,
        CAST(join_key AS STRING) as join_key,
        CAST(source_value AS STRING) as source_value,
        NULL as target_value,
        'DATA_SAMPLE' as comparison_result,
        CONCAT('Sample value: "', CAST(source_value AS STRING), '"') as comparison_details,
        2 as sort_order
    FROM (
        SELECT 
            join_key,
            source_value,
            ROW_NUMBER() OVER (ORDER BY join_key) as rn
        FROM source_data
        WHERE source_value IS NOT NULL
    )
    WHERE rn <= 5
)

SELECT 
    record_type,
    join_key,
    source_value,
    target_value,
    comparison_result,
    comparison_details
FROM final_results
ORDER BY sort_order, join_key
"""
    
    return sql.strip()


def create_direct_column_comparison_sql(source_table, target_table, source_join_key, target_join_key, 
                                      source_column, target_column, project_id, source_dataset_id, 
                                      target_dataset_id=None, reference_table=None, reference_join_key=None, 
                                      reference_lookup_column=None):
    """Create SQL for direct column-to-column comparison without transformation.
    Enhanced with reference data support."""
    
    # Use target_dataset_id if provided, otherwise fall back to source_dataset_id
    if target_dataset_id is None:
        target_dataset_id = source_dataset_id
    
    source_ref = f"`{project_id}.{source_dataset_id}.{source_table}`"
    
    # Check if we have a valid target table
    if target_table is None or target_table.strip() == '':
        # No target table - this is a data quality validation, not a comparison
        # We should not be in direct comparison mode for data quality checks
        # Return a data quality validation SQL instead
        return create_data_quality_validation_sql(
            source_table, source_column, target_column, project_id, source_dataset_id,
            reference_table, reference_join_key, reference_lookup_column, source_join_key
        )
    
    target_ref = f"`{project_id}.{target_dataset_id}.{target_table}`"
    
    # Parse keys for composite key support
    source_keys = parse_join_keys(source_join_key) if source_join_key else ['id']
    target_keys = parse_join_keys(target_join_key) if target_join_key else ['id']
    
    # Create join condition
    try:
        join_condition = create_join_condition(source_keys, target_keys, 's', 't')
    except ValueError as e:
        # Fallback to first key if composite key mismatch
        join_condition = f"s.{source_keys[0]} = t.{target_keys[0]}"
    
    # Enhanced: Add reference data CTE if reference table is provided
    reference_cte = ""
    reference_join = ""
    if reference_table and reference_join_key and reference_lookup_column:
        reference_ref = f"`{project_id}.{source_dataset_id}.{reference_table}`"
        # Only select columns that exist in the reference table
        # reference_lookup_column is from source table, not reference table
        reference_select = reference_join_key
            
        reference_cte = f"""
ref_data AS (
    SELECT 
        {reference_select}
    FROM {reference_ref}
),"""
        # Add reference join to source data CTE (not target)
        reference_join = f"""
    INNER JOIN ref_data r ON s.{reference_lookup_column} = r.{reference_join_key}"""
    
    # Clean the source column reference to avoid double table names
    clean_source_column = source_column
    if '.' in source_column:
        # Extract just the column name if it's in table.column format
        parts = source_column.split('.')
        if len(parts) == 2:
            table_part, column_part = parts
            # If table_part matches source_table, use just the column with 's.' alias
            if table_part == source_table:
                clean_source_column = f"s.{column_part}"
            elif table_part == 'r' and reference_table:
                # Reference table column - keep as is since 'r' is the alias
                clean_source_column = source_column
            else:
                clean_source_column = source_column
        else:
            clean_source_column = source_column
    else:
        # Simple column name - add 's.' alias if no reference data, otherwise check context
        if reference_table and source_column in ['category_name', 'description', 'type']:
            # Common reference columns - assume they come from reference table
            clean_source_column = f"r.{source_column}"
        else:
            clean_source_column = f"s.{source_column}"
    
    # Create join key selectors
    if len(source_keys) == 1:
        source_join_select = f"s.{source_keys[0]}"
        target_join_select = f"t.{target_keys[0]}"
    else:
        # Use proper separator for composite keys
        source_parts = [f"s.{key}" for key in source_keys]
        target_parts = [f"t.{key}" for key in target_keys]
        separator = '", "-", "'
        source_join_select = f"CONCAT({separator.join(source_parts)})"
        target_join_select = f"CONCAT({separator.join(target_parts)})"
    
    sql = f"""
-- DIRECT Column-to-Column Comparison Validation
-- Source: {source_table}.{source_column.split('.')[-1] if '.' in source_column else source_column} vs Target: {target_table}.{target_column}
-- Join: {' + '.join(source_keys)} -> {' + '.join(target_keys)}
{f"-- Reference: {reference_table} on {reference_join_key} -> {reference_lookup_column}" if reference_table else ""}
-- No transformation applied - direct value comparison

WITH source_data AS (
    SELECT 
        {source_join_select} as join_keys,
        {clean_source_column} as source_value
    FROM {source_ref} s{reference_join if reference_cte else ""}
),{reference_cte}
target_data AS (
    SELECT 
        {target_join_select} as join_keys,
        {target_column} as target_value
    FROM {target_ref}
),
direct_comparison AS (
    SELECT 
        COALESCE(s.join_keys, t.join_keys) as join_key,
        s.source_value,
        t.target_value,
        CASE 
            WHEN s.source_value IS NULL AND t.target_value IS NULL THEN 'EXACT_MATCH'
            WHEN s.source_value IS NULL THEN 'EXACT_MATCH'  -- Source NULL also counts as match
            WHEN t.target_value IS NULL THEN 'EXACT_MATCH'  -- Target NULL also counts as match
            WHEN CAST(s.source_value AS STRING) = CAST(t.target_value AS STRING) THEN 'EXACT_MATCH'
            ELSE 'VALUE_MISMATCH'
        END as comparison_result,
        CASE 
            WHEN s.source_value IS NULL AND t.target_value IS NULL THEN 'Both values are NULL - MATCH'
            WHEN s.source_value IS NULL THEN 'Source value is NULL'
            WHEN t.target_value IS NULL THEN 'Target value is NULL'
            WHEN CAST(s.source_value AS STRING) = CAST(t.target_value AS STRING) THEN 'Values match exactly'
            ELSE CONCAT('MISMATCH: Source="', s.source_value, '", Target="', t.target_value, '"')
        END as comparison_details
    FROM source_data s
    FULL OUTER JOIN target_data t ON s.join_keys = t.join_keys
),
comparison_summary AS (
    SELECT 
        COUNT(*) as total_records,
        COUNTIF(comparison_result = 'EXACT_MATCH') as exact_matches,
        COUNTIF(comparison_result = 'VALUE_MISMATCH') as mismatches,
        COUNTIF(comparison_result = 'EXACT_MATCH' AND source_value IS NULL) as source_nulls,
        COUNTIF(comparison_result = 'EXACT_MATCH' AND target_value IS NULL) as target_nulls,
        0 as both_nulls  -- Both NULL now counts as EXACT_MATCH
    FROM direct_comparison
),
final_results AS (
    -- Summary row
    SELECT 
        'SUMMARY' as record_type,
        NULL as join_key,
        NULL as source_value,
        NULL as target_value,
        CASE 
            WHEN exact_matches = total_records THEN 'OVERALL_PASS'
            WHEN mismatches = 0 THEN 'PASS_WITH_NULLS'
            ELSE 'OVERALL_FAIL'
        END as comparison_result,
        CONCAT(
            'Total: ', CAST(total_records AS STRING),
            ', Exact Matches: ', CAST(exact_matches AS STRING),
            ', Mismatches: ', CAST(mismatches AS STRING),
            ', Source Nulls: ', CAST(source_nulls AS STRING),
            ', Target Nulls: ', CAST(target_nulls AS STRING),
            ', Match Rate: ', CAST(ROUND(exact_matches * 100.0 / NULLIF(total_records, 0), 2) AS STRING), '%'
        ) as comparison_details,
        1 as sort_order
    FROM comparison_summary

    UNION ALL

    -- Mismatch details (limited to first 10 for readability)
    SELECT 
        'MISMATCH' as record_type,
        CAST(join_key AS STRING) as join_key,
        CAST(source_value AS STRING) as source_value,
        CAST(target_value AS STRING) as target_value,
        comparison_result,
        comparison_details,
        2 as sort_order
    FROM (
        SELECT 
            join_key,
            source_value,
            target_value,
            comparison_result,
            comparison_details,
            ROW_NUMBER() OVER (ORDER BY join_key) as rn
        FROM direct_comparison
        WHERE comparison_result = 'VALUE_MISMATCH'
    )
    WHERE rn <= 10

    UNION ALL

    -- Sample matches (limited to 5 for brevity)
    SELECT 
        'SAMPLE_MATCH' as record_type,
        CAST(join_key AS STRING) as join_key,
        CAST(source_value AS STRING) as source_value,
        CAST(target_value AS STRING) as target_value,
        comparison_result,
        comparison_details,
        3 as sort_order
    FROM (
        SELECT 
            join_key,
            source_value,
            target_value,
            comparison_result,
            comparison_details,
            ROW_NUMBER() OVER (ORDER BY join_key) as rn
        FROM direct_comparison
        WHERE comparison_result = 'EXACT_MATCH'
    )
    WHERE rn <= 5
)

SELECT 
    record_type,
    join_key,
    source_value,
    target_value,
    comparison_result,
    comparison_details
FROM final_results
ORDER BY sort_order, join_key
"""
    
    return sql.strip()


def parse_join_keys(join_key_str):
    """Parse join key string into list of column names.
    Supports both single keys and comma-separated composite keys.
    """
    if not join_key_str:
        return []
    
    # Split by comma and clean whitespace
    keys = [key.strip() for key in join_key_str.split(',')]
    return [key for key in keys if key]  # Remove empty strings


def create_join_condition(source_keys, target_keys, source_alias='s', target_alias='t'):
    """Create SQL JOIN condition for composite keys."""
    if len(source_keys) != len(target_keys):
        raise ValueError(f"Source keys ({len(source_keys)}) and target keys ({len(target_keys)}) count mismatch")
    
    conditions = []
    for src_key, tgt_key in zip(source_keys, target_keys):
        conditions.append(f"{source_alias}.{src_key} = {target_alias}.{tgt_key}")
    
    return " AND ".join(conditions)


def create_transformation_validation_sql(source_table, target_table, source_join_key, target_join_key, 
                                        target_column, derivation_logic, project_id, source_dataset_id, 
                                        target_dataset_id=None, reference_table=None, reference_join_key=None, 
                                        reference_lookup_column=None):
    """Create SQL for transformation validation using column names exactly as provided.
    Enhanced with reference data mapping support."""
    
    # Use target_dataset_id if provided, otherwise fall back to source_dataset_id
    if target_dataset_id is None:
        target_dataset_id = source_dataset_id
    
    # Check if this is a direct column comparison (no transformation)
    if derivation_logic:
        # Clean the derivation logic
        clean_logic = derivation_logic.strip()
        
        # Check if derivation_logic is just a simple column name (table.column format)
        is_simple_column = (
            # Check if it's in table.column format without complex logic
            '.' in clean_logic and
            not any(func in clean_logic.upper() for func in ['CONCAT(', 'CASE ', 'IF(', 'SUM(', 'COUNT(', 'AVG(', 'MAX(', 'MIN(', 'COALESCE(']) and
            not any(op in clean_logic for op in ['+', '-', '*', '/', '>', '<', '=', 'AND', 'OR']) and
            # Count dots - should be exactly 1 for simple table.column
            clean_logic.count('.') == 1
        )
        
        if is_simple_column:
            logging.info(f"ðŸŽ¯ Detected direct column comparison: {clean_logic} -> {target_column}")
            # Check if we have a valid target table for comparison
            if target_table is None or target_table.strip() == '':
                # No target table - this is data quality validation for a simple column
                return create_data_quality_validation_sql(
                    source_table, clean_logic, target_column, project_id, source_dataset_id,
                    reference_table, reference_join_key, reference_lookup_column, source_join_key
                )
            else:
                # We have both source and target - do direct column comparison
                return create_direct_column_comparison_sql(
                    source_table, target_table, source_join_key, target_join_key,
                    clean_logic, target_column, project_id, source_dataset_id, target_dataset_id,
                    reference_table, reference_join_key, reference_lookup_column
                )
    
    # Continue with transformation validation for complex logic
    source_ref = f"`{project_id}.{source_dataset_id}.{source_table}`"
    
    # Handle composite keys - split by comma and clean whitespace
    source_keys = [key.strip() for key in source_join_key.split(',')]
    target_keys = [key.strip() for key in target_join_key.split(',')]
    
    # Create join key selections for SQL with proper aliasing
    source_key_select = ', '.join([f's.{key}' if not key.startswith('s.') else key for key in source_keys])
    source_key_group = ', '.join([f's.{key}' if not key.startswith('s.') else key for key in source_keys])
    
    # Create a unique identifier for composite keys
    if len(source_keys) > 1:
        composite_key_comment = f"Composite Key: {' + '.join(source_keys)}"
    else:
        composite_key_comment = f"Single Key: {source_keys[0]}"
    
    # Enhanced: Handle reference data mapping
    reference_cte = ""
    reference_join = ""
    reference_alias_logic = derivation_logic
    
    if reference_table and reference_join_key and reference_lookup_column:
        reference_ref = f"`{project_id}.{source_dataset_id}.{reference_table}`"
        # Only select columns that exist in the reference table
        # reference_lookup_column is from source table, not reference table
        reference_select = reference_join_key
        reference_except = f"* EXCEPT ({reference_join_key})"
        
        reference_cte = f"""
ref_data AS (
    SELECT 
        {reference_select},
        {reference_except}
    FROM {reference_ref}
),"""
        
        # Add reference join to source_calculated CTE
        reference_join = f"""
    LEFT JOIN ref_data r ON s.{reference_lookup_column} = r.{reference_join_key}"""
        
        # Replace reference table name with alias in derivation logic
        reference_alias_logic = derivation_logic.replace(reference_table, 'r')
        logging.info(f"ðŸ”— Reference mapping: {reference_table} -> r, Logic: {reference_alias_logic}")
    
    # Convert business logic to safe SQL
    safe_derivation_logic = convert_business_logic_to_safe_sql(
        reference_alias_logic, source_table, project_id, source_dataset_id, reference_table
    )
    
    if any(func in derivation_logic.upper() for func in ['SUM(', 'COUNT(', 'AVG(', 'MAX(', 'MIN(']):
        # Aggregation scenario - REAL validation comparing source vs target
        target_ref = f"`{project_id}.{target_dataset_id}.{target_table}`" if target_table else None
        
        if target_ref:
            # Real comparison between source calculation and target table
            sql = f"""
-- REAL Transformation Validation: {target_column}
-- Source Table: {source_table} vs Target Table: {target_table}
-- {composite_key_comment}
{f"-- Reference: {reference_table} on {reference_join_key} -> {reference_lookup_column}" if reference_table else ""}
-- Derivation Logic: {derivation_logic}
-- Comparing calculated values with actual target values

WITH {reference_cte.lstrip() if reference_cte else ""}source_calculated AS (
    SELECT 
        {source_key_select},
        {safe_derivation_logic} as calculated_{target_column}
    FROM {source_ref} s{reference_join}
    GROUP BY {source_key_group}
),
target_actual AS (
    SELECT 
        {', '.join(target_keys)},
        {target_column} as actual_{target_column}
    FROM {target_ref}
),
comparison AS (
    SELECT 
        s.{source_keys[0]} as join_key,
        s.calculated_{target_column},
        t.actual_{target_column},
        CASE 
            WHEN s.calculated_{target_column} IS NULL AND t.actual_{target_column} IS NULL THEN 'MATCH'
            WHEN s.calculated_{target_column} IS NULL THEN 'MATCH'  -- Source NULL also counts as match
            WHEN t.actual_{target_column} IS NULL THEN 'MATCH'  -- Target NULL also counts as match
            WHEN ABS(CAST(s.calculated_{target_column} AS FLOAT64) - CAST(t.actual_{target_column} AS FLOAT64)) < 0.01 THEN 'MATCH'
            ELSE 'MISMATCH'
        END as validation_result
    FROM source_calculated s
    FULL OUTER JOIN target_actual t ON s.{source_keys[0]} = t.{target_keys[0]}
),
validation_summary AS (
    SELECT 
        COUNT(*) as total_rows,
        COUNTIF(validation_result = 'MATCH') as matching_rows,
        COUNTIF(validation_result = 'MISMATCH') as mismatched_rows,
        COUNTIF(validation_result = 'MATCH' AND calculated_{target_column} IS NULL) as source_null_rows,
        COUNTIF(validation_result = 'MATCH' AND actual_{target_column} IS NULL) as target_null_rows,
        0 as both_null_rows  -- Both NULL now counts as MATCH
    FROM comparison
)
SELECT 
    CASE 
        WHEN matching_rows = total_rows THEN 'PASS'
        ELSE 'FAIL'
    END as validation_status,
    total_rows as row_count,
    ROUND(matching_rows * 100.0 / NULLIF(total_rows, 0), 2) as percentage,
    CONCAT('Matches: ', CAST(matching_rows AS STRING), 
           ', Mismatches: ', CAST(mismatched_rows AS STRING),
           ', Source Nulls: ', CAST(source_null_rows AS STRING),
           ', Target Nulls: ', CAST(target_null_rows AS STRING)) as details
FROM validation_summary
WHERE total_rows > 0
"""
        else:
            # If no target table, just validate the calculation can be performed
            sql = f"""
-- Calculation Validation: {target_column} (No Target Table)
-- Source Table: {source_table}
-- {composite_key_comment}
{f"-- Reference: {reference_table} on {reference_join_key} -> {reference_lookup_column}" if reference_table else ""}
-- Derivation Logic: {derivation_logic}
-- Validating calculation logic and data quality

WITH {reference_cte.lstrip() if reference_cte else ""}source_calculated AS (
    SELECT 
        {source_key_select},
        {safe_derivation_logic} as calculated_{target_column}
    FROM {source_ref} s{reference_join}
    GROUP BY {source_key_group}
),
validation_summary AS (
    SELECT 
        COUNT(*) as total_rows,
        COUNT(calculated_{target_column}) as non_null_results,
        COUNT(*) - COUNT(calculated_{target_column}) as null_results,
        COUNTIF(CAST(calculated_{target_column} AS FLOAT64) < 0) as negative_values,
        COUNTIF(CAST(calculated_{target_column} AS FLOAT64) = 0) as zero_values
    FROM source_calculated
)
SELECT 
    CASE 
        WHEN non_null_results >= total_rows * 0.9 THEN 'PASS'
        ELSE 'FAIL'
    END as validation_status,
    total_rows as row_count,
    ROUND(non_null_results * 100.0 / NULLIF(total_rows, 0), 2) as percentage,
    CONCAT('Calculation completed: ', CAST(non_null_results AS STRING), ' valid results out of ', 
           CAST(total_rows AS STRING), ' records. Negatives: ', CAST(negative_values AS STRING),
           ', Zeros: ', CAST(zero_values AS STRING)) as details
FROM validation_summary
WHERE total_rows > 0
"""
    else:
        # Simple transformation scenario - REAL validation comparing source vs target
        target_ref = f"`{project_id}.{target_dataset_id}.{target_table}`" if target_table else None
        
        if target_ref:
            # Real comparison between source calculation and target table (Enhanced with detailed rows)
            sql = f"""
-- REAL Transformation Validation: {target_column}
-- Source Table: {source_table} vs Target Table: {target_table}
-- {composite_key_comment}
{f"-- Reference: {reference_table} on {reference_join_key} -> {reference_lookup_column}" if reference_table else ""}
-- Derivation Logic: {derivation_logic}
-- Comparing calculated values with actual target values

WITH {reference_cte.lstrip() if reference_cte else ""}source_calculated AS (
    SELECT 
        {source_key_select},
        {safe_derivation_logic} as calculated_{target_column}
    FROM {source_ref} s{reference_join}
),
target_actual AS (
    SELECT 
        {', '.join(target_keys)},
        {target_column} as actual_{target_column}
    FROM {target_ref}
),
detailed_comparison AS (
    SELECT 
        COALESCE(s.{source_keys[0]}, t.{target_keys[0]}) as join_key,
        s.calculated_{target_column},
        t.actual_{target_column},
        CASE 
            WHEN s.calculated_{target_column} IS NULL AND t.actual_{target_column} IS NULL THEN 'PASS'
            WHEN s.calculated_{target_column} IS NULL THEN 'PASS'  -- Source NULL also counts as pass
            WHEN t.actual_{target_column} IS NULL THEN 'PASS'  -- Target NULL also counts as pass
            WHEN CAST(s.calculated_{target_column} AS STRING) = CAST(t.actual_{target_column} AS STRING) THEN 'PASS'
            ELSE 'FAIL'
        END as validation_result,
        CASE 
            WHEN s.calculated_{target_column} IS NULL AND t.actual_{target_column} IS NULL THEN 'Both values are NULL - MATCH'
            WHEN s.calculated_{target_column} IS NULL THEN 'Source calculation resulted in NULL'
            WHEN t.actual_{target_column} IS NULL THEN 'Target value is NULL'
            WHEN CAST(s.calculated_{target_column} AS STRING) = CAST(t.actual_{target_column} AS STRING) THEN 'Values match correctly'
            ELSE CONCAT('MISMATCH: Expected "', s.calculated_{target_column}, '" but got "', t.actual_{target_column}, '"')
        END as validation_details
    FROM source_calculated s
    FULL OUTER JOIN target_actual t ON s.{source_keys[0]} = t.{target_keys[0]}
),
validation_summary AS (
    SELECT 
        COUNT(*) as total_rows,
        COUNTIF(validation_result = 'PASS') as passed_rows,
        COUNTIF(validation_result = 'FAIL') as failed_rows,
        COUNTIF(validation_result = 'PASS' AND calculated_{target_column} IS NULL) as source_null_rows,
        COUNTIF(validation_result = 'PASS' AND actual_{target_column} IS NULL) as target_null_rows,
        0 as both_null_rows  -- Both NULL now counts as PASS
    FROM detailed_comparison
)

SELECT 
    record_type,
    join_key,
    calculated_{target_column},
    actual_{target_column},
    validation_result,
    validation_details
FROM (
    -- Show summary first
    SELECT 
        'SUMMARY' as record_type,
        NULL as join_key,
        NULL as calculated_{target_column},
        NULL as actual_{target_column},
        CASE 
            WHEN failed_rows = 0 THEN 'OVERALL_PASS'
            ELSE 'OVERALL_FAIL'
        END as validation_result,
        CONCAT(
            'Total: ', CAST(total_rows AS STRING),
            ', Passed: ', CAST(passed_rows AS STRING), 
            ', Failed: ', CAST(failed_rows AS STRING),
            ', Source Nulls: ', CAST(source_null_rows AS STRING),
            ', Target Nulls: ', CAST(target_null_rows AS STRING),
            ', Both Nulls: 0',  -- Both NULL now counts as passed
            ', Success Rate: ', CAST(ROUND(passed_rows * 100.0 / NULLIF(total_rows, 0), 2) AS STRING), '%'
        ) as validation_details,
        1 as sort_order
    FROM validation_summary

    UNION ALL

    -- Show all failed rows for debugging
    SELECT 
        'FAILED_RECORD' as record_type,
        CAST(join_key AS STRING) as join_key,
        calculated_{target_column},
        actual_{target_column},
        validation_result,
        validation_details,
        2 as sort_order
    FROM detailed_comparison
    WHERE validation_result = 'FAIL'

    UNION ALL

    -- Show sample passed rows (limited to 5 for brevity)
    SELECT 
        'PASSED_RECORD' as record_type,
        CAST(join_key AS STRING) as join_key,
        calculated_{target_column},
        actual_{target_column},
        validation_result,
        validation_details,
        3 as sort_order
    FROM (
        SELECT 
            join_key,
            calculated_{target_column},
            actual_{target_column},
            validation_result,
            validation_details,
            ROW_NUMBER() OVER (ORDER BY join_key) as rn
        FROM detailed_comparison
        WHERE validation_result = 'PASS'
    )
    WHERE rn <= 5
)
ORDER BY sort_order, join_key
"""
        else:
            # If no target table, validate data quality and transformation logic
            sql = f"""
-- Data Quality Validation: {target_column} (No Target Table)
-- Source Table: {source_table}
-- {composite_key_comment}
{f"-- Reference: {reference_table} on {reference_join_key} -> {reference_lookup_column}" if reference_table else ""}
-- Derivation Logic: {derivation_logic}
-- Validating transformation logic and data quality

WITH {reference_cte.lstrip() if reference_cte else ""}source_calculated AS (
    SELECT 
        {source_key_select},
        {safe_derivation_logic} as calculated_{target_column}
    FROM {source_ref} s{reference_join}
),
validation_summary AS (
    SELECT 
        COUNT(*) as total_rows,
        COUNT(calculated_{target_column}) as non_null_rows,
        COUNT(*) - COUNT(calculated_{target_column}) as null_rows,
        COUNT(DISTINCT calculated_{target_column}) as distinct_values,
        -- Additional quality checks
        COUNTIF(LENGTH(CAST(calculated_{target_column} AS STRING)) = 0) as empty_values,
        COUNTIF(CAST(calculated_{target_column} AS STRING) LIKE '%error%') as error_values
    FROM source_calculated
)
SELECT 
    CASE 
        WHEN non_null_rows >= total_rows * 0.95 AND error_values = 0 THEN 'PASS'
        ELSE 'FAIL'
    END as validation_status,
    total_rows as row_count,
    ROUND(non_null_rows * 100.0 / NULLIF(total_rows, 0), 2) as percentage,
    CONCAT('Quality check: ', CAST(non_null_rows AS STRING), ' valid of ', CAST(total_rows AS STRING),
           ' total. Distinct values: ', CAST(distinct_values AS STRING),
           ', Empty: ', CAST(empty_values AS STRING),
           ', Errors: ', CAST(error_values AS STRING)) as details
FROM validation_summary
WHERE total_rows > 0
"""
    
    return sql


# =============================================================================
# ENHANCED EXCEL HANDLER MODULE
# =============================================================================

logger = logging.getLogger(__name__)


def process_excel_file(uploaded_file):
    """Process uploaded Excel file and return data."""
    try:
        excel_data = pd.read_excel(uploaded_file, sheet_name=None)
        return excel_data, None
    except Exception as e:
        return None, f"Error reading file: {str(e)}"


def execute_all_excel_scenarios():
    """Execute validations directly from Excel data with enhanced reference data support."""
    if 'excel_data' not in st.session_state or st.session_state['excel_data'] is None:
        st.error("No Excel data to execute")
        return
    
    df = st.session_state['excel_data']
    results = []
    
    # Enhanced: Required columns including reference data fields
    # Source_Dataset_Id is required - either in 'project_id.dataset_name' format or with separate Source_Project_Id
    required_cols = ['Scenario_Name', 'Source_Table', 'Derivation_Logic', 'Source_Dataset_Id']
    optional_cols = ['Source_Project_Id', 'Target_Project_Id', 'Target_Dataset_Id']
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        st.error(f"Missing required columns: {', '.join(missing_cols)}")
        available_optional = [col for col in optional_cols if col in df.columns]
        if available_optional:
            st.info(f"Available optional columns: {', '.join(available_optional)}")
        st.info("Required columns:")
        st.info("- Scenario_Name: Name of the validation scenario")
        st.info("- Source_Table: Source table name")
        st.info("- Derivation_Logic: SQL logic for validation")
        st.info("- Source_Dataset_Id: Either 'project_id.dataset_name' format or dataset name with separate Source_Project_Id column")
        return
    
    # Validate Source_Dataset_Id format for early error detection
    validation_errors = []
    for index, row in df.iterrows():
        if pd.isna(row.get('Scenario_Name')) or str(row.get('Scenario_Name')).strip() == '':
            continue  # Skip empty rows
            
        scenario_name = str(row['Scenario_Name']).strip()
        
        # Check if we have proper project/dataset configuration
        has_source_project_id = 'Source_Project_Id' in row and not pd.isna(row['Source_Project_Id'])
        has_source_dataset_id = 'Source_Dataset_Id' in row and not pd.isna(row['Source_Dataset_Id'])
        
        if not has_source_dataset_id:
            validation_errors.append(f"Row {index+1} ({scenario_name}): Missing Source_Dataset_Id")
        elif not has_source_project_id:
            source_dataset_full = str(row['Source_Dataset_Id']).strip()
            if '.' not in source_dataset_full:
                validation_errors.append(f"Row {index+1} ({scenario_name}): Source_Dataset_Id '{source_dataset_full}' must be in 'project_id.dataset_name' format when Source_Project_Id is not provided")
    
    if validation_errors:
        st.error("Data validation errors found:")
        for error in validation_errors:
            st.error(f"âŒ {error}")
        st.info("Please fix these errors before proceeding:")
        st.info("- Provide Source_Project_Id column OR use 'project_id.dataset_name' format in Source_Dataset_Id")
        st.info("- Ensure all scenarios have proper project and dataset configuration")
        return
    
    # Progress tracking
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for i, (index, row) in enumerate(df.iterrows()):
        try:
            # Skip empty rows
            if pd.isna(row.get('Scenario_Name')) or str(row.get('Scenario_Name')).strip() == '':
                continue
            
            scenario_name = str(row['Scenario_Name']).strip()
            status_text.text(f"Executing {i+1}: {scenario_name}")
            # Removed unnecessary logging message
            
            # Extract enhanced parameters
            source_table = str(row['Source_Table']).strip()
            
            # Extract project_id and dataset_id from Excel columns
            # Priority: Source_Project_Id > extract from Source_Dataset_Id > raise error
            if 'Source_Project_Id' in row and not pd.isna(row['Source_Project_Id']):
                source_project_id = str(row['Source_Project_Id']).strip()
            elif 'Source_Dataset_Id' in row and not pd.isna(row['Source_Dataset_Id']):
                source_dataset_full = str(row['Source_Dataset_Id']).strip()
                if '.' in source_dataset_full:
                    source_project_id, _ = source_dataset_full.split('.', 1)
                else:
                    raise ValueError(f"Source_Dataset_Id '{source_dataset_full}' must be in format 'project_id.dataset_name' or provide Source_Project_Id column")
            else:
                raise ValueError("Missing required Source_Project_Id or Source_Dataset_Id column")
            
            # Extract source dataset_id
            if 'Source_Dataset_Id' in row and not pd.isna(row['Source_Dataset_Id']):
                source_dataset_full = str(row['Source_Dataset_Id']).strip()
                if '.' in source_dataset_full:
                    _, source_dataset_id = source_dataset_full.split('.', 1)
                else:
                    source_dataset_id = source_dataset_full
            else:
                raise ValueError("Missing required Source_Dataset_Id column")
            
            # Extract target project_id and dataset_id
            if 'Target_Project_Id' in row and not pd.isna(row['Target_Project_Id']):
                target_project_id = str(row['Target_Project_Id']).strip()
            elif 'Target_Dataset_Id' in row and not pd.isna(row['Target_Dataset_Id']):
                target_dataset_full = str(row['Target_Dataset_Id']).strip()
                if '.' in target_dataset_full:
                    target_project_id, _ = target_dataset_full.split('.', 1)
                else:
                    target_project_id = source_project_id  # use source project for same-project scenarios
            else:
                target_project_id = source_project_id  # use source project for same-project scenarios
            
            # Extract target dataset_id
            if 'Target_Dataset_Id' in row and not pd.isna(row['Target_Dataset_Id']):
                target_dataset_full = str(row['Target_Dataset_Id']).strip()
                if '.' in target_dataset_full:
                    _, target_dataset_id = target_dataset_full.split('.', 1)
                else:
                    target_dataset_id = target_dataset_full
            else:
                target_dataset_id = source_dataset_id  # use source dataset for same-dataset scenarios
            
            # Use source project_id as the primary project for the query
            # (assuming most operations will be within the same project)
            primary_project_id = source_project_id
            
            # Handle target table - if not specified or same as source, we're doing data quality validation
            target_table_raw = row.get('Target_Table', '')
            if pd.isna(target_table_raw) or str(target_table_raw).strip() == '' or str(target_table_raw).strip() == source_table:
                target_table = None  # No separate target table - data quality validation
            else:
                target_table = str(target_table_raw).strip()
            
            # Debug info: Log the extracted values for transparency
            logger.info(f"Scenario: {scenario_name}")
            logger.info(f"  Source: {source_project_id}.{source_dataset_id}.{source_table}")
            logger.info(f"  Target: {target_project_id}.{target_dataset_id}.{target_table or 'N/A'}")
            logger.info(f"  Primary Project ID: {primary_project_id}")
            
            # Enhanced: Handle reference data parameters
            reference_table = None
            reference_join_key = None
            reference_lookup_column = None
            
            if 'Reference_Table' in row and not pd.isna(row['Reference_Table']):
                reference_table = str(row['Reference_Table']).strip()
                reference_join_key = str(row.get('Reference_Join_Key', '')).strip() or None
                reference_lookup_column = str(row.get('Reference_Lookup_Column', '')).strip() or None
            
            # Use the exact derivation logic from Excel 
            derivation_logic = str(row['Derivation_Logic']).strip()
            
            # Extract target column name from derivation logic or use a meaningful name
            if 'Target_Column' in row and not pd.isna(row['Target_Column']):
                target_column = str(row['Target_Column']).strip()
            else:
                # Try to extract a meaningful column name from derivation logic
                # Look for simple table.column patterns
                if '.' in derivation_logic and derivation_logic.count('.') == 1:
                    # Simple table.column format
                    table_part, column_part = derivation_logic.split('.', 1)
                    target_column = column_part.strip()
                else:
                    # For complex logic, create a descriptive name
                    if 'COALESCE' in derivation_logic.upper():
                        target_column = 'calculated_coalesce_value'
                    elif 'CONCAT' in derivation_logic.upper():
                        target_column = 'concatenated_value'
                    elif 'SUM' in derivation_logic.upper():
                        target_column = 'sum_value'
                    elif 'COUNT' in derivation_logic.upper():
                        target_column = 'count_value'
                    elif 'AVG' in derivation_logic.upper():
                        target_column = 'avg_value'
                    elif '*' in derivation_logic or '+' in derivation_logic or '-' in derivation_logic:
                        target_column = 'calculated_value'
                    else:
                        target_column = 'derived_value'
            
            # Enhanced: Generate SQL with reference data support
            sql_query = create_transformation_validation_sql(
                source_table=source_table,
                target_table=target_table,  # Can be None for data quality validation
                source_join_key=str(row.get('Source_Join_Key', 'account_id')).strip(),
                target_join_key=str(row.get('Target_Join_Key', 'account_id')).strip(),
                target_column=target_column,
                derivation_logic=derivation_logic,
                project_id=primary_project_id,
                source_dataset_id=source_dataset_id,
                target_dataset_id=target_dataset_id,
                reference_table=reference_table,
                reference_join_key=reference_join_key,
                reference_lookup_column=reference_lookup_column
            )
            
            # Execute query
            if sql_query:
                query_result, message = execute_custom_query(sql_query, scenario_name)
                
                if query_result and query_result['status'] == 'success':
                    result_df = query_result['data']
                    
                    # Parse results (same logic as before)
                    if result_df is not None and not result_df.empty:
                        # Check for different result column patterns
                        if 'validation_status' in result_df.columns:
                            # Pattern 1: Simple validation_status column
                            status = result_df.iloc[0]['validation_status']
                            total_rows = result_df.iloc[0].get('row_count', 1)
                            passed_rows = total_rows if status == 'PASS' else 0
                        elif 'record_type' in result_df.columns and 'comparison_result' in result_df.columns:
                            # Pattern 2: Detailed comparison with record_type (Direct comparison)
                            summary_row = result_df[result_df['record_type'] == 'SUMMARY']
                            if not summary_row.empty:
                                comparison_result = summary_row.iloc[0]['comparison_result']
                                if comparison_result in ['OVERALL_PASS', 'PASS_WITH_NULLS']:
                                    status = 'PASS'
                                else:
                                    status = 'FAIL'
                                # Extract numbers from comparison_details if available
                                details = summary_row.iloc[0].get('comparison_details', '')
                                import re
                                total_match = re.search(r'Total: (\d+)', details)
                                total_rows = int(total_match.group(1)) if total_match else len(result_df)
                                matches_match = re.search(r'Exact Matches: (\d+)', details)
                                passed_rows = int(matches_match.group(1)) if matches_match else 0
                            else:
                                status = 'FAIL'
                                total_rows = len(result_df)
                                passed_rows = 0
                        elif 'record_type' in result_df.columns and 'validation_result' in result_df.columns:
                            # Pattern 3: Detailed validation with record_type (Transformation validation)
                            summary_row = result_df[result_df['record_type'] == 'SUMMARY']
                            if not summary_row.empty:
                                validation_result = summary_row.iloc[0]['validation_result']
                                if validation_result in ['OVERALL_PASS', 'PASS']:
                                    status = 'PASS'
                                else:
                                    status = 'FAIL'
                                # Extract numbers from validation_details if available
                                details = summary_row.iloc[0].get('validation_details', '')
                                import re
                                total_match = re.search(r'Total: (\d+)', details)
                                total_rows = int(total_match.group(1)) if total_match else len(result_df)
                                passed_match = re.search(r'Passed: (\d+)', details)
                                passed_rows = int(passed_match.group(1)) if passed_match else 0
                            else:
                                status = 'FAIL'
                                total_rows = len(result_df)
                                passed_rows = 0
                        else:
                            # Pattern 4: Fallback - assume presence of data means validation completed
                            status = 'PASS'  # Default to PASS unless we find failure indicators
                            total_rows = len(result_df)
                            passed_rows = total_rows
                    else:
                        status = 'PASS'
                        total_rows = 0
                        passed_rows = 0
                    
                    results.append({
                        'scenario_name': scenario_name,
                        'status': status,
                        'total_rows': total_rows,
                        'pass_rows': passed_rows,
                        'fail_rows': total_rows - passed_rows,
                        'source_table': source_table,
                        'target_table': target_table or 'N/A (Data Quality Check)',
                        'target_column': target_column,
                        'reference_table': reference_table or '',
                        'sql_query': sql_query,
                        'timestamp': datetime.now()
                    })
                else:
                    error_msg = query_result.get('error', 'Query failed') if query_result else 'Query failed'
                    results.append({
                        'scenario_name': scenario_name,
                        'status': 'ERROR',
                        'total_rows': 0,
                        'pass_rows': 0,
                        'fail_rows': 0,
                        'error_message': error_msg,
                        'source_table': source_table,
                        'target_table': target_table or 'N/A (Data Quality Check)',
                        'target_column': target_column,
                        'reference_table': reference_table or '',
                        'sql_query': sql_query,
                        'timestamp': datetime.now()
                    })
            else:
                results.append({
                    'scenario_name': scenario_name,
                    'status': 'ERROR',
                    'total_rows': 0,
                    'pass_rows': 0,
                    'fail_rows': 0,
                    'error_message': 'Failed to generate SQL',
                    'source_table': source_table,
                    'target_table': target_table or 'N/A (Data Quality Check)',
                    'target_column': target_column,
                    'reference_table': reference_table or '',
                    'sql_query': 'SQL generation failed',
                    'timestamp': datetime.now()
                })
                
        except Exception as e:
            logger.error(f"Error executing row {index}: {str(e)}")
            results.append({
                'scenario_name': f"Row_{index}",
                'status': 'ERROR',
                'total_rows': 0,
                'pass_rows': 0,
                'fail_rows': 0,
                'error_message': str(e),
                'source_table': '',
                'target_table': '',
                'target_column': '',
                'reference_table': '',
                'sql_query': 'Error in processing',
                'timestamp': datetime.now()
            })
        
        # Update progress
        progress_bar.progress((i + 1) / len(df))
    
    # Store results
    st.session_state['scenario_results'] = results
    
    # Clear progress
    progress_bar.empty()
    status_text.empty()
    
    # Show summary
    passed = len([r for r in results if r['status'] == 'PASS'])
    failed = len([r for r in results if r['status'] == 'FAIL'])
    errors = len([r for r in results if r['status'] == 'ERROR'])
    
    st.success(f"Completed! Passed: {passed}, Failed: {failed}, Errors: {errors}")


# =============================================================================
# DATA VISUALIZATION MODULE
# =============================================================================

def show_scenario_dashboard():
    """Display minimal dashboard with essential results."""
    if 'scenario_results' not in st.session_state or not st.session_state['scenario_results']:
        st.info("No results available.")
        return
    
    results = st.session_state['scenario_results']
    
    # Summary metrics
    total = len(results)
    passed = len([r for r in results if r['status'] == 'PASS'])
    failed = total - passed
    
    # Show basic metrics
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total", total)
    with col2:
        st.metric("Passed", passed)
    with col3:
        st.metric("Failed", failed)
    
    # Results table
    st.subheader("Results")
    
    # Create simple results dataframe
    results_data = []
    for result in results:
        status_emoji = "âœ…" if result['status'] == 'PASS' else ("âš ï¸" if result['status'] == 'ERROR' else "âŒ")
        
        # Prepare error message if it exists
        error_info = ""
        if result['status'] == 'ERROR' and 'error_message' in result:
            error_info = result['error_message']
            # Truncate long error messages for display
            if len(error_info) > 100:
                error_info = error_info[:97] + "..."
        
        results_data.append({
            'Status': f"{status_emoji} {result['status']}",
            'Scenario': result.get('scenario_name', result.get('name', 'Unknown')),
            'Source': result.get('source_table', ''),
            'Target': result.get('target_table', ''),
            'Reference': result.get('reference_table', ''),
            'Column': result.get('target_column', ''),
            'Rows': f"{result.get('total_rows', 0):,}",
            'Error Details': error_info if error_info else ""
        })
    
    df_results = pd.DataFrame(results_data)
    st.dataframe(df_results, use_container_width=True)
    
    # Create SQL queries dataframe for Excel
    sql_data = []
    for result in results:
        sql_data.append({
            'Scenario': result.get('scenario_name', result.get('name', 'Unknown')),
            'Status': result.get('status', 'UNKNOWN'),
            'Source_Table': result.get('source_table', ''),
            'Target_Table': result.get('target_table', ''),
            'Reference_Table': result.get('reference_table', ''),
            'Target_Column': result.get('target_column', ''),
            'Error_Message': result.get('error_message', ''),
            'SQL_Query': result.get('sql_query', 'No SQL query available')
        })
    
    df_sql = pd.DataFrame(sql_data)
    
    # Excel download with two worksheets
    buffer = BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df_results.to_excel(writer, sheet_name='Validation Results', index=False)
        df_sql.to_excel(writer, sheet_name='SQL Queries', index=False)
        
        # Auto-adjust column widths for SQL sheet
        worksheet = writer.sheets['SQL Queries']
        for column in worksheet.columns:
            max_length = 0
            column_letter = column[0].column_letter
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            # Set a reasonable max width for SQL queries
            adjusted_width = min(max_length + 2, 100)
            worksheet.column_dimensions[column_letter].width = adjusted_width
    
    st.download_button(
        label="ðŸ“¥ Download Results Excel (with SQL)",
        data=buffer.getvalue(),
        file_name="validation_results_with_sql.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )


# =============================================================================
# STREAMLIT APPLICATION
# =============================================================================

st.set_page_config(page_title="BigQuery Validation", page_icon="ðŸ”", layout="wide")


def main():
    initialize_session_state()
    
    # Set default values
    if 'project_id' not in st.session_state:
        st.session_state['project_id'] = 'cohesive-apogee-411113'
    if 'dataset_id' not in st.session_state:
        st.session_state['dataset_id'] = 'banking_sample_data'
    if 'connection_status' not in st.session_state:
        st.session_state['connection_status'] = 'connected'
    
    st.title("ðŸ” BigQuery Data Validation - Enhanced with Reference Data")
    show_validation_section()


def show_validation_section():
    st.subheader("Excel Upload & Validation")
    
    # File upload
    uploaded_file = st.file_uploader("Upload Excel file (Validation_Scenarios.xlsx)", type=['xlsx', 'xls'])
    
    if uploaded_file:
        excel_data, error = process_excel_file(uploaded_file)
        
        if error:
            st.error(error)
            return
            
        st.success("File loaded")
        
        # Sheet selection
        sheet = st.selectbox("Select sheet", list(excel_data.keys()))
        df = excel_data[sheet]
        
        with st.expander("Preview data"):
            st.dataframe(df.head())
            
        # Removed unnecessary column info message
        
        # Check for reference data support (silent check)
        has_reference = 'Reference_Table' in df.columns
        # Removed reference data info message
        
        # Direct execution - no generation step needed
        if st.button("Execute Validations with Reference Data", type="primary"):
            with st.spinner("Executing validations with reference data mapping..."):
                # Store sheet data directly for execution
                st.session_state['excel_data'] = df
                execute_all_excel_scenarios()
                st.success("Execution completed! Check results below.")
    
    # Results
    if 'scenario_results' in st.session_state and st.session_state['scenario_results']:
        st.subheader("Results")
        show_scenario_dashboard()
        
        # Show logs section only after results are available
        show_logs_section()
    else:
        st.info("Upload the new Validation_Scenarios.xlsx file to start validation with reference data support")


# =============================================================================
# LAUNCHER MODULE
# =============================================================================

def find_free_port():
    """Find a free port for the Streamlit server."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0))
        s.listen(1)
        port = s.getsockname()[1]
    return port


def check_port_in_use(port):
    """Check if a port is already in use."""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        try:
            s.bind(('127.0.0.1', port))
            return False
        except socket.error:
            return True


def open_browser_delayed(url, delay=3):
    """Open browser after a delay to ensure server is ready."""
    def delayed_open():
        time.sleep(delay)
        try:
            webbrowser.open(url)
            print(f"ðŸŒ Opened browser: {url}")
        except Exception as e:
            print(f"âš ï¸  Could not open browser automatically: {e}")
            print(f"ðŸ“ Please manually open: {url}")
    
    thread = threading.Thread(target=delayed_open)
    thread.daemon = True
    thread.start()


def launch_app():
    """Launch the unified application."""
    print("ðŸš€ BigQuery Data Validation Tool - Unified Launcher")
    print("================================================")
    print("ðŸ”„ Starting Streamlit server...")
    print("ðŸ“‹ Instructions:")
    print("1. Upload your Validation_Scenarios.xlsx file")
    print("2. Select the appropriate sheet") 
    print("3. Click 'Execute Validations with Reference Data'")
    print("4. View results and download reports")
    print()
    print("ðŸŒ Opening in your default browser...")
    print("ï¿½ To stop the server: Press Ctrl+C in this window")
    print("=" * 60)
    
    try:
        # Use streamlit run command directly
        import streamlit.web.cli as stcli
        import sys
        
        # Set up arguments for streamlit run
        sys.argv = [
            "streamlit", 
            "run", 
            __file__,
            "--browser.gatherUsageStats", "false"
        ]
        
        # Run streamlit
        stcli.main()
        
    except KeyboardInterrupt:
        print("\nðŸ›‘ Server stopped by user")
    except Exception as e:
        print(f"\nâŒ Error starting Streamlit: {e}")
        print("ðŸ”§ Try running manually: streamlit run unified_bigquery_validation_tool.py")
    
    print("\nâœ… Application stopped")


if __name__ == "__main__":
    # Always run as Streamlit app - no launcher needed
    main()
