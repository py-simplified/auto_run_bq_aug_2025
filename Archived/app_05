import streamlit as st
import pandas as pd
import os
from google.cloud import bigquery
import google.auth

def initialize_client(project_id):
    """Return an authenticated bigquery.Client using ADC. Keep optional proxy lines commented."""
    import os, google.auth
    from google.cloud import bigquery
    # Optional (corp) proxy – leave commented unless needed:
    # if 'prod' in project_id:
    #     os.environ["HTTP_PROXY"] = "googleapis:0000"
    #     os.environ["HTTPS_PROXY"] = "googleapis:0000"
    # elif 'dev' in project_id:
    #     os.environ["HTTP_PROXY"] = "googleapis:0000"
    #     os.environ["HTTPS_PROXY"] = "googleapis:0000"
    credentials, _ = google.auth.default()
    return bigquery.Client(credentials=credentials, project=project_id)

def get_or_switch_client(client, project_id):
    """Return the same client if client.project == project_id else returns initialize_client(project_id)."""
    if client and client.project == project_id:
        return client
    return initialize_client(project_id)

def fix_table_aliases_in_sql(sql_query):
    """
    Fix table alias references in SQL query to use proper aliases instead of full table names
    """
    import re
    
    # Extract table aliases from FROM and JOIN clauses
    table_aliases = {}
    
    # Pattern to find table aliases: `project.dataset.table` AS alias
    alias_pattern = r'`([^`]+)\.([^`]+)\.([^`]+)`\s+AS\s+(\w+)'
    matches = re.findall(alias_pattern, sql_query, re.IGNORECASE)
    
    for project, dataset, table, alias in matches:
        # Store both full qualified name and just table name for replacement
        full_name = f"{project}.{dataset}.{table}"
        table_aliases[table] = alias
        table_aliases[full_name] = alias
    
    # Split query into sections to only modify SELECT, CASE, WHERE clauses
    # Don't modify FROM and JOIN clauses
    lines = sql_query.split('\n')
    modified_lines = []
    
    for line in lines:
        # Skip FROM and JOIN lines - don't modify table references there
        if (re.match(r'\s*FROM\s+', line, re.IGNORECASE) or 
            re.match(r'\s*(?:LEFT\s+|RIGHT\s+|INNER\s+|FULL\s+)?JOIN\s+', line, re.IGNORECASE)):
            modified_lines.append(line)
            continue
        
        modified_line = line
        
        # Replace table.column references with alias.column
        for table_name, alias in table_aliases.items():
            # Pattern to match table_name.column_name but not in quotes
            pattern = r'\b' + re.escape(table_name) + r'\.(\w+)'
            
            def replace_match(match):
                column_name = match.group(1)
                return f"{alias}.{column_name}"
            
            # Only replace if not inside quotes
            # This is a simplified approach - for production, you might need a more sophisticated parser
            if "'" not in modified_line or modified_line.count("'") % 2 == 0:
                modified_line = re.sub(pattern, replace_match, modified_line)
        
        modified_lines.append(modified_line)
    
    return '\n'.join(modified_lines)

def get_column_data_type(client, target_project, target_dataset, target_table, target_column):
    """
    Query BigQuery INFORMATION_SCHEMA to get the data type of a target column
    """
    try:
        # Switch client if needed for different project
        project_client = get_or_switch_client(client, target_project)
        
        type_query = f"""
        SELECT data_type
        FROM `{target_project}.{target_dataset}.INFORMATION_SCHEMA.COLUMNS`
        WHERE table_name = '{target_table}' AND column_name = '{target_column}'
        """
        
        query_job = project_client.query(type_query)
        results = query_job.result(timeout=30)
        
        for row in results:
            return row.data_type
            
        return None  # Column not found
        
    except Exception as e:
        print(f"Error getting column type: {str(e)}")
        return None

def are_join_keys_unique(client, project, dataset, table, join_keys):
    """
    Verify that the provided join keys uniquely identify rows in a BigQuery table.
    
    Args:
        client: BigQuery client
        project: Project ID
        dataset: Dataset ID
        table: Table name
        join_keys: List of column names that should form a unique key
        
    Returns:
        bool: True if join keys are unique, False otherwise
        
    Raises:
        Exception: If query execution fails
    """
    try:
        # Switch client if needed for different project
        project_client = get_or_switch_client(client, project)
        
        # Construct CONCAT expression for composite keys
        if len(join_keys) == 1:
            concat_expr = f"CAST({join_keys[0]} AS STRING)"
        else:
            cast_keys = [f"CAST({key} AS STRING)" for key in join_keys]
            pipe_separator = '"|"'
            join_parts = f', {pipe_separator}, '.join(cast_keys)
            concat_expr = f"CONCAT({join_parts})"
        
        # Query to check uniqueness
        uniqueness_query = f"""
        SELECT 
            COUNT(*) as total_rows,
            COUNT(DISTINCT {concat_expr}) as distinct_keys
        FROM `{project}.{dataset}.{table}`
        WHERE {' AND '.join([f'{key} IS NOT NULL' for key in join_keys])}
        """
        
        query_job = project_client.query(uniqueness_query)
        results = query_job.result(timeout=30)
        
        for row in results:
            total_rows = row.total_rows
            distinct_keys = row.distinct_keys
            return total_rows == distinct_keys
            
        return False  # No results returned
        
    except Exception as e:
        raise Exception(f"Error checking uniqueness for join keys {join_keys} in `{project}.{dataset}.{table}`: {str(e)}")

def generate_validation_sql(row, client=None):
    """
    Generate BigQuery validation SQL for a single row of validation data with type-aware comparison
    Supports multiple join keys and reference columns (comma-separated values)
    """
    # Extract required fields
    source_project = row.get('Source_Project_Id', '')
    source_dataset = row.get('Source_Dataset_Id', '')
    source_table = row.get('Source_Table', '')
    source_join_key = row.get('Source_Join_Key', '')
    
    target_project = row.get('Target_Project_Id', '')
    target_dataset = row.get('Target_Dataset_Id', '')
    target_table = row.get('Target_Table', '')
    target_join_key = row.get('Target_Join_Key', '')
    target_column = row.get('Target_Column', '')
    
    reference_table = row.get('Reference_Table', '')
    reference_join_key = row.get('Reference_Join_Key', '')
    reference_lookup_column = row.get('Reference_Lookup_Column', '')
    
    derivation_logic = row.get('Derivation_Logic', '')
    
    # Parse join keys as lists (handle comma-separated values)
    source_join_keys = [key.strip() for key in str(source_join_key).split(',') if key.strip()]
    target_join_keys = [key.strip() for key in str(target_join_key).split(',') if key.strip()]
    
    # Validate that source and target join keys have the same count
    if len(source_join_keys) != len(target_join_keys):
        raise ValueError(f"Mismatch in join key counts: Source has {len(source_join_keys)} keys, Target has {len(target_join_keys)} keys")
    
    if not source_join_keys or not target_join_keys:
        raise ValueError("Source_Join_Key and Target_Join_Key cannot be empty")
    
    # Parse reference join keys as lists (if reference table is provided)
    reference_join_keys = []
    reference_lookup_columns = []
    if reference_table and str(reference_table).strip() and str(reference_table).strip().lower() != 'nan':
        reference_join_keys = [key.strip() for key in str(reference_join_key).split(',') if key.strip()]
        reference_lookup_columns = [col.strip() for col in str(reference_lookup_column).split(',') if col.strip()]
        
        # Validate that reference join keys and lookup columns have the same count
        if len(reference_join_keys) != len(reference_lookup_columns):
            raise ValueError(f"Mismatch in reference key counts: Reference_Join_Key has {len(reference_join_keys)} keys, Reference_Lookup_Column has {len(reference_lookup_columns)} columns")
    
    # Get the data type of the target column if client is provided
    column_data_type = None
    if client:
        column_data_type = get_column_data_type(client, target_project, target_dataset, target_table, target_column)
    
    # Verify join key uniqueness before proceeding with SQL generation
    if client:
        # Check source table join key uniqueness
        if not are_join_keys_unique(client, source_project, source_dataset, source_table, source_join_keys):
            raise ValueError(f"Join keys {source_join_keys} in table `{source_project}.{source_dataset}.{source_table}` do not uniquely identify rows. Validation halted.")
        
        # Check target table join key uniqueness
        if not are_join_keys_unique(client, target_project, target_dataset, target_table, target_join_keys):
            raise ValueError(f"Join keys {target_join_keys} in table `{target_project}.{target_dataset}.{target_table}` do not uniquely identify rows. Validation halted.")
        
        # Check reference table join key uniqueness (if reference table is provided)
        if reference_table and str(reference_table).strip() and str(reference_table).strip().lower() != 'nan':
            if not are_join_keys_unique(client, source_project, source_dataset, reference_table, reference_join_keys):
                raise ValueError(f"Join keys {reference_join_keys} in reference table `{source_project}.{source_dataset}.{reference_table}` do not uniquely identify rows. Validation halted.")
    
    # Build the SQL query
    sql_parts = []
    
    # Main data joining CTE
    sql_parts.append("WITH joined_data AS (")
    sql_parts.append("  SELECT")
    
    # Use the first join key for COALESCE
    sql_parts.append(f"    COALESCE(target.{target_join_keys[0]}, source.{source_join_keys[0]}) AS join_key,")
    sql_parts.append(f"    {derivation_logic} AS derived_value,")
    sql_parts.append(f"    target.{target_column} AS actual_value")
    
    # FROM clause with target table
    sql_parts.append(f"  FROM `{target_project}.{target_dataset}.{target_table}` AS target")
    
    # JOIN with source table - build multi-column JOIN condition
    sql_parts.append(f"  JOIN `{source_project}.{source_dataset}.{source_table}` AS source")
    
    # Create AND-separated join conditions
    join_conditions = []
    for i in range(len(source_join_keys)):
        join_conditions.append(f"target.{target_join_keys[i]} = source.{source_join_keys[i]}")
    
    sql_parts.append(f"    ON {' AND '.join(join_conditions)}")
    
    # LEFT JOIN with reference table (if provided) - build multi-column JOIN condition
    if reference_table and str(reference_table).strip() and str(reference_table).strip().lower() != 'nan':
        sql_parts.append(f"  LEFT JOIN `{source_project}.{source_dataset}.{reference_table}` AS ref")
        
        # Create AND-separated reference join conditions
        ref_join_conditions = []
        for i in range(len(reference_lookup_columns)):
            ref_join_conditions.append(f"source.{reference_lookup_columns[i]} = ref.{reference_join_keys[i]}")
        
        sql_parts.append(f"    ON {' AND '.join(ref_join_conditions)}")
    
    # WHERE clause
    sql_parts.append(f"  WHERE target.{target_column} IS NOT NULL")
    sql_parts.append(")")
    
    # Main SELECT with type-specific comparison logic
    sql_parts.append("")
    sql_parts.append("SELECT")
    sql_parts.append("  join_key,")
    sql_parts.append("  derived_value,")
    sql_parts.append("  actual_value,")
    
    # Generate type-specific comparison logic based on detected column type
    if column_data_type == 'STRING':
        sql_parts.append("  CASE WHEN TRIM(CAST(derived_value AS STRING)) = TRIM(CAST(actual_value AS STRING)) THEN 'PASS' ELSE 'FAIL' END AS status")
    elif column_data_type in ['FLOAT64', 'NUMERIC', 'BIGNUMERIC']:
        sql_parts.append("  CASE WHEN ABS(SAFE_CAST(derived_value AS FLOAT64) - SAFE_CAST(actual_value AS FLOAT64)) < 0.0001 THEN 'PASS' ELSE 'FAIL' END AS status")
    elif column_data_type == 'INT64':
        sql_parts.append("  CASE WHEN SAFE_CAST(derived_value AS INT64) = SAFE_CAST(actual_value AS INT64) THEN 'PASS' ELSE 'FAIL' END AS status")
    elif column_data_type == 'BOOL':
        sql_parts.append("  CASE WHEN SAFE_CAST(derived_value AS BOOL) = SAFE_CAST(actual_value AS BOOL) THEN 'PASS' ELSE 'FAIL' END AS status")
    elif column_data_type in ['DATE', 'DATETIME', 'TIMESTAMP']:
        sql_parts.append("  CASE WHEN DATE(derived_value) = DATE(actual_value) THEN 'PASS' ELSE 'FAIL' END AS status")
    else:
        # Default fallback for unknown or unsupported types
        sql_parts.append("  CASE WHEN CAST(derived_value AS STRING) = CAST(actual_value AS STRING) THEN 'PASS' ELSE 'FAIL' END AS status")
    
    sql_parts.append("FROM joined_data;")
    
    # Summary query with the same type-specific logic
    sql_parts.append("")
    sql_parts.append("-- Summary")
    sql_parts.append("WITH joined_data AS (")
    sql_parts.append("  SELECT")
    sql_parts.append(f"    COALESCE(target.{target_join_keys[0]}, source.{source_join_keys[0]}) AS join_key,")
    sql_parts.append(f"    {derivation_logic} AS derived_value,")
    sql_parts.append(f"    target.{target_column} AS actual_value")
    sql_parts.append(f"  FROM `{target_project}.{target_dataset}.{target_table}` AS target")
    sql_parts.append(f"  JOIN `{source_project}.{source_dataset}.{source_table}` AS source")
    sql_parts.append(f"    ON {' AND '.join(join_conditions)}")
    
    # Add reference table join in summary if needed
    if reference_table and str(reference_table).strip() and str(reference_table).strip().lower() != 'nan':
        sql_parts.append(f"  LEFT JOIN `{source_project}.{source_dataset}.{reference_table}` AS ref")
        sql_parts.append(f"    ON {' AND '.join(ref_join_conditions)}")
    
    sql_parts.append(f"  WHERE target.{target_column} IS NOT NULL")
    sql_parts.append(")")
    sql_parts.append("SELECT")
    
    # Apply the same type-specific comparison logic for summary
    if column_data_type == 'STRING':
        sql_parts.append("  CASE WHEN TRIM(CAST(derived_value AS STRING)) = TRIM(CAST(actual_value AS STRING)) THEN 'PASS' ELSE 'FAIL' END AS status,")
    elif column_data_type in ['FLOAT64', 'NUMERIC', 'BIGNUMERIC']:
        sql_parts.append("  CASE WHEN ABS(SAFE_CAST(derived_value AS FLOAT64) - SAFE_CAST(actual_value AS FLOAT64)) < 0.0001 THEN 'PASS' ELSE 'FAIL' END AS status,")
    elif column_data_type == 'INT64':
        sql_parts.append("  CASE WHEN SAFE_CAST(derived_value AS INT64) = SAFE_CAST(actual_value AS INT64) THEN 'PASS' ELSE 'FAIL' END AS status,")
    elif column_data_type == 'BOOL':
        sql_parts.append("  CASE WHEN SAFE_CAST(derived_value AS BOOL) = SAFE_CAST(actual_value AS BOOL) THEN 'PASS' ELSE 'FAIL' END AS status,")
    elif column_data_type in ['DATE', 'DATETIME', 'TIMESTAMP']:
        sql_parts.append("  CASE WHEN DATE(derived_value) = DATE(actual_value) THEN 'PASS' ELSE 'FAIL' END AS status,")
    else:
        sql_parts.append("  CASE WHEN CAST(derived_value AS STRING) = CAST(actual_value AS STRING) THEN 'PASS' ELSE 'FAIL' END AS status,")
    
    sql_parts.append("  COUNT(*) as count")
    sql_parts.append("FROM joined_data")
    sql_parts.append("GROUP BY 1;")
    
    # Fix table aliases in the generated SQL
    raw_sql = "\n".join(sql_parts)
    fixed_sql = fix_table_aliases_in_sql(raw_sql)
    
    return fixed_sql

def extract_columns_from_derivation_logic(derivation_logic):
    """
    Extract column references from derivation logic string using enhanced regex patterns.
    Returns a set of unique column names found in the derivation logic.
    Handles both qualified (table.column) and unqualified (column) formats.
    """
    import re
    
    # Convert to string and handle None/empty cases
    if not derivation_logic or str(derivation_logic).strip().lower() in ['', 'nan', 'none']:
        return set()
    
    derivation_str = str(derivation_logic).strip()
    
    # Enhanced regex pattern to extract column names from both qualified and unqualified formats
    # Pattern: r"(?:\b[\w]+\.)?([\w]+)" - captures column names with optional table prefix
    column_pattern = r"(?:\b[\w]+\.)?([\w]+)"
    
    columns = set()
    
    # Find all potential column matches
    matches = re.findall(column_pattern, derivation_str, re.IGNORECASE)
    
    for match in matches:
        column_name = match.strip()
        
        # Filter out SQL keywords, functions, operators, and literals
        excluded_keywords = {
            # SQL Keywords
            'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'AND', 'OR', 'NOT', 'IS', 'NULL', 'TRUE', 'FALSE',
            'AS', 'SELECT', 'FROM', 'WHERE', 'GROUP', 'BY', 'ORDER', 'HAVING', 'DISTINCT', 'UNION',
            'JOIN', 'LEFT', 'RIGHT', 'INNER', 'OUTER', 'ON', 'USING', 'IN', 'EXISTS', 'BETWEEN',
            'LIKE', 'ILIKE', 'REGEXP', 'RLIKE', 'CONTAINS', 'STARTS_WITH', 'ENDS_WITH',
            
            # SQL Functions
            'CAST', 'SAFE_CAST', 'COALESCE', 'CONCAT', 'TRIM', 'COUNT', 'SUM', 'AVG', 'MAX', 'MIN',
            'ABS', 'ROUND', 'CEIL', 'FLOOR', 'DATE', 'STRING', 'INT64', 'FLOAT64', 'BOOL', 'NUMERIC',
            'DATETIME', 'TIMESTAMP', 'TIME', 'UPPER', 'LOWER', 'LENGTH', 'SUBSTR', 'SUBSTRING', 'REPLACE', 
            'SPLIT', 'IF', 'IFNULL', 'NULLIF', 'GREATEST', 'LEAST', 'MOD', 'DIV', 'EXTRACT', 'FORMAT',
            'PARSE', 'SAFE', 'ARRAY', 'STRUCT', 'JSON', 'CURRENT_DATE', 'CURRENT_DATETIME', 
            'CURRENT_TIMESTAMP', 'CURRENT_TIME',
            
            # Common literals and values
            'PREMIUM', 'STANDARD', 'ACTIVE', 'INACTIVE', 'YES', 'NO', 'UNKNOWN'
        }
        
        # Validate column name:
        # 1. Not a SQL keyword/function
        # 2. Not a pure number
        # 3. Minimum length requirement
        # 4. Valid SQL identifier pattern
        # 5. Not an operator
        if (column_name.upper() not in excluded_keywords and 
            not column_name.isdigit() and 
            len(column_name) > 1 and 
            not re.match(r'^[0-9.]+$', column_name) and
            column_name not in ['<=', '>=', '<>', '!=', '=', '+', '-', '*', '/', '%'] and
            re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', column_name)):
            columns.add(column_name)
    
    return columns

def get_fully_qualified_table_name(project, dataset, table):
    """
    Helper function to ensure fully qualified table names in BigQuery.
    Returns the properly formatted table reference.
    """
    if not project or not dataset or not table:
        raise ValueError(f"Missing table components: project='{project}', dataset='{dataset}', table='{table}'")
    
    # Clean and validate components
    project = str(project).strip()
    dataset = str(dataset).strip() 
    table = str(table).strip()
    
    if not project or not dataset or not table or 'nan' in [project.lower(), dataset.lower(), table.lower()]:
        raise ValueError(f"Invalid table components: project='{project}', dataset='{dataset}', table='{table}'")
    
    return f"`{project}.{dataset}.{table}`"

def extract_required_columns_by_table(row):
    """
    Enhanced column extraction that separates required columns by their respective tables.
    
    Column Assignment Logic:
    - SOURCE TABLE: Derivation_Logic columns, Source_Join_Key, Reference_Lookup_Column
    - TARGET TABLE: Target_Column, Target_Join_Key  
    - REFERENCE TABLE: Reference_Join_Key
    
    Note: Reference_Lookup_Column belongs to SOURCE table (contains values to lookup in reference table)
          Reference_Join_Key belongs to REFERENCE table (the lookup key in reference table)
    
    Returns three sets: required_source_columns, required_target_columns, required_reference_columns
    """
    import re
    
    # Get all relevant fields
    derivation_logic = row.get('Derivation_Logic', '')
    target_column = row.get('Target_Column', '')
    source_join_key = row.get('Source_Join_Key', '')
    target_join_key = row.get('Target_Join_Key', '')
    reference_join_key = row.get('Reference_Join_Key', '')
    reference_lookup_column = row.get('Reference_Lookup_Column', '')
    reference_table = row.get('Reference_Table', '')
    
    # Initialize column sets
    required_source_columns = set()
    required_target_columns = set()
    required_reference_columns = set()
    
    # SQL keywords to exclude from column extraction
    excluded_keywords = {
        'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'AND', 'OR', 'NOT', 'IS', 'NULL', 'TRUE', 'FALSE',
        'CAST', 'SAFE_CAST', 'COALESCE', 'CONCAT', 'TRIM', 'UPPER', 'LOWER', 'STRING', 'INT64',
        'FLOAT64', 'BOOL', 'NUMERIC', 'DATE', 'DATETIME', 'TIMESTAMP', 'IF', 'IFNULL', 'NULLIF',
        'COUNT', 'SUM', 'AVG', 'MAX', 'MIN', 'ABS', 'ROUND', 'PREMIUM', 'STANDARD', 'ACTIVE', 'INACTIVE',
        'SELECT', 'FROM', 'WHERE', 'JOIN', 'LEFT', 'RIGHT', 'INNER', 'OUTER', 'ON', 'GROUP', 'BY',
        'ORDER', 'HAVING', 'DISTINCT', 'AS', 'WITH', 'UNION', 'ALL', 'EXISTS', 'IN', 'LIKE', 'BETWEEN'
    }
    
    # 1. Extract derivation logic columns → SOURCE TABLE
    if derivation_logic and str(derivation_logic).strip().lower() not in ['', 'nan', 'none']:
        derivation_str = str(derivation_logic).strip()
        
        # Get table names to exclude them from column extraction
        source_table_name = row.get('Source_Table', '')
        target_table_name = row.get('Target_Table', '')
        reference_table_name = row.get('Reference_Table', '')
        table_names_to_exclude = {source_table_name, target_table_name, reference_table_name}
        table_names_to_exclude = {name for name in table_names_to_exclude if name and str(name).strip().lower() not in ['', 'nan', 'none']}
        
        # Enhanced regex patterns to extract column references
        column_patterns = [
            r'\b[a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*\b',  # table.column format
            r'\b[a-zA-Z_][a-zA-Z0-9_]*\b(?!\s*\.)'  # standalone identifiers (not followed by a dot)
        ]
        
        all_matches = set()
        for pattern in column_patterns:
            matches = re.findall(pattern, derivation_str)
            for match in matches:
                if '.' in match:
                    # Extract column name from table.column format
                    table_part, col_part = match.split('.', 1)
                    all_matches.add(col_part)
                else:
                    # Standalone identifier
                    all_matches.add(match)
        
        # Filter out keywords, table names, and invalid identifiers
        for col in all_matches:
            if (col.upper() not in excluded_keywords and 
                len(col) > 1 and 
                not col.isdigit() and
                col not in table_names_to_exclude):
                required_source_columns.add(col)
    
    # 2. Add Source_Join_Key and Reference_Lookup_Column → SOURCE TABLE
    if source_join_key and str(source_join_key).strip().lower() not in ['', 'nan', 'none']:
        keys = [key.strip() for key in str(source_join_key).split(',') if key.strip()]
        required_source_columns.update(keys)
    
    # Reference_Lookup_Column is part of SOURCE table (used to lookup values in reference table)
    if reference_lookup_column and str(reference_lookup_column).strip().lower() not in ['', 'nan', 'none']:
        keys = [key.strip() for key in str(reference_lookup_column).split(',') if key.strip()]
        required_source_columns.update(keys)
    
    # 3. Add Target_Column and Target_Join_Key → TARGET TABLE
    if target_column and str(target_column).strip().lower() not in ['', 'nan', 'none']:
        required_target_columns.add(str(target_column).strip())
    
    if target_join_key and str(target_join_key).strip().lower() not in ['', 'nan', 'none']:
        keys = [key.strip() for key in str(target_join_key).split(',') if key.strip()]
        required_target_columns.update(keys)
    
    # 4. Add Reference columns → REFERENCE TABLE (if applicable)
    if reference_table and str(reference_table).strip().lower() not in ['', 'nan', 'none']:
        if reference_join_key and str(reference_join_key).strip().lower() not in ['', 'nan', 'none']:
            keys = [key.strip() for key in str(reference_join_key).split(',') if key.strip()]
            required_reference_columns.update(keys)
    
    # Clean and validate all column sets
    required_source_columns = {col for col in required_source_columns if col and str(col).strip() and str(col).strip().lower() != 'nan'}
    required_target_columns = {col for col in required_target_columns if col and str(col).strip() and str(col).strip().lower() != 'nan'}
    required_reference_columns = {col for col in required_reference_columns if col and str(col).strip() and str(col).strip().lower() != 'nan'}
    
    return required_source_columns, required_target_columns, required_reference_columns

def extract_all_required_columns(row):
    """
    Backward compatibility function - returns unified set of all required columns.
    """
    source_cols, target_cols, ref_cols = extract_required_columns_by_table(row)
    return source_cols.union(target_cols).union(ref_cols)

def validate_columns_exist_in_table(client, project, dataset, table, required_columns):
    """
    Validation check to ensure all required columns exist in the table schema.
    Returns (exists, missing_columns)
    """
    try:
        table_ref = f"{project}.{dataset}.{table}"
        table = client.get_table(table_ref)
        
        # Get all column names from schema
        existing_columns = {field.name for field in table.schema}
        
        # Check which required columns are missing
        missing_columns = required_columns - existing_columns
        
        return len(missing_columns) == 0, missing_columns
        
    except Exception as e:
        st.error(f"❌ Error validating table schema: {e}")
        return False, required_columns

def validate_all_tables_columns(client, row, required_source_columns, required_target_columns, required_reference_columns):
    """
    Enhanced validation that checks column existence in their respective tables.
    Returns (all_valid, validation_results) where validation_results contains detailed info.
    """
    validation_results = {
        'source': {'valid': True, 'missing': set(), 'table': ''},
        'target': {'valid': True, 'missing': set(), 'table': ''},
        'reference': {'valid': True, 'missing': set(), 'table': ''}
    }
    
    # Validate Source Table Columns
    if required_source_columns:
        source_project = row.get('Source_Project_Id', '')
        source_dataset = row.get('Source_Dataset_Id', '')
        source_table = row.get('Source_Table', '')
        validation_results['source']['table'] = f"{source_project}.{source_dataset}.{source_table}"
        
        source_valid, source_missing = validate_columns_exist_in_table(
            client, source_project, source_dataset, source_table, required_source_columns)
        validation_results['source']['valid'] = source_valid
        validation_results['source']['missing'] = source_missing
    
    # Validate Target Table Columns
    if required_target_columns:
        target_project = row.get('Target_Project_Id', '')
        target_dataset = row.get('Target_Dataset_Id', '')
        target_table = row.get('Target_Table', '')
        validation_results['target']['table'] = f"{target_project}.{target_dataset}.{target_table}"
        
        target_valid, target_missing = validate_columns_exist_in_table(
            client, target_project, target_dataset, target_table, required_target_columns)
        validation_results['target']['valid'] = target_valid
        validation_results['target']['missing'] = target_missing
    
    # Validate Reference Table Columns (if applicable)
    if required_reference_columns:
        reference_table = row.get('Reference_Table', '')
        if reference_table and str(reference_table).strip().lower() not in ['', 'nan', 'none']:
            # Reference table is typically in the same project/dataset as source
            ref_project = row.get('Source_Project_Id', '')
            ref_dataset = row.get('Source_Dataset_Id', '')
            validation_results['reference']['table'] = f"{ref_project}.{ref_dataset}.{reference_table}"
            
            ref_valid, ref_missing = validate_columns_exist_in_table(
                client, ref_project, ref_dataset, reference_table, required_reference_columns)
            validation_results['reference']['valid'] = ref_valid
            validation_results['reference']['missing'] = ref_missing
    
    # Check if all validations passed
    all_valid = (validation_results['source']['valid'] and 
                 validation_results['target']['valid'] and 
                 validation_results['reference']['valid'])
    
    return all_valid, validation_results

def build_unique_combinations_query(project, dataset, table, required_columns):
    """
    STEP 2: Build Unique Source Row Combinations
    Construct BigQuery SQL to get unique combinations with row counts.
    """
    if not required_columns:
        raise ValueError("No required columns provided for unique combinations query")
    
    # Build fully qualified table name
    table_fq = get_fully_qualified_table_name(project, dataset, table)
    
    # Sort columns for consistent ordering
    column_list = sorted(required_columns)
    column_select = ", ".join(column_list)
    
    query = f"""
    SELECT {column_select}, COUNT(*) AS row_count
    FROM {table_fq}
    GROUP BY {column_select}
    """
    
    return query

def apply_derivation_logic_in_python(df, derivation_logic):
    """
    STEP 3: Apply Derivation Logic in Python
    Use pandas operations to compute derived values on unique combinations.
    Handles common SQL patterns and converts them to pandas-compatible operations.
    """
    try:
        if not derivation_logic or str(derivation_logic).strip().lower() in ['', 'nan', 'none']:
            raise ValueError("Empty derivation logic")
        
        # Clean up the derivation logic
        cleaned_logic = str(derivation_logic).strip()
        
        # Handle common SQL patterns for pandas compatibility
        
        # 1. Handle COALESCE function
        if 'COALESCE' in cleaned_logic.upper():
            import re
            # Pattern for COALESCE(col1, col2, ...)
            coalesce_pattern = r'COALESCE\s*\(\s*([^)]+)\s*\)'
            
            def convert_coalesce(match):
                args = [arg.strip() for arg in match.group(1).split(',')]
                if len(args) == 2:
                    return f"{args[0]}.fillna({args[1]})"
                elif len(args) > 2:
                    # Chain fillna for multiple arguments
                    result = args[0]
                    for arg in args[1:]:
                        result = f"{result}.fillna({arg})"
                    return result
                else:
                    return args[0]
            
            cleaned_logic = re.sub(coalesce_pattern, convert_coalesce, cleaned_logic, flags=re.IGNORECASE)
        
        # 2. Handle CASE WHEN statements (simplified)
        if 'CASE' in cleaned_logic.upper() and 'WHEN' in cleaned_logic.upper():
            # For simple CASE WHEN col = value THEN 'result' ELSE 'default' END
            # Convert to pandas where condition
            case_pattern = r'CASE\s+WHEN\s+([^=]+)\s*=\s*([^T]+)\s+THEN\s+[\'"]([^\'"]+)[\'"]\s+ELSE\s+[\'"]([^\'"]+)[\'"]\s+END'
            
            def convert_case(match):
                col, value, then_val, else_val = match.groups()
                col = col.strip()
                value = value.strip()
                return f"'{then_val}' if ({col} == {value}) else '{else_val}'"
            
            import re
            if re.search(case_pattern, cleaned_logic, re.IGNORECASE):
                cleaned_logic = re.sub(case_pattern, convert_case, cleaned_logic, flags=re.IGNORECASE)
        
        # 3. Handle simple column references and string concatenation
        # If it's just a simple column name, use it directly
        if cleaned_logic in df.columns:
            df['derived_value'] = df[cleaned_logic]
            return df, True, "Success - simple column reference"
        
        # 4. Try to evaluate the expression
        try:
            df['derived_value'] = df.eval(cleaned_logic)
            return df, True, "Success - pandas eval"
        except:
            # Fallback: try as a simple assignment
            try:
                # Check if it's a simple mathematical operation on columns
                df['derived_value'] = eval(cleaned_logic, {'df': df, '__builtins__': {}})
                return df, True, "Success - simple evaluation"
            except:
                # Final fallback: create derived values using column operations
                available_cols = [col for col in df.columns if col in cleaned_logic]
                if available_cols:
                    # Use first available column as fallback
                    df['derived_value'] = df[available_cols[0]]
                    return df, True, f"Fallback - using column {available_cols[0]}"
                else:
                    df['derived_value'] = cleaned_logic  # Use literal value
                    return df, True, "Fallback - literal value"
        
    except Exception as e:
        # Ultimate fallback: create a None column
        df['derived_value'] = None
        return df, False, f"Failed to apply derivation logic: {str(e)}"

def fetch_target_data_and_join(client, df_source, row):
    """
    STEP 4: Join with Target Table
    Fetch target data and join with processed source DataFrame.
    """
    try:
        # Extract target table info
        target_project = row.get('Target_Project_Id', '')
        target_dataset = row.get('Target_Dataset_Id', '')
        target_table = row.get('Target_Table', '')
        target_column = row.get('Target_Column', '')
        target_join_key = row.get('Target_Join_Key', '')
        source_join_key = row.get('Source_Join_Key', '')
        
        # Parse join keys
        target_join_keys = [key.strip() for key in str(target_join_key).split(',') if key.strip()]
        source_join_keys = [key.strip() for key in str(source_join_key).split(',') if key.strip()]
        
        # Build target query
        target_table_fq = get_fully_qualified_table_name(target_project, target_dataset, target_table)
        target_columns = target_join_keys + [target_column]
        target_column_select = ", ".join(set(target_columns))  # Remove duplicates
        
        target_query = f"""
        SELECT {target_column_select}
        FROM {target_table_fq}
        WHERE {target_column} IS NOT NULL
        """
        
        # Execute target query
        df_target = client.query(target_query).to_dataframe()
        
        # Perform join
        if len(source_join_keys) == len(target_join_keys):
            # Create join condition
            left_on = source_join_keys
            right_on = target_join_keys
            
            df_joined = df_source.merge(
                df_target, 
                left_on=left_on, 
                right_on=right_on, 
                how='left',
                suffixes=('', '_target')
            )
            
            # Rename target column to standard name
            if target_column in df_joined.columns:
                df_joined['actual_value'] = df_joined[target_column]
            else:
                df_joined['actual_value'] = None
                
            return df_joined, True, "Join successful"
        else:
            return df_source, False, f"Join key count mismatch: source {len(source_join_keys)}, target {len(target_join_keys)}"
            
    except Exception as e:
        return df_source, False, f"Failed to join with target table: {str(e)}"

def calculate_validation_results(df):
    """
    STEP 5: Calculate Validation Results with Row Count Scaling
    Compare derived vs actual values and multiply by row_count.
    """
    try:
        # Handle missing derived_value or actual_value
        df['derived_value'] = df['derived_value'].fillna('')
        df['actual_value'] = df['actual_value'].fillna('')
        
        # Perform comparison (handle different data types)
        df['status'] = df.apply(lambda row: 
            str(row['derived_value']).strip() == str(row['actual_value']).strip() 
            if pd.notna(row['derived_value']) and pd.notna(row['actual_value']) 
            else False, axis=1)
        
        # Calculate scaled counts
        df['pass_count'] = df['status'].astype(int) * df['row_count']
        df['fail_count'] = (~df['status']).astype(int) * df['row_count']
        
        # Calculate totals
        total_pass = df['pass_count'].sum()
        total_fail = df['fail_count'].sum()
        
        return df, total_pass, total_fail
        
    except Exception as e:
        st.error(f"❌ Error calculating validation results: {e}")
        return df, 0, 0

def enhanced_validation_execution(row, client, scenario_id, scenario_name, output_dir):
    """
    Main function implementing the enhanced validation framework.
    Processes validation using unique combinations approach to avoid row-level operations.
    Uses table-specific column validation to prevent false errors.
    """
    try:
        # STEP 1: Extract Required Columns by Table
        st.write(f"🔍 Step 1: Extracting required columns by table for {scenario_id}...")
        required_source_columns, required_target_columns, required_reference_columns = extract_required_columns_by_table(row)
        
        if not required_source_columns and not required_target_columns:
            return False, "No required columns found in any table", 0, 0, None
        
        # Display column breakdown
        st.write(f"📋 **Column Breakdown:**")
        st.write(f"   • Source table columns: {sorted(required_source_columns) if required_source_columns else 'None'}")
        st.write(f"   • Target table columns: {sorted(required_target_columns) if required_target_columns else 'None'}")
        if required_reference_columns:
            st.write(f"   • Reference table columns: {sorted(required_reference_columns)}")
        
        # STEP 1.5: Enhanced Table-Specific Column Validation
        st.write(f"✅ Step 1.5: Validating columns exist in their respective tables...")
        all_valid, validation_results = validate_all_tables_columns(
            client, row, required_source_columns, required_target_columns, required_reference_columns)
        
        if not all_valid:
            error_details = []
            for table_type, result in validation_results.items():
                if not result['valid'] and result['missing']:
                    error_details.append(f"{table_type.title()} table ({result['table']}) missing columns: {sorted(result['missing'])}")
            
            error_msg = "Column validation failed:\n" + "\n".join(error_details)
            st.error(f"❌ {error_msg}")
            return False, error_msg, 0, 0, None
        
        st.success(f"✅ All required columns validated in their respective tables!")
        
        # Get source table info
        source_project = row.get('Source_Project_Id', '')
        source_dataset = row.get('Source_Dataset_Id', '')
        source_table = row.get('Source_Table', '')
        derivation_logic = row.get('Derivation_Logic', '')
        
        # STEP 2: Build Unique Source Row Combinations (only source columns)
        st.write(f"🔨 Step 2: Building unique combinations query for source table...")
        if not required_source_columns:
            return False, "No source columns found for query generation", 0, 0, None
            
        unique_query = build_unique_combinations_query(
            source_project, source_dataset, source_table, required_source_columns)
        
        st.write(f"📝 Executing unique combinations query...")
        df_source = client.query(unique_query).to_dataframe()
        
        if df_source.empty:
            return False, "No data returned from source query", 0, 0, None
        
        st.write(f"📊 Retrieved {len(df_source)} unique combinations covering {df_source['row_count'].sum()} total rows")
        
        # STEP 3: Apply Derivation Logic in Python
        st.write(f"⚙️ Step 3: Applying derivation logic...")
        df_source, derivation_success, derivation_message = apply_derivation_logic_in_python(df_source, derivation_logic)
        
        if not derivation_success:
            st.warning(f"⚠️ Derivation logic issue: {derivation_message}")
        
        # STEP 4: Join with Target Table
        st.write(f"🔗 Step 4: Joining with target table...")
        df_joined, join_success, join_message = fetch_target_data_and_join(client, df_source, row)
        
        if not join_success:
            st.warning(f"⚠️ Join issue: {join_message}")
        
        # STEP 5: Calculate Validation Results
        st.write(f"🧮 Step 5: Calculating validation results...")
        df_final, total_pass, total_fail = calculate_validation_results(df_joined)
        
        # STEP 6: Save Results
        st.write(f"💾 Step 6: Saving results...")
        failure_file_path = None
        
        if total_fail > 0:
            # Save failure details
            df_failures = df_final[df_final['fail_count'] > 0].copy()
            
            # Create failure file
            failure_filename = f"{scenario_id}_{scenario_name.replace(' ', '_')}_Failures.xlsx"
            failure_file_path = os.path.join(output_dir, "ScenarioFailures", failure_filename)
            
            # Ensure directory exists
            os.makedirs(os.path.dirname(failure_file_path), exist_ok=True)
            
            # Save to Excel
            df_failures.to_excel(failure_file_path, index=False)
            st.write(f"💾 Saved failure details: {failure_file_path}")
        
        success_message = f"Enhanced validation completed: {total_pass} passed, {total_fail} failed"
        st.success(f"✅ {success_message}")
        
        return True, success_message, total_pass, total_fail, failure_file_path
        
    except Exception as e:
        error_msg = f"Enhanced validation failed: {str(e)}"
        st.error(f"❌ {error_msg}")
        return False, error_msg, 0, 0, None

def generate_optimized_validation_sql(row, client=None):
    """
    Generate optimized BigQuery validation SQL using enhanced column extraction and proper aliasing.
    Follows the architecture: base CTE -> joined CTE -> final SELECT with comparison logic.
    This approach dramatically improves performance for large datasets.
    """
    # Extract required fields
    source_project = row.get('Source_Project_Id', '')
    source_dataset = row.get('Source_Dataset_Id', '')
    source_table = row.get('Source_Table', '')
    source_join_key = row.get('Source_Join_Key', '')
    
    target_project = row.get('Target_Project_Id', '')
    target_dataset = row.get('Target_Dataset_Id', '')
    target_table = row.get('Target_Table', '')
    target_join_key = row.get('Target_Join_Key', '')
    target_column = row.get('Target_Column', '')
    
    reference_table = row.get('Reference_Table', '')
    reference_join_key = row.get('Reference_Join_Key', '')
    reference_lookup_column = row.get('Reference_Lookup_Column', '')
    
    derivation_logic = row.get('Derivation_Logic', '')
    
    # Parse join keys as lists (handle comma-separated values)
    source_join_keys = [key.strip() for key in str(source_join_key).split(',') if key.strip()]
    target_join_keys = [key.strip() for key in str(target_join_key).split(',') if key.strip()]
    
    # Validate that source and target join keys have the same count
    if len(source_join_keys) != len(target_join_keys):
        raise ValueError(f"Mismatch in join key counts: Source has {len(source_join_keys)} keys, Target has {len(target_join_keys)} keys")
    
    if not source_join_keys or not target_join_keys:
        raise ValueError("Source_Join_Key and Target_Join_Key cannot be empty")
    
    # Parse reference join keys as lists (if reference table is provided)
    reference_join_keys = []
    reference_lookup_columns = []
    if reference_table and str(reference_table).strip() and str(reference_table).strip().lower() != 'nan':
        reference_join_keys = [key.strip() for key in str(reference_join_key).split(',') if key.strip()]
        reference_lookup_columns = [col.strip() for col in str(reference_lookup_column).split(',') if col.strip()]
        
        # Validate that reference join keys and lookup columns have the same count
        if len(reference_join_keys) != len(reference_lookup_columns):
            raise ValueError(f"Mismatch in reference key counts: Reference_Join_Key has {len(reference_join_keys)} keys, Reference_Lookup_Column has {len(reference_lookup_columns)} columns")
    
    # Step 1: Extract columns from derivation logic using enhanced regex
    derivation_columns = extract_columns_from_derivation_logic(derivation_logic)
    
    # Get the data type of the target column if client is provided
    column_data_type = None
    if client:
        column_data_type = get_column_data_type(client, target_project, target_dataset, target_table, target_column)
    
    # Step 2: Build required columns set - include join keys and derivation columns
    required_columns = set()
    required_columns.update(source_join_keys)  # Always include join keys
    required_columns.update(derivation_columns)  # Columns from derivation logic
    required_columns.update(reference_lookup_columns)  # Reference lookup columns if any
    
    # Clean and validate columns
    required_columns = {col for col in required_columns if col and str(col).strip() and str(col).strip().lower() != 'nan'}
    
    # Ensure we have at least the join keys
    if not required_columns:
        required_columns.update(source_join_keys)
        st.warning(f"⚠️ Could not extract columns from derivation logic. Using join keys only: {source_join_keys}")
    
    # Final validation
    if not required_columns:
        raise ValueError("No valid columns found for SQL generation")
    
    # Step 3: Build fully qualified table names with aliases
    source_table_fq = get_fully_qualified_table_name(source_project, source_dataset, source_table)
    target_table_fq = get_fully_qualified_table_name(target_project, target_dataset, target_table)
    
    sql_parts = []
    
    # Step 4: Create base CTE with unique combinations from source table
    sql_parts.append("-- Optimized validation SQL with enhanced column extraction and proper aliasing")
    sql_parts.append("WITH base AS (")
    sql_parts.append("  SELECT")
    
    # Add required columns with src alias prefix
    column_list = sorted(required_columns)
    for i, col in enumerate(column_list):
        comma = "," if i < len(column_list) - 1 else ""
        sql_parts.append(f"    src.{col}{comma}")
    
    sql_parts.append("    COUNT(*) AS row_count")
    sql_parts.append(f"  FROM {source_table_fq} AS src")
    
    # Handle reference table join if needed
    if reference_table and str(reference_table).strip() and str(reference_table).strip().lower() != 'nan':
        reference_table_fq = get_fully_qualified_table_name(source_project, source_dataset, reference_table)
        sql_parts.append(f"  LEFT JOIN {reference_table_fq} AS ref")
        
        # Create reference join conditions
        ref_join_conditions = []
        for i in range(len(reference_lookup_columns)):
            if i < len(reference_join_keys):
                ref_join_conditions.append(f"src.{reference_lookup_columns[i]} = ref.{reference_join_keys[i]}")
        
        if ref_join_conditions:
            sql_parts.append(f"    ON {' AND '.join(ref_join_conditions)}")
    
    sql_parts.append(f"  GROUP BY {', '.join([f'src.{col}' for col in sorted(required_columns)])}")
    sql_parts.append("),")
    
    # Step 5: Create joined CTE with target table data
    sql_parts.append("joined AS (")
    sql_parts.append("  SELECT")
    
    # Select columns from base CTE
    for i, col in enumerate(column_list):
        comma = "," if i < len(column_list) - 1 else ""
        sql_parts.append(f"    base.{col}{comma}")
    
    # Add derivation logic result and actual value
    sql_parts.append(f"    ({derivation_logic}) AS derived_value,")
    sql_parts.append(f"    tgt.{target_column} AS actual_value,")
    sql_parts.append("    base.row_count")
    
    sql_parts.append("  FROM base")
    sql_parts.append(f"  LEFT JOIN {target_table_fq} AS tgt")
    
    # Create join conditions between base and target
    join_conditions = []
    for i in range(len(source_join_keys)):
        if i < len(target_join_keys):
            join_conditions.append(f"base.{source_join_keys[i]} = tgt.{target_join_keys[i]}")
    
    sql_parts.append(f"    ON {' AND '.join(join_conditions)}")
    sql_parts.append(")")
    
    # Step 6: Final SELECT with comparison logic
    sql_parts.append("")
    sql_parts.append("SELECT *,")
    
    # Generate type-specific comparison logic
    if column_data_type == 'STRING':
        comparison_logic = "CASE WHEN TRIM(CAST(derived_value AS STRING)) = TRIM(CAST(actual_value AS STRING)) THEN 'PASS' ELSE 'FAIL' END"
    elif column_data_type in ['FLOAT64', 'NUMERIC', 'BIGNUMERIC']:
        comparison_logic = "CASE WHEN ABS(SAFE_CAST(derived_value AS FLOAT64) - SAFE_CAST(actual_value AS FLOAT64)) < 0.0001 THEN 'PASS' ELSE 'FAIL' END"
    elif column_data_type == 'INT64':
        comparison_logic = "CASE WHEN SAFE_CAST(derived_value AS INT64) = SAFE_CAST(actual_value AS INT64) THEN 'PASS' ELSE 'FAIL' END"
    elif column_data_type == 'BOOL':
        comparison_logic = "CASE WHEN SAFE_CAST(derived_value AS BOOL) = SAFE_CAST(actual_value AS BOOL) THEN 'PASS' ELSE 'FAIL' END"
    elif column_data_type in ['DATE', 'DATETIME', 'TIMESTAMP']:
        comparison_logic = "CASE WHEN DATE(derived_value) = DATE(actual_value) THEN 'PASS' ELSE 'FAIL' END"
    else:
        # Default fallback for unknown types
        comparison_logic = "CASE WHEN CAST(derived_value AS STRING) = CAST(actual_value AS STRING) THEN 'PASS' ELSE 'FAIL' END"
    
    sql_parts.append(f"  {comparison_logic} AS status")
    sql_parts.append("FROM joined;")
    
    # Step 7: Add summary query with scaled counts  
    sql_parts.append("")
    sql_parts.append("-- Summary with scaled counts")
    sql_parts.append("SELECT")
    sql_parts.append(f"  {comparison_logic} AS status,")
    sql_parts.append("  SUM(row_count) AS scaled_count")
    sql_parts.append("FROM joined")
    sql_parts.append("GROUP BY 1;")
    
    # Build final SQL and add debugging information
    raw_sql = "\n".join(sql_parts)
    
    # Add debugging information as SQL comments
    debug_info = [
        "-- DEBUG INFORMATION:",
        f"-- Extracted derivation columns: {sorted(derivation_columns)}",
        f"-- All required columns: {sorted(required_columns)}",
        f"-- Join keys - Source: {source_join_keys}, Target: {target_join_keys}",
        f"-- Source table (FQ): {source_table_fq}",
        f"-- Target table (FQ): {target_table_fq}",
        f"-- Derivation logic: {derivation_logic}",
        f"-- Column data type: {column_data_type}",
        "-- END DEBUG INFO",
        "",
    ]
    
    final_sql = "\n".join(debug_info) + raw_sql
    
    return final_sql

def generate_validation_sql_without_uniqueness_check(row, client=None):
    """
    Generate BigQuery validation SQL for a single row of validation data with type-aware comparison
    Supports multiple join keys and reference columns (comma-separated values)
    SKIPS join key uniqueness validation (assumes it was done earlier)
    """
    # Extract required fields
    source_project = row.get('Source_Project_Id', '')
    source_dataset = row.get('Source_Dataset_Id', '')
    source_table = row.get('Source_Table', '')
    source_join_key = row.get('Source_Join_Key', '')
    
    target_project = row.get('Target_Project_Id', '')
    target_dataset = row.get('Target_Dataset_Id', '')
    target_table = row.get('Target_Table', '')
    target_join_key = row.get('Target_Join_Key', '')
    target_column = row.get('Target_Column', '')
    
    reference_table = row.get('Reference_Table', '')
    reference_join_key = row.get('Reference_Join_Key', '')
    reference_lookup_column = row.get('Reference_Lookup_Column', '')
    
    derivation_logic = row.get('Derivation_Logic', '')
    
    # Parse join keys as lists (handle comma-separated values)
    source_join_keys = [key.strip() for key in str(source_join_key).split(',') if key.strip()]
    target_join_keys = [key.strip() for key in str(target_join_key).split(',') if key.strip()]
    
    # Validate that source and target join keys have the same count
    if len(source_join_keys) != len(target_join_keys):
        raise ValueError(f"Mismatch in join key counts: Source has {len(source_join_keys)} keys, Target has {len(target_join_keys)} keys")
    
    if not source_join_keys or not target_join_keys:
        raise ValueError("Source_Join_Key and Target_Join_Key cannot be empty")
    
    # Parse reference join keys as lists (if reference table is provided)
    reference_join_keys = []
    reference_lookup_columns = []
    if reference_table and str(reference_table).strip() and str(reference_table).strip().lower() != 'nan':
        reference_join_keys = [key.strip() for key in str(reference_join_key).split(',') if key.strip()]
        reference_lookup_columns = [col.strip() for col in str(reference_lookup_column).split(',') if col.strip()]
        
        # Validate that reference join keys and lookup columns have the same count
        if len(reference_join_keys) != len(reference_lookup_columns):
            raise ValueError(f"Mismatch in reference key counts: Reference_Join_Key has {len(reference_join_keys)} keys, Reference_Lookup_Column has {len(reference_lookup_columns)} columns")
    
    # Get the data type of the target column if client is provided
    column_data_type = None
    if client:
        column_data_type = get_column_data_type(client, target_project, target_dataset, target_table, target_column)
    
    # SKIP join key uniqueness validation - assume it was done in connectivity phase
    
    # Build the SQL query
    sql_parts = []
    
    # Main data joining CTE
    sql_parts.append("WITH joined_data AS (")
    sql_parts.append("  SELECT")
    
    # Use the first join key for COALESCE
    sql_parts.append(f"    COALESCE(target.{target_join_keys[0]}, source.{source_join_keys[0]}) AS join_key,")
    sql_parts.append(f"    {derivation_logic} AS derived_value,")
    sql_parts.append(f"    target.{target_column} AS actual_value")
    
    # FROM clause with target table
    sql_parts.append(f"  FROM `{target_project}.{target_dataset}.{target_table}` AS target")
    
    # JOIN with source table - build multi-column JOIN condition
    sql_parts.append(f"  JOIN `{source_project}.{source_dataset}.{source_table}` AS source")
    
    # Create AND-separated join conditions
    join_conditions = []
    for i in range(len(source_join_keys)):
        join_conditions.append(f"target.{target_join_keys[i]} = source.{source_join_keys[i]}")
    
    sql_parts.append(f"    ON {' AND '.join(join_conditions)}")
    
    # LEFT JOIN with reference table (if provided) - build multi-column JOIN condition
    if reference_table and str(reference_table).strip() and str(reference_table).strip().lower() != 'nan':
        sql_parts.append(f"  LEFT JOIN `{source_project}.{source_dataset}.{reference_table}` AS ref")
        
        # Create AND-separated reference join conditions
        ref_join_conditions = []
        for i in range(len(reference_lookup_columns)):
            ref_join_conditions.append(f"source.{reference_lookup_columns[i]} = ref.{reference_join_keys[i]}")
        
        sql_parts.append(f"    ON {' AND '.join(ref_join_conditions)}")
    
    # WHERE clause
    sql_parts.append(f"  WHERE target.{target_column} IS NOT NULL")
    sql_parts.append(")")
    
    # Main SELECT with type-specific comparison logic
    sql_parts.append("")
    sql_parts.append("SELECT")
    sql_parts.append("  join_key,")
    sql_parts.append("  derived_value,")
    sql_parts.append("  actual_value,")
    
    # Generate type-specific comparison logic based on detected column type
    if column_data_type == 'STRING':
        sql_parts.append("  CASE WHEN TRIM(CAST(derived_value AS STRING)) = TRIM(CAST(actual_value AS STRING)) THEN 'PASS' ELSE 'FAIL' END AS status")
    elif column_data_type in ['FLOAT64', 'NUMERIC', 'BIGNUMERIC']:
        sql_parts.append("  CASE WHEN ABS(SAFE_CAST(derived_value AS FLOAT64) - SAFE_CAST(actual_value AS FLOAT64)) < 0.0001 THEN 'PASS' ELSE 'FAIL' END AS status")
    elif column_data_type == 'INT64':
        sql_parts.append("  CASE WHEN SAFE_CAST(derived_value AS INT64) = SAFE_CAST(actual_value AS INT64) THEN 'PASS' ELSE 'FAIL' END AS status")
    elif column_data_type == 'BOOL':
        sql_parts.append("  CASE WHEN SAFE_CAST(derived_value AS BOOL) = SAFE_CAST(actual_value AS BOOL) THEN 'PASS' ELSE 'FAIL' END AS status")
    elif column_data_type in ['DATE', 'DATETIME', 'TIMESTAMP']:
        sql_parts.append("  CASE WHEN DATE(derived_value) = DATE(actual_value) THEN 'PASS' ELSE 'FAIL' END AS status")
    else:
        # Default fallback for unknown or unsupported types
        sql_parts.append("  CASE WHEN CAST(derived_value AS STRING) = CAST(actual_value AS STRING) THEN 'PASS' ELSE 'FAIL' END AS status")
    
    sql_parts.append("FROM joined_data;")
    
    # Summary query with the same type-specific logic
    sql_parts.append("")
    sql_parts.append("-- Summary")
    sql_parts.append("WITH joined_data AS (")
    sql_parts.append("  SELECT")
    sql_parts.append(f"    COALESCE(target.{target_join_keys[0]}, source.{source_join_keys[0]}) AS join_key,")
    sql_parts.append(f"    {derivation_logic} AS derived_value,")
    sql_parts.append(f"    target.{target_column} AS actual_value")
    sql_parts.append(f"  FROM `{target_project}.{target_dataset}.{target_table}` AS target")
    sql_parts.append(f"  JOIN `{source_project}.{source_dataset}.{source_table}` AS source")
    sql_parts.append(f"    ON {' AND '.join(join_conditions)}")
    
    # Add reference table join in summary if needed
    if reference_table and str(reference_table).strip() and str(reference_table).strip().lower() != 'nan':
        sql_parts.append(f"  LEFT JOIN `{source_project}.{source_dataset}.{reference_table}` AS ref")
        sql_parts.append(f"    ON {' AND '.join(ref_join_conditions)}")
    
    sql_parts.append(f"  WHERE target.{target_column} IS NOT NULL")
    sql_parts.append(")")
    sql_parts.append("SELECT")
    
    # Apply the same type-specific comparison logic for summary
    if column_data_type == 'STRING':
        sql_parts.append("  CASE WHEN TRIM(CAST(derived_value AS STRING)) = TRIM(CAST(actual_value AS STRING)) THEN 'PASS' ELSE 'FAIL' END AS status,")
    elif column_data_type in ['FLOAT64', 'NUMERIC', 'BIGNUMERIC']:
        sql_parts.append("  CASE WHEN ABS(SAFE_CAST(derived_value AS FLOAT64) - SAFE_CAST(actual_value AS FLOAT64)) < 0.0001 THEN 'PASS' ELSE 'FAIL' END AS status,")
    elif column_data_type == 'INT64':
        sql_parts.append("  CASE WHEN SAFE_CAST(derived_value AS INT64) = SAFE_CAST(actual_value AS INT64) THEN 'PASS' ELSE 'FAIL' END AS status,")
    elif column_data_type == 'BOOL':
        sql_parts.append("  CASE WHEN SAFE_CAST(derived_value AS BOOL) = SAFE_CAST(actual_value AS BOOL) THEN 'PASS' ELSE 'FAIL' END AS status,")
    elif column_data_type in ['DATE', 'DATETIME', 'DATE']:
        sql_parts.append("  CASE WHEN DATE(derived_value) = DATE(actual_value) THEN 'PASS' ELSE 'FAIL' END AS status,")
    else:
        sql_parts.append("  CASE WHEN CAST(derived_value AS STRING) = CAST(actual_value AS STRING) THEN 'PASS' ELSE 'FAIL' END AS status,")
    
    sql_parts.append("  COUNT(*) as count")
    sql_parts.append("FROM joined_data")
    sql_parts.append("GROUP BY 1;")
    
    # Fix table aliases in the generated SQL
    raw_sql = "\n".join(sql_parts)
    fixed_sql = fix_table_aliases_in_sql(raw_sql)
    
    return fixed_sql

def execute_optimized_validation_sql_and_save_failures(sql_script, client, scenario_id, scenario_name, output_dir):
    """
    Execute optimized validation SQL that works with unique combinations and scaled results.
    Handles the new SQL structure with applicable_row_count and scaled_count columns.
    """
    import re
    
    try:
        # Extract project from the first table reference in the SQL
        project_match = re.search(r'`([^.]+)\.[^.]+\.[^`]+`', sql_script)
        if not project_match:
            return False, "Could not extract project ID from SQL", 0, 0, None
        
        project_id = project_match.group(1)
        
        # Ensure client project is set if needed
        if client.project != project_id:
            # Switch project if needed using our helper function
            client = get_or_switch_client(client, project_id)
        
        # Execute the main query (not the summary) to get all results
        main_query = sql_script.split('-- Scaled Summary')[0].strip()
        
        # Execute with dry run first to validate syntax
        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)
        dry_run_job = client.query(main_query, job_config=job_config)
        
        # If dry run succeeds, execute the actual query without limit to get full results
        job_config = bigquery.QueryJobConfig(
            dry_run=False, 
            use_query_cache=False,
            maximum_bytes_billed=500 * 1024 * 1024  # 500 MB limit for full results
        )
        
        query_job = client.query(main_query, job_config=job_config)
        results = query_job.result(timeout=120)  # 2 minute timeout for full results
        
        # Convert all results to DataFrame
        all_rows = []
        for row in results:
            all_rows.append(dict(row))
        
        if not all_rows:
            return True, "Query executed but returned no results", 0, 0, None
        
        df_results = pd.DataFrame(all_rows)
        
        # Calculate scaled PASS and FAIL counts using applicable_row_count
        total_pass_count = 0
        total_fail_count = 0
        
        for _, row in df_results.iterrows():
            row_count = row.get('applicable_row_count', 1)
            status = str(row.get('status', 'UNKNOWN')).upper()
            
            if status == 'PASS':
                total_pass_count += row_count
            elif status == 'FAIL':
                total_fail_count += row_count
        
        # Filter for failed records only and expand them for detailed reporting
        failed_records = df_results[df_results['status'].str.upper() == 'FAIL'].copy()
        
        # Create expanded failure records for individual row reporting
        expanded_failures = []
        for _, row in failed_records.iterrows():
            row_count = row.get('applicable_row_count', 1)
            base_record = {
                'join_key': row['join_key'],
                'derived_value': row['derived_value'], 
                'actual_value': row['actual_value'],
                'status': row['status']
            }
            
            # For performance, limit expansion to reasonable size
            # If row_count > 1000, just add summary records
            if row_count <= 1000:
                for i in range(int(row_count)):
                    record = base_record.copy()
                    record['row_number'] = i + 1
                    record['combination_count'] = row_count
                    expanded_failures.append(record)
            else:
                # For large counts, add a single summary record
                summary_record = base_record.copy()
                summary_record['row_number'] = f"1-{int(row_count)}"
                summary_record['combination_count'] = row_count
                summary_record['note'] = f"This failure combination represents {int(row_count)} actual rows"
                expanded_failures.append(summary_record)
        
        failure_file_path = None
        if len(expanded_failures) > 0:
            # Check if expanded failures exceed 10,000 rows - skip download if so
            if len(expanded_failures) > 10000:
                failure_file_path = f"SKIPPED - {len(expanded_failures)} expanded rows exceed 10,000 limit (represents {total_fail_count} actual failures)"
            else:
                # Sanitize scenario name for filename
                sanitized_name = re.sub(r'[^\w\-_.]', '_', str(scenario_name))
                sanitized_name = re.sub(r'_+', '_', sanitized_name).strip('_')
                
                # Create filename
                filename = f"{scenario_id}_{sanitized_name}_Failures.xlsx"
                failure_file_path = os.path.join(output_dir, filename)
                
                # Create output directory if it doesn't exist
                os.makedirs(output_dir, exist_ok=True)
                
                # Save expanded failure records to Excel
                df_expanded_failures = pd.DataFrame(expanded_failures)
                with pd.ExcelWriter(failure_file_path, engine='openpyxl') as writer:
                    df_expanded_failures.to_excel(writer, sheet_name='FailedValidations', index=False)
                    
                    # Also add summary sheet with unique combinations
                    failed_records.to_excel(writer, sheet_name='UniqueCombinations', index=False)
                    
                    # Auto-adjust column widths for both sheets
                    for sheet_name in writer.sheets:
                        worksheet = writer.sheets[sheet_name]
                        for column in worksheet.columns:
                            max_length = 0
                            column_letter = column[0].column_letter
                            for cell in column:
                                try:
                                    if len(str(cell.value)) > max_length:
                                        max_length = len(str(cell.value))
                                except:
                                    pass
                            # Set a reasonable max width
                            adjusted_width = min(max_length + 2, 50)
                            worksheet.column_dimensions[column_letter].width = adjusted_width
        
        return True, f"Query executed successfully. {total_pass_count} PASS, {total_fail_count} FAIL (optimized)", total_pass_count, total_fail_count, failure_file_path
        
    except Exception as e:
        return False, f"Execution error: {str(e)}", 0, 0, None

def execute_validation_sql_and_save_failures(sql_script, client, scenario_id, scenario_name, output_dir):
    """
    Execute validation SQL and save failure records to individual Excel files
    """
    import re
    
    try:
        # Extract project from the first table reference in the SQL
        project_match = re.search(r'`([^.]+)\.[^.]+\.[^`]+`', sql_script)
        if not project_match:
            return False, "Could not extract project ID from SQL", 0, 0, None
        
        project_id = project_match.group(1)
        
        # Ensure client project is set if needed
        if client.project != project_id:
            # Switch project if needed using our helper function
            client = get_or_switch_client(project_id)
        
        # Execute the main query (not the summary) to get all results
        main_query = sql_script.split('-- Summary')[0].strip()
        
        # Execute with dry run first to validate syntax
        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)
        dry_run_job = client.query(main_query, job_config=job_config)
        
        # If dry run succeeds, execute the actual query without limit to get full results
        job_config = bigquery.QueryJobConfig(
            dry_run=False, 
            use_query_cache=False,
            maximum_bytes_billed=500 * 1024 * 1024  # 500 MB limit for full results
        )
        
        query_job = client.query(main_query, job_config=job_config)
        results = query_job.result(timeout=120)  # 2 minute timeout for full results
        
        # Convert all results to DataFrame
        all_rows = []
        for row in results:
            all_rows.append(dict(row))
        
        if not all_rows:
            return True, "Query executed but returned no results", 0, 0, None
        
        df_results = pd.DataFrame(all_rows)
        
        # Count PASS and FAIL results
        pass_count = len(df_results[df_results['status'].str.upper() == 'PASS'])
        fail_count = len(df_results[df_results['status'].str.upper() == 'FAIL'])
        
        # Filter for failed records only
        failed_records = df_results[df_results['status'].str.upper() == 'FAIL']
        
        failure_file_path = None
        if len(failed_records) > 0:
            # Check if failed records exceed 10,000 rows - skip download if so
            if len(failed_records) > 10000:
                failure_file_path = f"SKIPPED - {len(failed_records)} rows exceed 10,000 limit"
            else:
                # Sanitize scenario name for filename
                sanitized_name = re.sub(r'[^\w\-_.]', '_', str(scenario_name))
                sanitized_name = re.sub(r'_+', '_', sanitized_name).strip('_')
                
                # Create filename
                filename = f"{scenario_id}_{sanitized_name}_Failures.xlsx"
                failure_file_path = os.path.join(output_dir, filename)
                
                # Create output directory if it doesn't exist
                os.makedirs(output_dir, exist_ok=True)
                
                # Save failed records to Excel
                with pd.ExcelWriter(failure_file_path, engine='openpyxl') as writer:
                    failed_records.to_excel(writer, sheet_name='FailedValidations', index=False)
                    
                    # Auto-adjust column widths
                    worksheet = writer.sheets['FailedValidations']
                    for column in worksheet.columns:
                        max_length = 0
                        column_letter = column[0].column_letter
                        for cell in column:
                            try:
                                if len(str(cell.value)) > max_length:
                                    max_length = len(str(cell.value))
                            except:
                                pass
                        # Set a reasonable max width
                        adjusted_width = min(max_length + 2, 50)
                        worksheet.column_dimensions[column_letter].width = adjusted_width
        
        return True, f"Query executed successfully. {pass_count} PASS, {fail_count} FAIL", pass_count, fail_count, failure_file_path
        
    except Exception as e:
        return False, f"Execution error: {str(e)}", 0, 0, None



def execute_validation_sql(sql_script, client):
    """
    Execute a validation SQL script and return results
    """
    try:
        # Extract project from the first table reference in the SQL
        import re
        project_match = re.search(r'`([^.]+)\.[^.]+\.[^`]+`', sql_script)
        if not project_match:
            return False, "Could not extract project ID from SQL", None
        
        project_id = project_match.group(1)
        
        # Ensure client project is set if needed
        if client.project != project_id:
            # Switch project if needed using our helper function
            client = get_or_switch_client(project_id)
        
        # Execute the main query (not the summary)
        main_query = sql_script.split('-- Summary')[0].strip()
        
        # Execute with dry run first to validate syntax
        job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)
        dry_run_job = client.query(main_query, job_config=job_config)
        
        # If dry run succeeds, execute the actual query with limit
        job_config = bigquery.QueryJobConfig(
            dry_run=False, 
            use_query_cache=False,
            maximum_bytes_billed=100 * 1024 * 1024  # 100 MB limit
        )
        
        # Add LIMIT to prevent large result sets
        limited_query = main_query.rstrip(';') + "\nLIMIT 10;"
        
        query_job = client.query(limited_query, job_config=job_config)
        results = query_job.result(timeout=60)  # 60 second timeout
        
        # Convert results to list
        result_rows = []
        for row in results:
            result_rows.append(dict(row))
        
        return True, f"Query executed successfully. Processed {dry_run_job.total_bytes_processed} bytes", result_rows
        
    except Exception as e:
        return False, f"Execution error: {str(e)}", None

def test_bigquery_connectivity(project_id, dataset_id, client=None):
    """
    Test connectivity to BigQuery project and dataset
    """
    try:
        # Use provided client or initialize a new one
        if client is None:
            client = initialize_client(project_id)
        elif client.project != project_id:
            # Switch project if needed using our helper function
            client = get_or_switch_client(project_id)
        
        # Test basic connectivity with a simple query
        test_query = f"""
        SELECT 
            COUNT(*) as table_count,
            '{project_id}' as project_id,
            '{dataset_id}' as dataset_id,
            'connectivity_test' as status
        FROM `{project_id}.{dataset_id}.INFORMATION_SCHEMA.TABLES`
        """
        
        # Execute the query with a timeout
        query_job = client.query(test_query)
        results = query_job.result(timeout=30)  # 30 second timeout
        
        # If we get here, the connection worked
        row_count = 0
        for row in results:
            row_count = row.table_count
            break
            
        return True, f"Success - {row_count} tables found"
        
    except Exception as e:
        return False, f"Error: {str(e)}"

def main():
    st.title("File Browser and Attachment App")
    
    # File uploader widget
    uploaded_file = st.file_uploader(
        "Choose a file to attach",
        type=['csv', 'xlsx', 'xls', 'txt', 'json', 'pdf'],
        help="Select a file to upload and attach"
    )
    
    # Display file information if a file is uploaded
    if uploaded_file is not None:
        st.success(f"File successfully attached: {uploaded_file.name}")
        
        # Preview file content based on file type
        if uploaded_file.type in ['text/csv', 'application/vnd.ms-excel', 
                                 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']:
            st.write("**File Preview:**")
            try:
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)
                
                st.dataframe(df.head())
                
                # Check for required columns and find unique combinations
                required_columns = ['Source_Project_Id', 'Source_Dataset_Id', 'Target_Project_Id', 'Target_Dataset_Id']
                missing_columns = [col for col in required_columns if col not in df.columns]
                
                if missing_columns:
                    st.warning(f"⚠️ Warning: The following required columns are missing from the file: {', '.join(missing_columns)}")
                    st.info("Expected columns: Source_Project_Id, Source_Dataset_Id, Target_Project_Id, Target_Dataset_Id")
                else:
                    st.success("✅ All required columns found!")
                    
                    # Get unique combinations for connectivity testing
                    source_combinations = df[['Source_Project_Id', 'Source_Dataset_Id']].drop_duplicates().reset_index(drop=True)
                    target_combinations = df[['Target_Project_Id', 'Target_Dataset_Id']].drop_duplicates().reset_index(drop=True)
                    
                    # Connectivity Testing Section
                    st.write("**BigQuery Connectivity Testing:**")
                    
                    # Initialize BigQuery client using ADC
                    try:
                        # Get a sample project ID from the data to initialize client
                        sample_project = df['Source_Project_Id'].iloc[0] if 'Source_Project_Id' in df.columns else None
                        
                        if sample_project:
                            client = initialize_client(sample_project)
                            st.success("✅ BigQuery client initialized successfully using Application Default Credentials")
                            # Store client in session state for reuse
                            st.session_state['bq_client'] = client
                        else:
                            st.error("⚠️ Could not determine project ID for BigQuery client initialization")
                            client = None
                            st.session_state['bq_client'] = None
                    except Exception as e:
                        st.error(f"⚠️ Failed to initialize BigQuery client: {str(e)}")
                        st.info("Please ensure Application Default Credentials are set up: `gcloud auth application-default login`")
                        client = None
                        st.session_state['bq_client'] = None
                    
                    if client:
                        st.write("*Testing Source Combinations:*")
                        source_results = []
                        
                        # Test each source combination
                        for idx, row in source_combinations.iterrows():
                            project_id = row['Source_Project_Id']
                            dataset_id = row['Source_Dataset_Id']
                            
                            with st.spinner(f"Testing {project_id}.{dataset_id}..."):
                                success, message = test_bigquery_connectivity(project_id, dataset_id, client)
                                source_results.append({
                                    'Project_Id': project_id,
                                    'Dataset_Id': dataset_id,
                                    'Status': '✅ Connected' if success else '❌ Failed',
                                    'Details': message
                                })
                        
                        # Display source results
                        source_df = pd.DataFrame(source_results)
                        st.dataframe(source_df, use_container_width=True)
                        
                        st.write("*Testing Target Combinations:*")
                        target_results = []
                        
                        # Test each target combination
                        for idx, row in target_combinations.iterrows():
                            project_id = row['Target_Project_Id']
                            dataset_id = row['Target_Dataset_Id']
                            
                            with st.spinner(f"Testing {project_id}.{dataset_id}..."):
                                success, message = test_bigquery_connectivity(project_id, dataset_id, client)
                                target_results.append({
                                    'Project_Id': project_id,
                                    'Dataset_Id': dataset_id,
                                    'Status': '✅ Connected' if success else '❌ Failed',
                                    'Details': message
                                })
                        
                        # Display target results
                        target_df = pd.DataFrame(target_results)
                        st.dataframe(target_df, use_container_width=True)
                        
                        # Join Key Uniqueness Validation Section
                        st.write("**Join Key Uniqueness Validation:**")
                        
                        # Check for required columns for join key validation
                        join_key_required_columns = ['Source_Table', 'Source_Join_Key', 'Target_Table', 'Target_Join_Key']
                        join_key_missing_columns = [col for col in join_key_required_columns if col not in df.columns]
                        
                        if join_key_missing_columns:
                            st.warning(f"⚠️ Missing columns for join key validation: {', '.join(join_key_missing_columns)}")
                            st.info("Required columns: Source_Table, Source_Join_Key, Target_Table, Target_Join_Key")
                            join_key_validation_passed = False
                        else:
                            st.write("*Validating Join Key Uniqueness for All Scenarios:*")
                            
                            # Enhancement 1: Avoid repeating join key checks for identical table combinations
                            st.write("🔍 Step 1: Identifying unique table combinations...")
                            
                            # Remove non-relevant columns and get unique table combinations
                            exclude_columns = ['Scenario_ID', 'Scenario_Name', 'Description', 'Derivation_Logic']
                            df_for_uniqueness = df.copy()
                            
                            # Drop excluded columns if they exist
                            columns_to_drop = [col for col in exclude_columns if col in df_for_uniqueness.columns]
                            if columns_to_drop:
                                df_for_uniqueness = df_for_uniqueness.drop(columns=columns_to_drop)
                            
                            # Get unique combinations for join key validation
                            unique_combinations = df_for_uniqueness.drop_duplicates().reset_index(drop=True)
                            st.write(f"✅ Reduced from {len(df)} rows to {len(unique_combinations)} unique table combinations")
                            
                            # Dictionary to cache join key validation results
                            join_key_cache = {}
                            
                            # Progress bar for join key validation
                            join_progress = st.progress(0)
                            
                            st.write("🔍 Step 2: Running join key uniqueness checks on unique combinations...")
                            
                            # Run join key validation only on unique combinations
                            for idx, row in unique_combinations.iterrows():
                                # Update progress
                                progress = (idx + 1) / len(unique_combinations)
                                join_progress.progress(progress)
                                
                                # Parse join keys
                                source_join_keys = [key.strip() for key in str(row.get('Source_Join_Key', '')).split(',') if key.strip()]
                                target_join_keys = [key.strip() for key in str(row.get('Target_Join_Key', '')).split(',') if key.strip()]
                                
                                # Create cache key for this table combination
                                source_project = row.get('Source_Project_Id', '')
                                source_dataset = row.get('Source_Dataset_Id', '')
                                source_table = row.get('Source_Table', '')
                                target_project = row.get('Target_Project_Id', '')
                                target_dataset = row.get('Target_Dataset_Id', '')
                                target_table = row.get('Target_Table', '')
                                reference_table = row.get('Reference_Table', '')
                                
                                # Create cache key tuple
                                cache_key = (source_project, source_dataset, source_table, 
                                           target_project, target_dataset, target_table, 
                                           str(reference_table).strip() if reference_table else '')
                                
                                # Initialize cache result
                                cache_result = {
                                    'source_unique': False,
                                    'target_unique': False, 
                                    'reference_unique': True,  # Default True if no reference table
                                    'source_status': 'SKIPPED',
                                    'target_status': 'SKIPPED',
                                    'reference_status': 'N/A'
                                }
                                
                                try:
                                    if client and source_join_keys and target_join_keys:
                                        # Check source table join key uniqueness
                                        try:
                                            source_unique = are_join_keys_unique(client, source_project, source_dataset, source_table, source_join_keys)
                                            cache_result['source_unique'] = source_unique
                                            cache_result['source_status'] = '✅ UNIQUE' if source_unique else '❌ NOT_UNIQUE'
                                        except Exception as e:
                                            cache_result['source_status'] = f'❌ ERROR: {str(e)[:50]}...'
                                            cache_result['source_unique'] = False
                                        
                                        # Check target table join key uniqueness
                                        try:
                                            target_unique = are_join_keys_unique(client, target_project, target_dataset, target_table, target_join_keys)
                                            cache_result['target_unique'] = target_unique
                                            cache_result['target_status'] = '✅ UNIQUE' if target_unique else '❌ NOT_UNIQUE'
                                        except Exception as e:
                                            cache_result['target_status'] = f'❌ ERROR: {str(e)[:50]}...'
                                            cache_result['target_unique'] = False
                                        
                                        # Check reference table if present
                                        if reference_table and str(reference_table).strip() and str(reference_table).strip().lower() != 'nan':
                                            reference_join_keys = [key.strip() for key in str(row.get('Reference_Join_Key', '')).split(',') if key.strip()]
                                            
                                            if reference_join_keys:
                                                try:
                                                    reference_unique = are_join_keys_unique(client, source_project, source_dataset, reference_table, reference_join_keys)
                                                    cache_result['reference_unique'] = reference_unique
                                                    cache_result['reference_status'] = '✅ UNIQUE' if reference_unique else '❌ NOT_UNIQUE'
                                                except Exception as e:
                                                    cache_result['reference_status'] = f'❌ ERROR: {str(e)[:50]}...'
                                                    cache_result['reference_unique'] = False
                                        
                                    else:
                                        cache_result['source_status'] = 'No client or missing join keys'
                                        cache_result['target_status'] = 'No client or missing join keys'
                                        
                                except Exception as e:
                                    cache_result['source_status'] = f'Validation error: {str(e)[:50]}...'
                                    cache_result['target_status'] = f'Validation error: {str(e)[:50]}...'
                                
                                # Store result in cache
                                join_key_cache[cache_key] = cache_result
                            
                            # Clear progress bar
                            join_progress.empty()
                            
                            st.write("🔍 Step 3: Building validation results table using cached results...")
                            
                            # Build validation results for all original rows using cached results
                            join_key_results = []
                            join_key_validation_passed = True
                            
                            for idx, row in df.iterrows():
                                scenario_id = str(row.get('Scenario_ID', f'SC{idx+1:03d}'))
                                scenario_name = str(row.get('Scenario_Name', f'Scenario_{idx+1}'))
                                
                                # Create cache key for lookup
                                source_project = row.get('Source_Project_Id', '')
                                source_dataset = row.get('Source_Dataset_Id', '')
                                source_table = row.get('Source_Table', '')
                                target_project = row.get('Target_Project_Id', '')
                                target_dataset = row.get('Target_Dataset_Id', '')
                                target_table = row.get('Target_Table', '')
                                reference_table = row.get('Reference_Table', '')
                                
                                cache_key = (source_project, source_dataset, source_table, 
                                           target_project, target_dataset, target_table, 
                                           str(reference_table).strip() if reference_table else '')
                                
                                # Lookup cached result
                                cached_result = join_key_cache.get(cache_key, {
                                    'source_unique': False, 'target_unique': False, 'reference_unique': False,
                                    'source_status': 'NOT_FOUND', 'target_status': 'NOT_FOUND', 'reference_status': 'NOT_FOUND'
                                })
                                
                                # Create scenario result using cached data
                                scenario_result = {
                                    'Scenario_ID': scenario_id,
                                    'Scenario_Name': scenario_name,
                                    'Source_Table_Status': cached_result['source_status'],
                                    'Target_Table_Status': cached_result['target_status'],
                                    'Reference_Table_Status': cached_result['reference_status'],
                                    'Overall_Status': 'ERROR',
                                    'Details': ''
                                }
                                
                                # Set overall status based on cached results
                                if cached_result['source_unique'] and cached_result['target_unique'] and cached_result['reference_unique']:
                                    scenario_result['Overall_Status'] = '✅ ALL_UNIQUE'
                                    scenario_result['Details'] = 'All join keys are unique'
                                else:
                                    scenario_result['Overall_Status'] = '❌ VALIDATION_FAILED'
                                    scenario_result['Details'] = 'One or more join keys are not unique'
                                    join_key_validation_passed = False
                                
                                join_key_results.append(scenario_result)
                            
                            # Display join key validation results
                            join_key_df = pd.DataFrame(join_key_results)
                            st.dataframe(join_key_df, use_container_width=True)
                            
                            # Store join key validation results in session state
                            st.session_state['join_key_validation_results'] = join_key_results
                            st.session_state['join_key_validation_passed'] = join_key_validation_passed
                            st.session_state['join_key_cache'] = join_key_cache  # Cache for reuse
                            
                            st.write(f"✅ Join key validation completed using {len(join_key_cache)} unique table checks instead of {len(df)} individual checks")
                            
                            # Show summary
                            if join_key_validation_passed:
                                st.success("✅ All join key uniqueness validations passed!")
                            else:
                                st.error("❌ Some join key uniqueness validations failed. Please review the results above.")
                                st.warning("⚠️ Scenarios with failed join key validation will be skipped during SQL generation.")
                        
                    # Validation Results Generation Section
                    st.write("**Validation Results Generation:**")
                    
                    # Check for required columns for SQL generation
                    sql_required_columns = [
                        'Source_Table', 'Source_Join_Key', 'Target_Table', 'Target_Join_Key', 
                        'Target_Column', 'Derivation_Logic'
                    ]
                    sql_missing_columns = [col for col in sql_required_columns if col not in df.columns]
                    
                    if sql_missing_columns:
                        st.warning(f"⚠️ Missing columns for validation: {', '.join(sql_missing_columns)}")
                        st.info("Required columns: Source_Table, Source_Join_Key, Target_Table, Target_Join_Key, Target_Column, Derivation_Logic")
                    else:
                        st.success("✅ All validation columns found!")
                        
                        # Configuration Option for Validation Method
                        st.write("**🔧 Validation Method Configuration:**")
                        use_enhanced_framework = st.radio(
                            "Choose validation approach:",
                            options=[
                                "Enhanced Framework (Recommended)", 
                                "Original SQL-based Approach"
                            ],
                            index=0,
                            help="""
                            • **Enhanced Framework**: Avoids row-level SQL operations, uses unique combinations 
                              with pandas processing. Faster for large datasets and prevents SQL syntax errors.
                            • **Original Approach**: Traditional SQL-based validation with row-level operations.
                            """
                        )
                        
                        # Generate validation results
                        if st.button("Generate Validation Results", type="primary"):
                            
                            # Enhancement 2: Avoid redundant connectivity and join key checks
                            st.write("🚀 **Starting Validation Process...**")
                            
                            # Check if join key validation was already performed and use cached results
                            join_key_validation_passed = st.session_state.get('join_key_validation_passed', None)
                            join_key_validation_results = st.session_state.get('join_key_validation_results', [])
                            join_key_cache = st.session_state.get('join_key_cache', {})
                            
                            # Check if BigQuery client is already cached
                            client = st.session_state.get('bq_client')
                            
                            if join_key_validation_passed is not None and client is not None:
                                st.info("✅ Skipping redundant checks. Using cached connectivity and join key validation results.")
                                
                                if not join_key_validation_passed and join_key_validation_results:
                                    st.warning("⚠️ Some scenarios failed join key uniqueness validation. Only scenarios with unique join keys will be processed.")
                            else:
                                st.error("❌ Missing cached validation results. Please run the file upload and validation process first.")
                                st.stop()
                            
                            # Generate Excel file with SQL column and execution results
                            st.write("📊 **Excel File with Generated SQL and Execution Results:**")
                            
                            # Create a copy of the original dataframe
                            st.write("📋 Step 1: Preparing data structures...")
                            df_with_sql = df.copy()
                            
                            # Initialize lists for new columns
                            generated_sqls = []
                            total_passed = []
                            total_failed = []
                            overall_status = []
                            failure_files_created = []
                            st.write("✅ Data structures initialized")
                            
                            # Create output directory for failure files
                            st.write("📁 Step 2: Setting up output directories...")
                            script_dir = os.path.dirname(os.path.abspath(__file__))
                            output_dir = os.path.join(script_dir, "output", "ScenarioFailures")
                            st.write(f"✅ Output directory: {output_dir}")
                            
                            # Clean up existing output files from previous runs
                            st.write("🧹 Step 2.1: Cleaning up old output files...")
                            if os.path.exists(output_dir):
                                try:
                                    # Remove all files in the output directory
                                    for filename in os.listdir(output_dir):
                                        file_path = os.path.join(output_dir, filename)
                                        if os.path.isfile(file_path):
                                            os.remove(file_path)
                                    st.info("🧹 Cleaned up old output files from previous runs.")
                                except Exception as e:
                                    st.warning(f"⚠️ Warning: Could not clean all old files: {str(e)}")
                            else:
                                # Create directory if it doesn't exist
                                os.makedirs(output_dir, exist_ok=True)
                                st.info("📁 Created new output directory.")
                            
                            # Also clean up the main output Excel file
                            enhanced_excel_path = os.path.join(script_dir, "output", "ValidationScenarios_WithSQL.xlsx")
                            if os.path.exists(enhanced_excel_path):
                                try:
                                    os.remove(enhanced_excel_path)
                                    st.info("🧹 Removed previous validation results Excel file.")
                                except Exception as e:
                                    st.warning(f"⚠️ Warning: Could not remove previous Excel file: {str(e)}")
                            
                            # Enhancement 2: Skip redundant client initialization - already have cached client
                            st.write("🔌 Step 3: Using cached BigQuery client and validation results...")
                            st.write("✅ BigQuery client and join key validation results retrieved from cache")
                            
                            # Create a lookup dictionary for join key validation results
                            join_key_lookup = {}
                            if join_key_validation_results:
                                for result in join_key_validation_results:
                                    scenario_id = result['Scenario_ID']
                                    join_key_lookup[scenario_id] = result['Overall_Status'] == '✅ ALL_UNIQUE'
                            
                            # Process each row
                            st.write(f"🔄 Step 4: Processing {len(df)} validation scenarios with optimized SQL...")
                            st.info("🚀 Performance Enhancement: Using unique combinations approach to dramatically reduce data processing volume for large datasets.")
                            
                            # Create a progress bar and status container
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            
                            for idx, row in df.iterrows():
                                # Update progress
                                progress = (idx + 1) / len(df)
                                progress_bar.progress(progress)
                                
                                scenario_id = str(row.get('Scenario_ID', f'SC{idx+1:03d}'))
                                scenario_name = str(row.get('Scenario_Name', f'Scenario_{idx+1}'))
                                
                                status_text.write(f"🔍 Processing scenario {idx+1}/{len(df)}: {scenario_id} - {scenario_name}")
                                
                                # Check if this scenario passed join key validation (if validation was performed)
                                if join_key_lookup and not join_key_lookup.get(scenario_id, False):
                                    st.warning(f"⏭️ Skipping scenario {scenario_id}: Failed join key uniqueness validation")
                                    # Add default values for skipped scenario
                                    generated_sqls.append(f"-- Scenario skipped: Join key uniqueness validation failed")
                                    total_passed.append(0)
                                    total_failed.append(0)
                                    overall_status.append('SKIPPED_UNIQUENESS')
                                    failure_files_created.append("Skipped - Join key uniqueness failed")
                                    continue
                                
                                try:
                                    # Generate SQL with client for type detection
                                    st.write(f"� Generating SQL for scenario {scenario_id}...")
                                    
                                    # Use optimized SQL generation that works with unique combinations
                                    sql_script = generate_optimized_validation_sql(row, client)
                                    generated_sqls.append(sql_script)
                                    st.write(f"✅ Optimized SQL generated for scenario {scenario_id} (performance enhanced)")
                                    
                                    # Show debug information in an expander
                                    with st.expander(f"🔍 Enhanced Debug Info for {scenario_id} (Click to expand)", expanded=False):
                                        derivation_logic = row.get('Derivation_Logic', '')
                                        source_cols, target_cols, ref_cols = extract_required_columns_by_table(row)
                                        
                                        st.write(f"**Derivation Logic:** `{derivation_logic}`")
                                        st.write(f"**Table-Specific Column Requirements:**")
                                        st.write(f"   • Source table columns: `{sorted(source_cols) if source_cols else 'None'}`")
                                        st.write(f"   • Target table columns: `{sorted(target_cols) if target_cols else 'None'}`")
                                        if ref_cols:
                                            st.write(f"   • Reference table columns: `{sorted(ref_cols)}`")
                                        
                                        st.write(f"**Table Information:**")
                                        st.write(f"   • Source Table: `{row.get('Source_Project_Id')}.{row.get('Source_Dataset_Id')}.{row.get('Source_Table')}`")
                                        st.write(f"   • Target Table: `{row.get('Target_Project_Id')}.{row.get('Target_Dataset_Id')}.{row.get('Target_Table')}`")
                                        if row.get('Reference_Table') and str(row.get('Reference_Table')).strip().lower() != 'nan':
                                            st.write(f"   • Reference Table: `{row.get('Source_Project_Id')}.{row.get('Source_Dataset_Id')}.{row.get('Reference_Table')}`")
                                    
                                    # Execute Enhanced Validation Framework (avoids row-level operations)
                                    if client:
                                        st.write(f"🚀 Executing enhanced validation framework for scenario {scenario_id}...")
                                        success, message, pass_count, fail_count, failure_file_path = enhanced_validation_execution(
                                            row, client, scenario_id, scenario_name, output_dir
                                        )
                                        
                                        if success:
                                            st.write(f"✅ Scenario {scenario_id}: {pass_count} PASS, {fail_count} FAIL (enhanced)")
                                            total_passed.append(pass_count)
                                            total_failed.append(fail_count)
                                            # Set overall status: PASS if no failures, FAIL if any failures
                                            overall_status.append('PASS' if fail_count == 0 else 'FAIL')
                                            
                                            # Track failure file creation
                                            if failure_file_path:
                                                if failure_file_path.startswith("SKIPPED"):
                                                    st.write(f"⚠️ Scenario {scenario_id}: {failure_file_path}")
                                                    failure_files_created.append(failure_file_path)
                                                else:
                                                    relative_path = os.path.relpath(failure_file_path, script_dir)
                                                    st.write(f"📄 Failure file created: {relative_path}")
                                                    failure_files_created.append(relative_path)
                                            else:
                                                failure_files_created.append("No failures")
                                        else:
                                            st.error(f"❌ Scenario {scenario_id} execution failed: {message}")
                                            # If execution failed, set default values
                                            total_passed.append(0)
                                            total_failed.append(0)
                                            overall_status.append('EXECUTION_ERROR')
                                            failure_files_created.append("Execution failed")
                                    else:
                                        st.warning(f"⚠️ Scenario {scenario_id}: No BigQuery client available")
                                        # If no client, set default values
                                        total_passed.append(0)
                                        total_failed.append(0)
                                        overall_status.append('NO_CLIENT')
                                        failure_files_created.append("No BigQuery client")
                                        
                                except ValueError as ve:
                                    # Handle uniqueness validation errors specifically
                                    if "do not uniquely identify rows" in str(ve):
                                        st.error(f"❌ Join key uniqueness validation failed for scenario {scenario_id}: {str(ve)}")
                                        # Add default values for failed scenario
                                        generated_sqls.append(f"-- Join key uniqueness validation failed: {str(ve)}")
                                        total_passed.append(0)
                                        total_failed.append(0)
                                        overall_status.append('UNIQUENESS_ERROR')
                                        failure_files_created.append(f"Uniqueness Error: {str(ve)}")
                                    else:
                                        st.error(f"❌ Validation error for scenario {scenario_id}: {str(ve)}")
                                        # Add default values for failed scenario
                                        generated_sqls.append(f"-- Validation error: {str(ve)}")
                                        total_passed.append(0)
                                        total_failed.append(0)
                                        overall_status.append('VALIDATION_ERROR')
                                        failure_files_created.append(f"Validation Error: {str(ve)}")
                                except Exception as e:
                                    st.error(f"❌ Error processing scenario {scenario_id}: {str(e)}")
                                    # Add default values for failed scenario
                                    generated_sqls.append(f"-- Error generating SQL: {str(e)}")
                                    total_passed.append(0)
                                    total_failed.append(0)
                                    overall_status.append('ERROR')
                                    failure_files_created.append(f"Error: {str(e)}")
                            
                            # Clear progress indicators
                            progress_bar.empty()
                            status_text.empty()
                            
                            st.write("💾 Step 5: Saving results to Excel file...")
                            
                            # Add the new columns to dataframe
                            df_with_sql['Generated_SQL'] = generated_sqls
                            df_with_sql['Total_Passed'] = total_passed
                            df_with_sql['Total_Failed'] = total_failed
                            df_with_sql['Overall_Status'] = overall_status
                            df_with_sql['Failure_File_Path'] = failure_files_created
                            
                            # Save enhanced Excel file to output directory
                            enhanced_excel_path = os.path.join(script_dir, "output", "ValidationScenarios_WithSQL.xlsx")
                            os.makedirs(os.path.dirname(enhanced_excel_path), exist_ok=True)
                            st.write(f"📁 Creating Excel file: {enhanced_excel_path}")
                            
                            # Write to Excel file in output directory
                            with pd.ExcelWriter(enhanced_excel_path, engine='openpyxl') as writer:
                                st.write("📝 Writing Results sheet...")
                                # Create Results sheet with specified columns in order (FIRST TAB)
                                results_columns = [
                                    'Scenario_ID', 'Scenario_Name', 'Description', 'Total_Failed', 
                                    'Total_Passed', 'Overall_Status', 'Failure_File_Path', 'Generated_SQL'
                                ]
                                
                                # Filter DataFrame to include only the specified columns that exist
                                available_columns = [col for col in results_columns if col in df_with_sql.columns]
                                df_results = df_with_sql[available_columns].copy()
                                
                                # Write Results sheet first
                                df_results.to_excel(writer, sheet_name='Results', index=False)
                                
                                # Auto-adjust column widths for Results sheet
                                results_worksheet = writer.sheets['Results']
                                for column in results_worksheet.columns:
                                    max_length = 0
                                    column_letter = column[0].column_letter
                                    for cell in column:
                                        try:
                                            if len(str(cell.value)) > max_length:
                                                max_length = len(str(cell.value))
                                        except:
                                            pass
                                    # Set a reasonable max width
                                    adjusted_width = min(max_length + 2, 50)
                                    results_worksheet.column_dimensions[column_letter].width = adjusted_width
                                
                                st.write("📝 Writing ValidationScenarios sheet...")
                                # Create ValidationScenarios sheet excluding execution result columns (SECOND TAB)
                                validation_exclude_columns = ['Generated_SQL', 'Total_Passed', 'Total_Failed', 'Overall_Status', 'Failure_File_Path']
                                df_validation = df_with_sql.drop(columns=[col for col in validation_exclude_columns if col in df_with_sql.columns])
                                
                                # Write filtered validation results to ValidationScenarios sheet
                                df_validation.to_excel(writer, sheet_name='ValidationScenarios', index=False)
                                
                                # Auto-adjust column widths for ValidationScenarios sheet
                                worksheet = writer.sheets['ValidationScenarios']
                                for column in worksheet.columns:
                                    max_length = 0
                                    column_letter = column[0].column_letter
                                    for cell in column:
                                        try:
                                            if len(str(cell.value)) > max_length:
                                                max_length = len(str(cell.value))
                                        except:
                                            pass
                                    # Set a reasonable max width
                                    adjusted_width = min(max_length + 2, 50)
                                    worksheet.column_dimensions[column_letter].width = adjusted_width
                            
                            st.write("✅ Excel file created successfully!")
                            
                            # Show summary statistics
                            st.write("📊 **Final Execution Summary:**")
                            summary_col1, summary_col2, summary_col3, summary_col4 = st.columns(4)
                            
                            with summary_col1:
                                total_scenarios = len(df_with_sql)
                                st.metric("Total Scenarios", total_scenarios)
                            
                            with summary_col2:
                                passed_scenarios = len(df_with_sql[df_with_sql['Overall_Status'] == 'PASS'])
                                st.metric("Scenarios Passed", passed_scenarios)
                            
                            with summary_col3:
                                failed_scenarios = len(df_with_sql[df_with_sql['Overall_Status'] == 'FAIL'])
                                st.metric("Scenarios Failed", failed_scenarios)
                            
                            with summary_col4:
                                failure_files_count = len([f for f in failure_files_created if f not in ["No failures", "Execution failed", "No BigQuery client"] and not f.startswith("SKIPPED")])
                                st.metric("Failure Files Created", failure_files_count)
                            
                            # Show information about all created files
                            st.write("**Created Files:**")
                            enhanced_excel_relative_path = os.path.relpath(enhanced_excel_path, script_dir)
                            st.write(f"📄 **Main Report:** {enhanced_excel_relative_path}")
                            
                            # Show created failure files
                            if failure_files_count > 0:
                                st.write(f"📋 **Individual Failure Files ({failure_files_count}):**")
                                failure_files_list = [f for f in failure_files_created if f not in ["No failures", "Execution failed", "No BigQuery client"] and not f.startswith("SKIPPED")]
                                for file_path in failure_files_list:
                                    st.write(f"   • {file_path}")
                            
                            # Show skipped files due to size limit
                            skipped_files = [f for f in failure_files_created if f.startswith("SKIPPED")]
                            if skipped_files:
                                st.write(f"⚠️ **Skipped Files Due to Size Limit (>10,000 rows):**")
                                for skipped_file in skipped_files:
                                    st.write(f"   • {skipped_file}")
                            
                            st.info(f"💡 All validation files have been saved to the `output/` directory.")
                            
                            st.success("✅ Validation completed! All Excel files with execution results and individual failure files have been created!")
                        
            except Exception as e:
                st.error(f"Error reading file: {e}")
        
        elif uploaded_file.type == 'text/plain':
            st.write("**File Content:**")
            content = uploaded_file.read().decode('utf-8')
            st.text_area("File content", content, height=200)
        
        elif uploaded_file.type == 'application/json':
            st.write("**JSON Content:**")
            import json
            content = uploaded_file.read().decode('utf-8')
            try:
                json_data = json.loads(content)
                st.json(json_data)
            except Exception as e:
                st.error(f"Error parsing JSON: {e}")
    
    else:
        st.info("Please select a file to attach using the file uploader above.")

if __name__ == "__main__":
    main()
