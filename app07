import streamlit as st
import pandas as pd
import os

# Configuration: Hard stop behavior for join key uniqueness failures
# Set to True to halt validation immediately when join key uniqueness fails
# Set to False to skip failing scenarios and continue with valid ones
HARD_STOP_ON_UNIQUENESS = True

def initialize_client(project_id):
    """Return an authenticated bigquery.Client using ADC. Keep optional proxy lines commented."""
    import os, google.auth
    from google.cloud import bigquery
    # Optional (corp) proxy ‚Äì leave commented unless needed:
    # if 'prod' in project_id:
    #     os.environ["HTTP_PROXY"] = "googleapis:0000"
    #     os.environ["HTTPS_PROXY"] = "googleapis:0000"
    # elif 'dev' in project_id:
    #     os.environ["HTTP_PROXY"] = "googleapis:0000"
    #     os.environ["HTTPS_PROXY"] = "googleapis:0000"
    credentials, _ = google.auth.default()
    return bigquery.Client(credentials=credentials, project=project_id)

def get_or_switch_client(client, project_id):
    """Return the same client if client.project == project_id else returns initialize_client(project_id)."""
    if client and client.project == project_id:
        return client
    return initialize_client(project_id)



def get_column_data_type(client, target_project, target_dataset, target_table, target_column):
    """
    Query BigQuery INFORMATION_SCHEMA to get the data type of a target column
    """
    try:
        # Switch client if needed for different project
        project_client = get_or_switch_client(client, target_project)
        
        type_query = f"""
        SELECT data_type
        FROM `{target_project}.{target_dataset}.INFORMATION_SCHEMA.COLUMNS`
        WHERE table_name = '{target_table}' AND column_name = '{target_column}'
        """
        
        query_job = project_client.query(type_query)
        results = query_job.result(timeout=30)
        
        for row in results:
            return row.data_type
            
        return None  # Column not found
        
    except Exception as e:
        print(f"Error getting column type: {str(e)}")
        return None

def are_join_keys_unique(client, project, dataset, table, join_keys):
    """
    Verify that the provided join keys uniquely identify rows in a BigQuery table.
    
    Args:
        client: BigQuery client
        project: Project ID
        dataset: Dataset ID
        table: Table name
        join_keys: List of column names that should form a unique key
        
    Returns:
        bool: True if join keys are unique, False otherwise
        
    Raises:
        Exception: If query execution fails
    """
    try:
        # Switch client if needed for different project
        project_client = get_or_switch_client(client, project)
        
        # Construct CONCAT expression for composite keys
        if len(join_keys) == 1:
            concat_expr = f"CAST({join_keys[0]} AS STRING)"
        else:
            cast_keys = [f"CAST({key} AS STRING)" for key in join_keys]
            pipe_separator = '"|"'
            join_parts = f', {pipe_separator}, '.join(cast_keys)
            concat_expr = f"CONCAT({join_parts})"
        
        # Query to check uniqueness
        uniqueness_query = f"""
        SELECT 
            COUNT(*) as total_rows,
            COUNT(DISTINCT {concat_expr}) as distinct_keys
        FROM `{project}.{dataset}.{table}`
        WHERE {' AND '.join([f'{key} IS NOT NULL' for key in join_keys])}
        """
        
        query_job = project_client.query(uniqueness_query)
        results = query_job.result(timeout=30)
        
        for row in results:
            total_rows = row.total_rows
            distinct_keys = row.distinct_keys
            return total_rows == distinct_keys
            
        return False  # No results returned
        
    except Exception as e:
        raise Exception(f"Error checking uniqueness for join keys {join_keys} in `{project}.{dataset}.{table}`: {str(e)}")




def get_fully_qualified_table_name(project, dataset, table):
    """
    Helper function to ensure fully qualified table names in BigQuery.
    Returns the properly formatted table reference.
    """
    if not project or not dataset or not table:
        raise ValueError(f"Missing table components: project='{project}', dataset='{dataset}', table='{table}'")
    
    # Clean and validate components
    project = str(project).strip()
    dataset = str(dataset).strip() 
    table = str(table).strip()
    
    if not project or not dataset or not table or 'nan' in [project.lower(), dataset.lower(), table.lower()]:
        raise ValueError(f"Invalid table components: project='{project}', dataset='{dataset}', table='{table}'")
    
    return f"`{project}.{dataset}.{table}`"

def extract_required_columns_by_table(row):
    """
    Enhanced column extraction that separates required columns by their respective tables.
    
    Column Assignment Logic:
    - SOURCE TABLE: Derivation_Logic columns, Source_Join_Key, Reference_Lookup_Column
    - TARGET TABLE: Target_Column, Target_Join_Key  
    - REFERENCE TABLE: Reference_Join_Key
    
    Note: Reference_Lookup_Column belongs to SOURCE table (contains values to lookup in reference table)
          Reference_Join_Key belongs to REFERENCE table (the lookup key in reference table)
    
    Returns three sets: required_source_columns, required_target_columns, required_reference_columns
    """
    import re
    
    # Get all relevant fields
    derivation_logic = row.get('Derivation_Logic', '')
    target_column = row.get('Target_Column', '')
    source_join_key = row.get('Source_Join_Key', '')
    target_join_key = row.get('Target_Join_Key', '')
    reference_join_key = row.get('Reference_Join_Key', '')
    reference_lookup_column = row.get('Reference_Lookup_Column', '')
    reference_table = row.get('Reference_Table', '')
    
    # Initialize column sets
    required_source_columns = set()
    required_target_columns = set()
    required_reference_columns = set()
    
    # SQL keywords to exclude from column extraction
    excluded_keywords = {
        'CASE', 'WHEN', 'THEN', 'ELSE', 'END', 'AND', 'OR', 'NOT', 'IS', 'NULL', 'TRUE', 'FALSE',
        'CAST', 'SAFE_CAST', 'COALESCE', 'CONCAT', 'TRIM', 'UPPER', 'LOWER', 'STRING', 'INT64',
        'FLOAT64', 'BOOL', 'NUMERIC', 'DATE', 'DATETIME', 'TIMESTAMP', 'IF', 'IFNULL', 'NULLIF',
        'COUNT', 'SUM', 'AVG', 'MAX', 'MIN', 'ABS', 'ROUND', 'PREMIUM', 'STANDARD', 'ACTIVE', 'INACTIVE',
        'SELECT', 'FROM', 'WHERE', 'JOIN', 'LEFT', 'RIGHT', 'INNER', 'OUTER', 'ON', 'GROUP', 'BY',
        'ORDER', 'HAVING', 'DISTINCT', 'AS', 'WITH', 'UNION', 'ALL', 'EXISTS', 'IN', 'LIKE', 'BETWEEN'
    }
    
    # 1. Extract derivation logic columns ‚Üí SOURCE TABLE
    if derivation_logic and str(derivation_logic).strip().lower() not in ['', 'nan', 'none']:
        derivation_str = str(derivation_logic).strip()
        
        # Get table names to exclude them from column extraction
        source_table_name = row.get('Source_Table', '')
        target_table_name = row.get('Target_Table', '')
        reference_table_name = row.get('Reference_Table', '')
        table_names_to_exclude = {source_table_name, target_table_name, reference_table_name}
        table_names_to_exclude = {name for name in table_names_to_exclude if name and str(name).strip().lower() not in ['', 'nan', 'none']}
        
        # Enhanced regex patterns to extract column references
        column_patterns = [
            r'\b[a-zA-Z_][a-zA-Z0-9_]*\.[a-zA-Z_][a-zA-Z0-9_]*\b',  # table.column format
            r'\b[a-zA-Z_][a-zA-Z0-9_]*\b(?!\s*\.)'  # standalone identifiers (not followed by a dot)
        ]
        
        all_matches = set()
        for pattern in column_patterns:
            matches = re.findall(pattern, derivation_str)
            for match in matches:
                if '.' in match:
                    # Extract column name from table.column format
                    table_part, col_part = match.split('.', 1)
                    all_matches.add(col_part)
                else:
                    # Standalone identifier
                    all_matches.add(match)
        
        # Filter out keywords, table names, and invalid identifiers
        for col in all_matches:
            if (col.upper() not in excluded_keywords and 
                len(col) > 1 and 
                not col.isdigit() and
                col not in table_names_to_exclude):
                required_source_columns.add(col)
    
    # 2. Add Source_Join_Key and Reference_Lookup_Column ‚Üí SOURCE TABLE
    if source_join_key and str(source_join_key).strip().lower() not in ['', 'nan', 'none']:
        keys = [key.strip() for key in str(source_join_key).split(',') if key.strip()]
        required_source_columns.update(keys)
    
    # Reference_Lookup_Column is part of SOURCE table (used to lookup values in reference table)
    if reference_lookup_column and str(reference_lookup_column).strip().lower() not in ['', 'nan', 'none']:
        keys = [key.strip() for key in str(reference_lookup_column).split(',') if key.strip()]
        required_source_columns.update(keys)
    
    # 3. Add Target_Column and Target_Join_Key ‚Üí TARGET TABLE
    if target_column and str(target_column).strip().lower() not in ['', 'nan', 'none']:
        required_target_columns.add(str(target_column).strip())
    
    if target_join_key and str(target_join_key).strip().lower() not in ['', 'nan', 'none']:
        keys = [key.strip() for key in str(target_join_key).split(',') if key.strip()]
        required_target_columns.update(keys)
    
    # 4. Add Reference columns ‚Üí REFERENCE TABLE (if applicable)
    if reference_table and str(reference_table).strip().lower() not in ['', 'nan', 'none']:
        if reference_join_key and str(reference_join_key).strip().lower() not in ['', 'nan', 'none']:
            keys = [key.strip() for key in str(reference_join_key).split(',') if key.strip()]
            required_reference_columns.update(keys)
    
    # Clean and validate all column sets
    required_source_columns = {col for col in required_source_columns if col and str(col).strip() and str(col).strip().lower() != 'nan'}
    required_target_columns = {col for col in required_target_columns if col and str(col).strip() and str(col).strip().lower() != 'nan'}
    required_reference_columns = {col for col in required_reference_columns if col and str(col).strip() and str(col).strip().lower() != 'nan'}
    
    return required_source_columns, required_target_columns, required_reference_columns

def extract_all_required_columns(row):
    """
    Backward compatibility function - returns unified set of all required columns.
    """
    source_cols, target_cols, ref_cols = extract_required_columns_by_table(row)
    return source_cols.union(target_cols).union(ref_cols)

def validate_columns_exist_in_table(client, project, dataset, table, required_columns):
    """
    Validation check to ensure all required columns exist in the table schema.
    Returns (exists, missing_columns)
    """
    try:
        table_ref = f"{project}.{dataset}.{table}"
        table = client.get_table(table_ref)
        
        # Get all column names from schema
        existing_columns = {field.name for field in table.schema}
        
        # Check which required columns are missing
        missing_columns = required_columns - existing_columns
        
        return len(missing_columns) == 0, missing_columns
        
    except Exception as e:
        st.error(f"‚ùå Error validating table schema: {e}")
        return False, required_columns

def validate_all_tables_columns(client, row, required_source_columns, required_target_columns, required_reference_columns):
    """
    Enhanced validation that checks column existence in their respective tables.
    Returns (all_valid, validation_results) where validation_results contains detailed info.
    """
    validation_results = {
        'source': {'valid': True, 'missing': set(), 'table': ''},
        'target': {'valid': True, 'missing': set(), 'table': ''},
        'reference': {'valid': True, 'missing': set(), 'table': ''}
    }
    
    # Validate Source Table Columns
    if required_source_columns:
        source_project = row.get('Source_Project_Id', '')
        source_dataset = row.get('Source_Dataset_Id', '')
        source_table = row.get('Source_Table', '')
        validation_results['source']['table'] = f"{source_project}.{source_dataset}.{source_table}"
        
        source_valid, source_missing = validate_columns_exist_in_table(
            client, source_project, source_dataset, source_table, required_source_columns)
        validation_results['source']['valid'] = source_valid
        validation_results['source']['missing'] = source_missing
    
    # Validate Target Table Columns
    if required_target_columns:
        target_project = row.get('Target_Project_Id', '')
        target_dataset = row.get('Target_Dataset_Id', '')
        target_table = row.get('Target_Table', '')
        validation_results['target']['table'] = f"{target_project}.{target_dataset}.{target_table}"
        
        target_valid, target_missing = validate_columns_exist_in_table(
            client, target_project, target_dataset, target_table, required_target_columns)
        validation_results['target']['valid'] = target_valid
        validation_results['target']['missing'] = target_missing
    
    # Validate Reference Table Columns (if applicable)
    if required_reference_columns:
        reference_table = row.get('Reference_Table', '')
        if reference_table and str(reference_table).strip().lower() not in ['', 'nan', 'none']:
            # Reference table is typically in the same project/dataset as source
            ref_project = row.get('Source_Project_Id', '')
            ref_dataset = row.get('Source_Dataset_Id', '')
            validation_results['reference']['table'] = f"{ref_project}.{ref_dataset}.{reference_table}"
            
            ref_valid, ref_missing = validate_columns_exist_in_table(
                client, ref_project, ref_dataset, reference_table, required_reference_columns)
            validation_results['reference']['valid'] = ref_valid
            validation_results['reference']['missing'] = ref_missing
    
    # Check if all validations passed
    all_valid = (validation_results['source']['valid'] and 
                 validation_results['target']['valid'] and 
                 validation_results['reference']['valid'])
    
    return all_valid, validation_results

def build_unique_combinations_query(project, dataset, table, required_columns):
    """
    STEP 2: Build Unique Source Row Combinations
    Construct BigQuery SQL to get unique combinations with row counts.
    """
    if not required_columns:
        raise ValueError("No required columns provided for unique combinations query")
    
    # Build fully qualified table name
    table_fq = get_fully_qualified_table_name(project, dataset, table)
    
    # Sort columns for consistent ordering
    column_list = sorted(required_columns)
    column_select = ", ".join(column_list)
    
    query = f"""
    SELECT {column_select}, COUNT(*) AS row_count
    FROM {table_fq}
    GROUP BY {column_select}
    """
    
    return query

def apply_derivation_logic_in_python(df, derivation_logic):
    """
    STEP 3: Apply Derivation Logic in Python
    Use pandas operations to compute derived values on unique combinations.
    Handles common SQL patterns and converts them to pandas-compatible operations.
    """
    try:
        if not derivation_logic or str(derivation_logic).strip().lower() in ['', 'nan', 'none']:
            raise ValueError("Empty derivation logic")
        
        # Clean up the derivation logic
        cleaned_logic = str(derivation_logic).strip()
        
        # Handle common SQL patterns for pandas compatibility
        
        # 1. Handle COALESCE function
        if 'COALESCE' in cleaned_logic.upper():
            import re
            # Pattern for COALESCE(col1, col2, ...)
            coalesce_pattern = r'COALESCE\s*\(\s*([^)]+)\s*\)'
            
            def convert_coalesce(match):
                args = [arg.strip() for arg in match.group(1).split(',')]
                if len(args) == 2:
                    return f"{args[0]}.fillna({args[1]})"
                elif len(args) > 2:
                    # Chain fillna for multiple arguments
                    result = args[0]
                    for arg in args[1:]:
                        result = f"{result}.fillna({arg})"
                    return result
                else:
                    return args[0]
            
            cleaned_logic = re.sub(coalesce_pattern, convert_coalesce, cleaned_logic, flags=re.IGNORECASE)
        
        # 2. Handle CASE WHEN statements (simplified)
        if 'CASE' in cleaned_logic.upper() and 'WHEN' in cleaned_logic.upper():
            # For simple CASE WHEN col = value THEN 'result' ELSE 'default' END
            # Convert to pandas where condition
            case_pattern = r'CASE\s+WHEN\s+([^=]+)\s*=\s*([^T]+)\s+THEN\s+[\'"]([^\'"]+)[\'"]\s+ELSE\s+[\'"]([^\'"]+)[\'"]\s+END'
            
            def convert_case(match):
                col, value, then_val, else_val = match.groups()
                col = col.strip()
                value = value.strip()
                return f"'{then_val}' if ({col} == {value}) else '{else_val}'"
            
            import re
            if re.search(case_pattern, cleaned_logic, re.IGNORECASE):
                cleaned_logic = re.sub(case_pattern, convert_case, cleaned_logic, flags=re.IGNORECASE)
        
        # 3. Handle simple column references and string concatenation
        # If it's just a simple column name, use it directly
        if cleaned_logic in df.columns:
            df['derived_value'] = df[cleaned_logic]
            return df, True, "Success - simple column reference"
        
        # 4. Try to evaluate the expression
        try:
            df['derived_value'] = df.eval(cleaned_logic)
            return df, True, "Success - pandas eval"
        except:
            # Fallback: try as a simple assignment
            try:
                # Check if it's a simple mathematical operation on columns
                df['derived_value'] = eval(cleaned_logic, {'df': df, '__builtins__': {}})
                return df, True, "Success - simple evaluation"
            except:
                # Final fallback: create derived values using column operations
                available_cols = [col for col in df.columns if col in cleaned_logic]
                if available_cols:
                    # Use first available column as fallback
                    df['derived_value'] = df[available_cols[0]]
                    return df, True, f"Fallback - using column {available_cols[0]}"
                else:
                    df['derived_value'] = cleaned_logic  # Use literal value
                    return df, True, "Fallback - literal value"
        
    except Exception as e:
        # Ultimate fallback: create a None column
        df['derived_value'] = None
        return df, False, f"Failed to apply derivation logic: {str(e)}"

def fetch_target_data_and_join(client, df_source, row):
    """
    STEP 4: Join with Target Table
    Fetch target data and join with processed source DataFrame.
    """
    try:
        # Extract target table info
        target_project = row.get('Target_Project_Id', '')
        target_dataset = row.get('Target_Dataset_Id', '')
        target_table = row.get('Target_Table', '')
        target_column = row.get('Target_Column', '')
        target_join_key = row.get('Target_Join_Key', '')
        source_join_key = row.get('Source_Join_Key', '')
        
        # Parse join keys
        target_join_keys = [key.strip() for key in str(target_join_key).split(',') if key.strip()]
        source_join_keys = [key.strip() for key in str(source_join_key).split(',') if key.strip()]
        
        # Build target query
        target_table_fq = get_fully_qualified_table_name(target_project, target_dataset, target_table)
        target_columns = target_join_keys + [target_column]
        target_column_select = ", ".join(set(target_columns))  # Remove duplicates
        
        target_query = f"""
        SELECT {target_column_select}
        FROM {target_table_fq}
        WHERE {target_column} IS NOT NULL
        """
        
        # Execute target query
        df_target = client.query(target_query).to_dataframe()
        
        # Perform join
        if len(source_join_keys) == len(target_join_keys):
            # Create join condition
            left_on = source_join_keys
            right_on = target_join_keys
            
            df_joined = df_source.merge(
                df_target, 
                left_on=left_on, 
                right_on=right_on, 
                how='left',
                suffixes=('', '_target')
            )
            
            # Rename target column to standard name
            if target_column in df_joined.columns:
                df_joined['actual_value'] = df_joined[target_column]
            else:
                df_joined['actual_value'] = None
                
            return df_joined, True, "Join successful"
        else:
            return df_source, False, f"Join key count mismatch: source {len(source_join_keys)}, target {len(target_join_keys)}"
            
    except Exception as e:
        return df_source, False, f"Failed to join with target table: {str(e)}"

def calculate_validation_results(df):
    """
    STEP 5: Calculate Validation Results with Row Count Scaling
    Compare derived vs actual values and multiply by row_count.
    """
    try:
        # Handle missing derived_value or actual_value
        df['derived_value'] = df['derived_value'].fillna('')
        df['actual_value'] = df['actual_value'].fillna('')
        
        # Perform comparison (handle different data types)
        df['status'] = df.apply(lambda row: 
            str(row['derived_value']).strip() == str(row['actual_value']).strip() 
            if pd.notna(row['derived_value']) and pd.notna(row['actual_value']) 
            else False, axis=1)
        
        # Calculate scaled counts
        df['pass_count'] = df['status'].astype(int) * df['row_count']
        df['fail_count'] = (~df['status']).astype(int) * df['row_count']
        
        # Calculate totals
        total_pass = df['pass_count'].sum()
        total_fail = df['fail_count'].sum()
        
        return df, total_pass, total_fail
        
    except Exception as e:
        st.error(f"‚ùå Error calculating validation results: {e}")
        return df, 0, 0

def enhanced_validation_execution(row, client, scenario_id, scenario_name, output_dir):
    """
    Main function implementing the enhanced validation framework.
    Processes validation using unique combinations approach to avoid row-level operations.
    Uses table-specific column validation to prevent false errors.
    """
    try:
        # STEP 1: Extract Required Columns by Table
        st.write(f"üîç Step 1: Extracting required columns by table for {scenario_id}...")
        required_source_columns, required_target_columns, required_reference_columns = extract_required_columns_by_table(row)
        
        if not required_source_columns and not required_target_columns:
            return False, "No required columns found in any table", 0, 0, None
        
        # Display column breakdown
        st.write(f"üìã **Column Breakdown:**")
        st.write(f"   ‚Ä¢ Source table columns: {sorted(required_source_columns) if required_source_columns else 'None'}")
        st.write(f"   ‚Ä¢ Target table columns: {sorted(required_target_columns) if required_target_columns else 'None'}")
        if required_reference_columns:
            st.write(f"   ‚Ä¢ Reference table columns: {sorted(required_reference_columns)}")
        
        # STEP 1.5: Enhanced Table-Specific Column Validation
        st.write(f"‚úÖ Step 1.5: Validating columns exist in their respective tables...")
        all_valid, validation_results = validate_all_tables_columns(
            client, row, required_source_columns, required_target_columns, required_reference_columns)
        
        if not all_valid:
            error_details = []
            for table_type, result in validation_results.items():
                if not result['valid'] and result['missing']:
                    error_details.append(f"{table_type.title()} table ({result['table']}) missing columns: {sorted(result['missing'])}")
            
            error_msg = "Column validation failed:\n" + "\n".join(error_details)
            st.error(f"‚ùå {error_msg}")
            return False, error_msg, 0, 0, None
        
        st.success(f"‚úÖ All required columns validated in their respective tables!")
        
        # Get source table info
        source_project = row.get('Source_Project_Id', '')
        source_dataset = row.get('Source_Dataset_Id', '')
        source_table = row.get('Source_Table', '')
        derivation_logic = row.get('Derivation_Logic', '')
        
        # STEP 2: Build Unique Source Row Combinations (only source columns)
        st.write(f"üî® Step 2: Building unique combinations query for source table...")
        if not required_source_columns:
            return False, "No source columns found for query generation", 0, 0, None
            
        unique_query = build_unique_combinations_query(
            source_project, source_dataset, source_table, required_source_columns)
        
        st.write(f"üìù Executing unique combinations query...")
        df_source = client.query(unique_query).to_dataframe()
        
        if df_source.empty:
            return False, "No data returned from source query", 0, 0, None
        
        st.write(f"üìä Retrieved {len(df_source)} unique combinations covering {df_source['row_count'].sum()} total rows")
        
        # STEP 3: Apply Derivation Logic in Python
        st.write(f"‚öôÔ∏è Step 3: Applying derivation logic...")
        df_source, derivation_success, derivation_message = apply_derivation_logic_in_python(df_source, derivation_logic)
        
        if not derivation_success:
            st.warning(f"‚ö†Ô∏è Derivation logic issue: {derivation_message}")
        
        # STEP 4: Join with Target Table
        st.write(f"üîó Step 4: Joining with target table...")
        df_joined, join_success, join_message = fetch_target_data_and_join(client, df_source, row)
        
        if not join_success:
            st.warning(f"‚ö†Ô∏è Join issue: {join_message}")
        
        # STEP 5: Calculate Validation Results
        st.write(f"üßÆ Step 5: Calculating validation results...")
        df_final, total_pass, total_fail = calculate_validation_results(df_joined)
        
        # STEP 6: Save Results
        st.write(f"üíæ Step 6: Saving results...")
        failure_file_path = None
        
        if total_fail > 0:
            # Save failure details
            df_failures = df_final[df_final['fail_count'] > 0].copy()
            
            # Create failure file
            failure_filename = f"{scenario_id}_{scenario_name.replace(' ', '_')}_Failures.xlsx"
            failure_file_path = os.path.join(output_dir, "ScenarioFailures", failure_filename)
            
            # Ensure directory exists
            os.makedirs(os.path.dirname(failure_file_path), exist_ok=True)
            
            # Save to Excel
            df_failures.to_excel(failure_file_path, index=False)
            st.write(f"üíæ Saved failure details: {failure_file_path}")
        
        success_message = f"Enhanced validation completed: {total_pass} passed, {total_fail} failed"
        st.success(f"‚úÖ {success_message}")
        
        return True, success_message, total_pass, total_fail, failure_file_path
        
    except Exception as e:
        error_msg = f"Enhanced validation failed: {str(e)}"
        st.error(f"‚ùå {error_msg}")
        return False, error_msg, 0, 0, None

def test_bigquery_connectivity(project_id, dataset_id, client=None):
    """
    Test connectivity to BigQuery project and dataset
    """
    """
    Test connectivity to BigQuery project and dataset
    """
    try:
        # Use provided client or initialize a new one
        if client is None:
            client = initialize_client(project_id)
        elif client.project != project_id:
            # Switch project if needed using our helper function
            client = get_or_switch_client(project_id)
        
        # Test basic connectivity with a simple query
        test_query = f"""
        SELECT 
            COUNT(*) as table_count,
            '{project_id}' as project_id,
            '{dataset_id}' as dataset_id,
            'connectivity_test' as status
        FROM `{project_id}.{dataset_id}.INFORMATION_SCHEMA.TABLES`
        """
        
        # Execute the query with a timeout
        query_job = client.query(test_query)
        results = query_job.result(timeout=30)  # 30 second timeout
        
        # If we get here, the connection worked
        row_count = 0
        for row in results:
            row_count = row.table_count
            break
            
        return True, f"Success - {row_count} tables found"
        
    except Exception as e:
        return False, f"Error: {str(e)}"

def main():
    st.title("Data Validation Framework - Enhanced Processing")
    
    # File uploader widget
    uploaded_file = st.file_uploader(
        "Choose a file to attach",
        type=['csv', 'xlsx', 'xls', 'txt', 'json', 'pdf'],
        help="Select a file to upload and attach"
    )
    
    # Display file information if a file is uploaded
    if uploaded_file is not None:
        st.success(f"File successfully attached: {uploaded_file.name}")
        
        # Preview file content based on file type
        if uploaded_file.type in ['text/csv', 'application/vnd.ms-excel', 
                                 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet']:
            st.write("**File Preview:**")
            try:
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)
                
                st.dataframe(df.head())
                
                # Check for required columns and find unique combinations
                required_columns = ['Source_Project_Id', 'Source_Dataset_Id', 'Target_Project_Id', 'Target_Dataset_Id']
                missing_columns = [col for col in required_columns if col not in df.columns]
                
                if missing_columns:
                    st.warning(f"‚ö†Ô∏è Warning: The following required columns are missing from the file: {', '.join(missing_columns)}")
                    st.info("Expected columns: Source_Project_Id, Source_Dataset_Id, Target_Project_Id, Target_Dataset_Id")
                else:
                    st.success("‚úÖ All required columns found!")
                    
                    # Store the DataFrame in session state for later use
                    st.session_state['uploaded_df'] = df
                    st.session_state['uploaded_file_name'] = uploaded_file.name
                        
                    # Validation Results Generation Section
                    st.write("**Validation Results Generation:**")
                    
                    # Check for required columns for Enhanced Framework validation
                    validation_required_columns = [
                        'Source_Table', 'Source_Join_Key', 'Target_Table', 'Target_Join_Key', 
                        'Target_Column', 'Derivation_Logic'
                    ]
                    validation_missing_columns = [col for col in validation_required_columns if col not in df.columns]
                    
                    if validation_missing_columns:
                        st.warning(f"‚ö†Ô∏è Missing columns for validation: {', '.join(validation_missing_columns)}")
                        st.info("Required columns: Source_Table, Source_Join_Key, Target_Table, Target_Join_Key, Target_Column, Derivation_Logic")
                    else:
                        st.success("‚úÖ All validation columns found!")
                        
                        # Enhanced Framework is the only validation method
                        st.write("**üîß Validation Method:**")
                        st.info("‚úÖ Using Enhanced Framework - Optimized validation with unique combinations approach for better performance")
                        
                        # Generate validation results
                        if st.button("Execute Enhanced Validation", type="primary"):
                            
                            # Get DataFrame from session state
                            df = st.session_state.get('uploaded_df')
                            if df is None:
                                st.error("‚ùå No uploaded data found. Please refresh and upload the file again.")
                                st.stop()
                            
                            st.write("üöÄ **Starting Validation Process...**")
                            
                            # Step 1: BigQuery Connectivity Testing
                            st.write("**Step 1: BigQuery Connectivity Testing**")
                            
                            # Get unique combinations for connectivity testing
                            source_combinations = df[['Source_Project_Id', 'Source_Dataset_Id']].drop_duplicates().reset_index(drop=True)
                            target_combinations = df[['Target_Project_Id', 'Target_Dataset_Id']].drop_duplicates().reset_index(drop=True)
                            
                            # Initialize BigQuery client using ADC
                            try:
                                # Get a sample project ID from the data to initialize client
                                sample_project = df['Source_Project_Id'].iloc[0] if 'Source_Project_Id' in df.columns else None
                                
                                if sample_project:
                                    client = initialize_client(sample_project)
                                    st.success("‚úÖ BigQuery client initialized successfully using Application Default Credentials")
                                    # Store client in session state for reuse
                                    st.session_state['bq_client'] = client
                                else:
                                    st.error("‚ö†Ô∏è Could not determine project ID for BigQuery client initialization")
                                    client = None
                                    st.session_state['bq_client'] = None
                            except Exception as e:
                                st.error(f"‚ö†Ô∏è Failed to initialize BigQuery client: {str(e)}")
                                st.info("Please ensure Application Default Credentials are set up: `gcloud auth application-default login`")
                                client = None
                                st.session_state['bq_client'] = None
                            
                            if not client:
                                st.error("üõë **VALIDATION HALTED** - BigQuery client is required for validation.")
                                st.stop()
                            
                            st.write("*Testing Source Combinations:*")
                            source_results = []
                            connectivity_failed = False
                            
                            # Test each source combination
                            for idx, row in source_combinations.iterrows():
                                project_id = row['Source_Project_Id']
                                dataset_id = row['Source_Dataset_Id']
                                
                                with st.spinner(f"Testing {project_id}.{dataset_id}..."):
                                    success, message = test_bigquery_connectivity(project_id, dataset_id, client)
                                    source_results.append({
                                        'Project_Id': project_id,
                                        'Dataset_Id': dataset_id,
                                        'Status': '‚úÖ Connected' if success else '‚ùå Failed',
                                        'Details': message
                                    })
                                    if not success:
                                        connectivity_failed = True
                            
                            # Display source results
                            source_df = pd.DataFrame(source_results)
                            st.dataframe(source_df, use_container_width=True)
                            
                            st.write("*Testing Target Combinations:*")
                            target_results = []
                            
                            # Test each target combination
                            for idx, row in target_combinations.iterrows():
                                project_id = row['Target_Project_Id']
                                dataset_id = row['Target_Dataset_Id']
                                
                                with st.spinner(f"Testing {project_id}.{dataset_id}..."):
                                    success, message = test_bigquery_connectivity(project_id, dataset_id, client)
                                    target_results.append({
                                        'Project_Id': project_id,
                                        'Dataset_Id': dataset_id,
                                        'Status': '‚úÖ Connected' if success else '‚ùå Failed',
                                        'Details': message
                                    })
                                    if not success:
                                        connectivity_failed = True
                            
                            # Display target results
                            target_df = pd.DataFrame(target_results)
                            st.dataframe(target_df, use_container_width=True)
                            
                            # Check connectivity results
                            if connectivity_failed:
                                st.error("‚ùå BigQuery connectivity test failed for one or more project/dataset combinations.")
                                st.error("üõë **VALIDATION HALTED** - All datasets must be accessible before validation can proceed.")
                                st.stop()
                            
                            st.success("‚úÖ All BigQuery connectivity tests passed!")
                            
                            # Step 2: Join Key Uniqueness Validation
                            st.write("**Step 2: Join Key Uniqueness Validation**")
                            
                            # Check for required columns for join key validation
                            join_key_required_columns = ['Source_Table', 'Source_Join_Key', 'Target_Table', 'Target_Join_Key']
                            join_key_missing_columns = [col for col in join_key_required_columns if col not in df.columns]
                            
                            if join_key_missing_columns:
                                st.error(f"‚ùå Missing columns for join key validation: {', '.join(join_key_missing_columns)}")
                                st.info("Required columns: Source_Table, Source_Join_Key, Target_Table, Target_Join_Key")
                                st.error("üõë **VALIDATION HALTED** - Join key validation columns are required.")
                                st.stop()
                            
                            st.write("*Validating Join Key Uniqueness for All Scenarios:*")
                            
                            # Enhancement: Avoid repeating join key checks for identical table combinations
                            st.write("üîç Step 2.1: Identifying unique table combinations...")
                            
                            # Remove non-relevant columns and get unique table combinations
                            exclude_columns = ['Scenario_ID', 'Scenario_Name', 'Description', 'Derivation_Logic']
                            df_for_uniqueness = df.copy()
                            
                            # Drop excluded columns if they exist
                            columns_to_drop = [col for col in exclude_columns if col in df_for_uniqueness.columns]
                            if columns_to_drop:
                                df_for_uniqueness = df_for_uniqueness.drop(columns=columns_to_drop)
                            
                            # Get unique combinations for join key validation
                            unique_combinations = df_for_uniqueness.drop_duplicates().reset_index(drop=True)
                            st.write(f"‚úÖ Reduced from {len(df)} rows to {len(unique_combinations)} unique table combinations")
                            
                            # Dictionary to cache join key validation results
                            join_key_cache = {}
                            
                            # Progress bar for join key validation
                            join_progress = st.progress(0)
                            
                            st.write("üîç Step 2.2: Running join key uniqueness checks on unique combinations...")
                            
                            # Run join key validation only on unique combinations
                            for idx, row in unique_combinations.iterrows():
                                # Update progress
                                progress = (idx + 1) / len(unique_combinations)
                                join_progress.progress(progress)
                                
                                # Parse join keys
                                source_join_keys = [key.strip() for key in str(row.get('Source_Join_Key', '')).split(',') if key.strip()]
                                target_join_keys = [key.strip() for key in str(row.get('Target_Join_Key', '')).split(',') if key.strip()]
                                
                                # Create cache key for this table combination
                                source_project = row.get('Source_Project_Id', '')
                                source_dataset = row.get('Source_Dataset_Id', '')
                                source_table = row.get('Source_Table', '')
                                target_project = row.get('Target_Project_Id', '')
                                target_dataset = row.get('Target_Dataset_Id', '')
                                target_table = row.get('Target_Table', '')
                                reference_table = row.get('Reference_Table', '')
                                
                                # Create cache key tuple
                                cache_key = (source_project, source_dataset, source_table, 
                                           target_project, target_dataset, target_table, 
                                           str(reference_table).strip() if reference_table else '')
                                
                                # Initialize cache result
                                cache_result = {
                                    'source_unique': False,
                                    'target_unique': False, 
                                    'reference_unique': True,  # Default True if no reference table
                                    'source_status': 'SKIPPED',
                                    'target_status': 'SKIPPED',
                                    'reference_status': 'N/A'
                                }
                                
                                try:
                                    if client and source_join_keys and target_join_keys:
                                        # Check source table join key uniqueness
                                        try:
                                            source_unique = are_join_keys_unique(client, source_project, source_dataset, source_table, source_join_keys)
                                            cache_result['source_unique'] = source_unique
                                            cache_result['source_status'] = '‚úÖ UNIQUE' if source_unique else '‚ùå NOT_UNIQUE'
                                        except Exception as e:
                                            cache_result['source_status'] = f'‚ùå ERROR: {str(e)[:50]}...'
                                            cache_result['source_unique'] = False
                                        
                                        # Check target table join key uniqueness
                                        try:
                                            target_unique = are_join_keys_unique(client, target_project, target_dataset, target_table, target_join_keys)
                                            cache_result['target_unique'] = target_unique
                                            cache_result['target_status'] = '‚úÖ UNIQUE' if target_unique else '‚ùå NOT_UNIQUE'
                                        except Exception as e:
                                            cache_result['target_status'] = f'‚ùå ERROR: {str(e)[:50]}...'
                                            cache_result['target_unique'] = False
                                        
                                        # Check reference table if present
                                        if reference_table and str(reference_table).strip() and str(reference_table).strip().lower() != 'nan':
                                            reference_join_keys = [key.strip() for key in str(row.get('Reference_Join_Key', '')).split(',') if key.strip()]
                                            
                                            if reference_join_keys:
                                                try:
                                                    reference_unique = are_join_keys_unique(client, source_project, source_dataset, reference_table, reference_join_keys)
                                                    cache_result['reference_unique'] = reference_unique
                                                    cache_result['reference_status'] = '‚úÖ UNIQUE' if reference_unique else '‚ùå NOT_UNIQUE'
                                                except Exception as e:
                                                    cache_result['reference_status'] = f'‚ùå ERROR: {str(e)[:50]}...'
                                                    cache_result['reference_unique'] = False
                                        
                                    else:
                                        cache_result['source_status'] = 'No client or missing join keys'
                                        cache_result['target_status'] = 'No client or missing join keys'
                                        
                                except Exception as e:
                                    cache_result['source_status'] = f'Validation error: {str(e)[:50]}...'
                                    cache_result['target_status'] = f'Validation error: {str(e)[:50]}...'
                                
                                # Store result in cache
                                join_key_cache[cache_key] = cache_result
                            
                            # Clear progress bar
                            join_progress.empty()
                            
                            st.write("üîç Step 2.3: Building validation results table using cached results...")
                            
                            # Build validation results for all original rows using cached results
                            join_key_results = []
                            join_key_validation_passed = True
                            
                            for idx, row in df.iterrows():
                                scenario_id = str(row.get('Scenario_ID', f'SC{idx+1:03d}'))
                                scenario_name = str(row.get('Scenario_Name', f'Scenario_{idx+1}'))
                                
                                # Create cache key for lookup
                                source_project = row.get('Source_Project_Id', '')
                                source_dataset = row.get('Source_Dataset_Id', '')
                                source_table = row.get('Source_Table', '')
                                target_project = row.get('Target_Project_Id', '')
                                target_dataset = row.get('Target_Dataset_Id', '')
                                target_table = row.get('Target_Table', '')
                                reference_table = row.get('Reference_Table', '')
                                
                                cache_key = (source_project, source_dataset, source_table, 
                                           target_project, target_dataset, target_table, 
                                           str(reference_table).strip() if reference_table else '')
                                
                                # Lookup cached result
                                cached_result = join_key_cache.get(cache_key, {
                                    'source_unique': False, 'target_unique': False, 'reference_unique': False,
                                    'source_status': 'NOT_FOUND', 'target_status': 'NOT_FOUND', 'reference_status': 'NOT_FOUND'
                                })
                                
                                # Create scenario result using cached data
                                scenario_result = {
                                    'Scenario_ID': scenario_id,
                                    'Scenario_Name': scenario_name,
                                    'Source_Table_Status': cached_result['source_status'],
                                    'Target_Table_Status': cached_result['target_status'],
                                    'Reference_Table_Status': cached_result['reference_status'],
                                    'Overall_Status': 'ERROR',
                                    'Details': ''
                                }
                                
                                # Set overall status based on cached results
                                if cached_result['source_unique'] and cached_result['target_unique'] and cached_result['reference_unique']:
                                    scenario_result['Overall_Status'] = '‚úÖ ALL_UNIQUE'
                                    scenario_result['Details'] = 'All join keys are unique'
                                else:
                                    scenario_result['Overall_Status'] = '‚ùå VALIDATION_FAILED'
                                    scenario_result['Details'] = 'One or more join keys are not unique'
                                    join_key_validation_passed = False
                                
                                join_key_results.append(scenario_result)
                            
                            # Display join key validation results
                            join_key_df = pd.DataFrame(join_key_results)
                            st.dataframe(join_key_df, use_container_width=True)
                            
                            # Store join key validation results in session state
                            st.session_state['join_key_validation_results'] = join_key_results
                            st.session_state['join_key_validation_passed'] = join_key_validation_passed
                            st.session_state['join_key_cache'] = join_key_cache  # Cache for reuse
                            
                            st.write(f"‚úÖ Join key validation completed using {len(join_key_cache)} unique table checks instead of {len(df)} individual checks")
                            
                            # Show summary and handle hard stop logic
                            if join_key_validation_passed:
                                st.success("‚úÖ All join key uniqueness validations passed!")
                            else:
                                if HARD_STOP_ON_UNIQUENESS:
                                    st.error("‚ùå Join key uniqueness validation failed. This is a critical requirement for data validation.")
                                    st.error("üõë **VALIDATION HALTED** - All scenarios must have unique join keys before processing can continue.")
                                    st.info("üí° Please resolve the uniqueness issues in your data tables before retrying validation.")
                                    st.stop()
                                else:
                                    st.error("‚ùå Some join key uniqueness validations failed. Please review the results above.")
                                    st.warning("‚ö†Ô∏è Scenarios with failed join key validation will be skipped during validation processing.")
                            
                            # Step 3: Enhanced Validation Framework Execution
                            st.write("**Step 3: Enhanced Validation Framework Execution**")
                            
                            # Enhancement 2: Avoid redundant connectivity and join key checks
                            # (Now handled in Steps 1 & 2 above)
                            
                            # Use the already validated results from Steps 1 & 2
                            join_key_validation_passed = st.session_state.get('join_key_validation_passed', False)
                            join_key_validation_results = st.session_state.get('join_key_validation_results', [])
                            join_key_cache = st.session_state.get('join_key_cache', {})
                            client = st.session_state.get('bq_client')
                            
                            # Create lookup for join key validation results for per-scenario processing
                            join_key_lookup = {}
                            if join_key_validation_results:
                                for result in join_key_validation_results:
                                    scenario_id = result['Scenario_ID']
                                    join_key_lookup[scenario_id] = result['Overall_Status'] == '‚úÖ ALL_UNIQUE'
                            
                            # Generate Excel file with execution results
                            st.write("üìä **Excel File with Execution Results:**")
                            
                            # Create a copy of the original dataframe
                            st.write("üìã Step 1: Preparing data structures...")
                            df_with_results = df.copy()
                            
                            # Initialize lists for new columns
                            processing_methods = []
                            total_passed = []
                            total_failed = []
                            overall_status = []
                            failure_files_created = []
                            st.write("‚úÖ Data structures initialized")
                            
                            # Create output directory for failure files
                            st.write("üìÅ Step 2: Setting up output directories...")
                            script_dir = os.path.dirname(os.path.abspath(__file__))
                            output_dir = os.path.join(script_dir, "output", "ScenarioFailures")
                            st.write(f"‚úÖ Output directory: {output_dir}")
                            
                            # Clean up existing output files from previous runs
                            st.write("üßπ Step 2.1: Cleaning up old output files...")
                            if os.path.exists(output_dir):
                                try:
                                    # Remove all files in the output directory
                                    for filename in os.listdir(output_dir):
                                        file_path = os.path.join(output_dir, filename)
                                        if os.path.isfile(file_path):
                                            os.remove(file_path)
                                    st.info("üßπ Cleaned up old output files from previous runs.")
                                except Exception as e:
                                    st.warning(f"‚ö†Ô∏è Warning: Could not clean all old files: {str(e)}")
                            else:
                                # Create directory if it doesn't exist
                                os.makedirs(output_dir, exist_ok=True)
                                st.info("üìÅ Created new output directory.")
                            
                            # Also clean up the main output Excel file
                            enhanced_excel_path = os.path.join(script_dir, "output", "ValidationResults_Enhanced.xlsx")
                            if os.path.exists(enhanced_excel_path):
                                try:
                                    os.remove(enhanced_excel_path)
                                    st.info("üßπ Removed previous validation results Excel file.")
                                except Exception as e:
                                    st.warning(f"‚ö†Ô∏è Warning: Could not remove previous Excel file: {str(e)}")
                            
                            # Enhancement 2: Skip redundant client initialization - already have cached client
                            st.write("üîå Step 3: Using cached BigQuery client and validation results...")
                            st.write("‚úÖ BigQuery client and join key validation results retrieved from cache")
                            
                            # Create a lookup dictionary for join key validation results (already validated above)
                            join_key_lookup = {}
                            if join_key_validation_results:
                                for result in join_key_validation_results:
                                    scenario_id = result['Scenario_ID']
                                    join_key_lookup[scenario_id] = result['Overall_Status'] == '‚úÖ ALL_UNIQUE'
                            
                            # Process each row
                            st.write(f"üîÑ Step 4: Processing {len(df)} validation scenarios with Enhanced Framework...")
                            st.info("üöÄ Performance Enhancement: Using unique combinations approach to dramatically reduce data processing volume for large datasets.")
                            
                            # Create a progress bar and status container
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            
                            for idx, row in df.iterrows():
                                # Update progress
                                progress = (idx + 1) / len(df)
                                progress_bar.progress(progress)
                                
                                scenario_id = str(row.get('Scenario_ID', f'SC{idx+1:03d}'))
                                scenario_name = str(row.get('Scenario_Name', f'Scenario_{idx+1}'))
                                
                                status_text.write(f"üîç Processing scenario {idx+1}/{len(df)}: {scenario_id} - {scenario_name}")
                                
                                # Check join key uniqueness for this scenario (per-scenario check)
                                if not HARD_STOP_ON_UNIQUENESS:
                                    # SOFT SKIP: Check if this specific scenario has failed join key validation
                                    scenario_passed_uniqueness = join_key_lookup.get(scenario_id, True)  # Default to True if not found
                                    if not scenario_passed_uniqueness:
                                        st.warning(f"‚ö†Ô∏è Skipping scenario {scenario_id} - Join key uniqueness validation failed")
                                        # Add default values for skipped scenario
                                        processing_methods.append(f"SKIPPED - Join key uniqueness failed")
                                        total_passed.append(0)
                                        total_failed.append(0)
                                        overall_status.append('SKIPPED')
                                        failure_files_created.append("Skipped due to join key uniqueness failure")
                                        continue  # Skip to next scenario
                                
                                try:
                                    # Execute Enhanced Validation Framework (direct processing, no SQL generation)
                                    st.write(f"üöÄ Executing enhanced validation framework for scenario {scenario_id}...")
                                    
                                    # Show debug information in an expander
                                    with st.expander(f"üîç Enhanced Debug Info for {scenario_id} (Click to expand)", expanded=False):
                                        derivation_logic = row.get('Derivation_Logic', '')
                                        source_cols, target_cols, ref_cols = extract_required_columns_by_table(row)
                                        
                                        st.write(f"**Derivation Logic:** `{derivation_logic}`")
                                        st.write(f"**Table-Specific Column Requirements:**")
                                        st.write(f"   ‚Ä¢ Source table columns: `{sorted(source_cols) if source_cols else 'None'}`")
                                        st.write(f"   ‚Ä¢ Target table columns: `{sorted(target_cols) if target_cols else 'None'}`")
                                        if ref_cols:
                                            st.write(f"   ‚Ä¢ Reference table columns: `{sorted(ref_cols)}`")
                                        
                                        st.write(f"**Table Information:**")
                                        st.write(f"   ‚Ä¢ Source Table: `{row.get('Source_Project_Id')}.{row.get('Source_Dataset_Id')}.{row.get('Source_Table')}`")
                                        st.write(f"   ‚Ä¢ Target Table: `{row.get('Target_Project_Id')}.{row.get('Target_Dataset_Id')}.{row.get('Target_Table')}`")
                                        if row.get('Reference_Table') and str(row.get('Reference_Table')).strip().lower() != 'nan':
                                            st.write(f"   ‚Ä¢ Reference Table: `{row.get('Source_Project_Id')}.{row.get('Source_Dataset_Id')}.{row.get('Reference_Table')}`")
                                    
                                    success, message, pass_count, fail_count, failure_file_path = enhanced_validation_execution(
                                        row, client, scenario_id, scenario_name, output_dir
                                    )
                                    
                                    if success:
                                        st.write(f"‚úÖ Scenario {scenario_id}: {pass_count} PASS, {fail_count} FAIL (enhanced)")
                                        total_passed.append(pass_count)
                                        total_failed.append(fail_count)
                                        # Set overall status: PASS if no failures, FAIL if any failures
                                        overall_status.append('PASS' if fail_count == 0 else 'FAIL')
                                        # Enhanced Framework processing completed successfully
                                        processing_methods.append("Enhanced Framework - Direct Processing")
                                        
                                        # Track failure file creation
                                        if failure_file_path:
                                            if failure_file_path.startswith("SKIPPED"):
                                                st.write(f"‚ö†Ô∏è Scenario {scenario_id}: {failure_file_path}")
                                                failure_files_created.append(failure_file_path)
                                            else:
                                                relative_path = os.path.relpath(failure_file_path, script_dir)
                                                st.write(f"üìÑ Failure file created: {relative_path}")
                                                failure_files_created.append(relative_path)
                                        else:
                                            failure_files_created.append("No failures")
                                    else:
                                        st.error(f"‚ùå Scenario {scenario_id} execution failed: {message}")
                                        # If execution failed, set default values
                                        processing_methods.append(f"EXECUTION_ERROR - {message}")
                                        total_passed.append(0)
                                        total_failed.append(0)
                                        overall_status.append('EXECUTION_ERROR')
                                        failure_files_created.append("Execution failed")
                                        
                                except ValueError as ve:
                                    # Handle uniqueness validation errors specifically
                                    if "do not uniquely identify rows" in str(ve):
                                        st.error(f"‚ùå Join key uniqueness validation failed for scenario {scenario_id}: {str(ve)}")
                                        # Add default values for failed scenario
                                        processing_methods.append(f"UNIQUENESS_ERROR - {str(ve)}")
                                        total_passed.append(0)
                                        total_failed.append(0)
                                        overall_status.append('UNIQUENESS_ERROR')
                                        failure_files_created.append(f"Uniqueness Error: {str(ve)}")
                                    else:
                                        st.error(f"‚ùå Validation error for scenario {scenario_id}: {str(ve)}")
                                        # Add default values for failed scenario
                                        processing_methods.append(f"VALIDATION_ERROR - {str(ve)}")
                                        total_passed.append(0)
                                        total_failed.append(0)
                                        overall_status.append('VALIDATION_ERROR')
                                        failure_files_created.append(f"Validation Error: {str(ve)}")
                                except Exception as e:
                                    st.error(f"‚ùå Error processing scenario {scenario_id}: {str(e)}")
                                    # Add default values for failed scenario
                                    processing_methods.append(f"PROCESSING_ERROR - {str(e)}")
                                    total_passed.append(0)
                                    total_failed.append(0)
                                    overall_status.append('ERROR')
                                    failure_files_created.append(f"Error: {str(e)}")
                            
                            # Clear progress indicators
                            progress_bar.empty()
                            status_text.empty()
                            
                            st.write("üíæ Step 5: Saving results to Excel file...")
                            
                            # Add the new columns to dataframe
                            df_with_results['Processing_Method'] = processing_methods
                            df_with_results['Total_Passed'] = total_passed
                            df_with_results['Total_Failed'] = total_failed
                            df_with_results['Overall_Status'] = overall_status
                            df_with_results['Failure_File_Path'] = failure_files_created
                            
                            # Save enhanced Excel file to output directory
                            enhanced_excel_path = os.path.join(script_dir, "output", "ValidationResults_Enhanced.xlsx")
                            os.makedirs(os.path.dirname(enhanced_excel_path), exist_ok=True)
                            st.write(f"üìÅ Creating Excel file: {enhanced_excel_path}")
                            
                            # Write to Excel file in output directory
                            with pd.ExcelWriter(enhanced_excel_path, engine='openpyxl') as writer:
                                st.write("üìù Writing Results sheet...")
                                # Create Results sheet with specified columns in order (FIRST TAB)
                                results_columns = [
                                    'Scenario_ID', 'Scenario_Name', 'Description', 'Total_Failed', 
                                    'Total_Passed', 'Overall_Status', 'Failure_File_Path', 'Processing_Method'
                                ]
                                
                                # Filter DataFrame to include only the specified columns that exist
                                available_columns = [col for col in results_columns if col in df_with_results.columns]
                                df_results = df_with_results[available_columns].copy()
                                
                                # Write Results sheet first
                                df_results.to_excel(writer, sheet_name='Results', index=False)
                                
                                # Auto-adjust column widths for Results sheet
                                results_worksheet = writer.sheets['Results']
                                for column in results_worksheet.columns:
                                    max_length = 0
                                    column_letter = column[0].column_letter
                                    for cell in column:
                                        try:
                                            if len(str(cell.value)) > max_length:
                                                max_length = len(str(cell.value))
                                        except:
                                            pass
                                    # Set a reasonable max width
                                    adjusted_width = min(max_length + 2, 50)
                                    results_worksheet.column_dimensions[column_letter].width = adjusted_width
                                
                                st.write("üìù Writing ValidationScenarios sheet...")
                                # Create ValidationScenarios sheet excluding execution result columns (SECOND TAB)
                                validation_exclude_columns = ['Processing_Method', 'Total_Passed', 'Total_Failed', 'Overall_Status', 'Failure_File_Path']
                                df_validation = df_with_results.drop(columns=[col for col in validation_exclude_columns if col in df_with_results.columns])
                                
                                # Write filtered validation results to ValidationScenarios sheet
                                df_validation.to_excel(writer, sheet_name='ValidationScenarios', index=False)
                                
                                # Auto-adjust column widths for ValidationScenarios sheet
                                worksheet = writer.sheets['ValidationScenarios']
                                for column in worksheet.columns:
                                    max_length = 0
                                    column_letter = column[0].column_letter
                                    for cell in column:
                                        try:
                                            if len(str(cell.value)) > max_length:
                                                max_length = len(str(cell.value))
                                        except:
                                            pass
                                    # Set a reasonable max width
                                    adjusted_width = min(max_length + 2, 50)
                                    worksheet.column_dimensions[column_letter].width = adjusted_width
                            
                            st.write("‚úÖ Excel file created successfully!")
                            
                            # Show summary statistics
                            st.write("üìä **Final Execution Summary:**")
                            summary_col1, summary_col2, summary_col3, summary_col4 = st.columns(4)
                            
                            with summary_col1:
                                total_scenarios = len(df_with_results)
                                st.metric("Total Scenarios", total_scenarios)
                            
                            with summary_col2:
                                passed_scenarios = len(df_with_results[df_with_results['Overall_Status'] == 'PASS'])
                                st.metric("Scenarios Passed", passed_scenarios)
                            
                            with summary_col3:
                                failed_scenarios = len(df_with_results[df_with_results['Overall_Status'] == 'FAIL'])
                                st.metric("Scenarios Failed", failed_scenarios)
                            
                            with summary_col4:
                                failure_files_count = len([f for f in failure_files_created if f not in ["No failures", "Execution failed", "No BigQuery client"] and not f.startswith("SKIPPED")])
                                st.metric("Failure Files Created", failure_files_count)
                            
                            # Show information about all created files
                            st.write("**Created Files:**")
                            enhanced_excel_relative_path = os.path.relpath(enhanced_excel_path, script_dir)
                            st.write(f"üìÑ **Main Report:** {enhanced_excel_relative_path}")
                            
                            # Show created failure files
                            if failure_files_count > 0:
                                st.write(f"üìã **Individual Failure Files ({failure_files_count}):**")
                                failure_files_list = [f for f in failure_files_created if f not in ["No failures", "Execution failed", "No BigQuery client"] and not f.startswith("SKIPPED")]
                                for file_path in failure_files_list:
                                    st.write(f"   ‚Ä¢ {file_path}")
                            
                            # Show skipped files due to size limit
                            skipped_files = [f for f in failure_files_created if f.startswith("SKIPPED")]
                            if skipped_files:
                                st.write(f"‚ö†Ô∏è **Skipped Files Due to Size Limit (>10,000 rows):**")
                                for skipped_file in skipped_files:
                                    st.write(f"   ‚Ä¢ {skipped_file}")
                            
                            st.info(f"üí° All validation files have been saved to the `output/` directory.")
                            
                            st.success("‚úÖ Enhanced validation completed! All result files and individual failure files have been created!")
                        
            except Exception as e:
                st.error(f"Error reading file: {e}")
        
        elif uploaded_file.type == 'text/plain':
            st.write("**File Content:**")
            content = uploaded_file.read().decode('utf-8')
            st.text_area("File content", content, height=200)
        
        elif uploaded_file.type == 'application/json':
            st.write("**JSON Content:**")
            import json
            content = uploaded_file.read().decode('utf-8')
            try:
                json_data = json.loads(content)
                st.json(json_data)
            except Exception as e:
                st.error(f"Error parsing JSON: {e}")
    
    else:
        st.info("Please select a file to attach using the file uploader above.")

if __name__ == "__main__":
    main()
